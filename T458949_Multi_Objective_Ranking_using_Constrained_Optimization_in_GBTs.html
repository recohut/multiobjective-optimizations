
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Multi-Objective Ranking using Constrained Optimization in GBTs &#8212; multiobjective-optimizations</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Multi-objective Optimization using Pymoo Library" href="T145475_Multi_objective_Optimization_using_Pymoo_Library.html" />
    <link rel="prev" title="Sparsely-gated Mixture-of-Experts" href="T948705_Sparsely_gated_Mixture_of_Experts.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">multiobjective-optimizations</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="L069335_Multi_Objective_Optimization.html">
   Multi-Objective Optimization
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Concepts
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="L492548_Efficient_Frontier.html">
   Efficient Frontier
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L839342_Pareto_Optimality.html">
   Pareto Optimality
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L483593_MOO_Decision_Maker.html">
   MOO Decision Maker
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L336869_Multi_Task_Learning.html">
   Multi-Task Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L553498_MOO_in_Recommender_Systems.html">
   MOO in Recommender Systems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L480379_Marketplace_Recommenders.html">
   Marketplace Recommenders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L556513_Multi_Objective_Hyperparameter_Optimization.html">
   Multi-Objective Hyperparameter Optimization
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Methods
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="L005003_Scalarization.html">
   Scalarization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L762719_Shared_Bottom.html">
   Shared Bottom
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L915313_Expected_Hypervolume_Improvement_%28EHVI%29.html">
   Expected Hypervolume Improvement (EHVI)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L587138_Multi_gradient_Descent_%28MGDRec%29.html">
   Multi-gradient Descent (MGDRec)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L861465_Mixture_of_Experts_%28MoE%29.html">
   Mixture of Experts (MoE)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L485744_Multi_gate_Mixture_of_Experts_%28MMoE%29.html">
   Multi-gate Mixture-of-Experts (MMoE)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L519476_Entire_Space_Multi_Task_Model_%28ESSM%29.html">
   Entire Space Multi-Task Model (ESSM)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L718205_Progressive_Layered_Extraction_%28PLE%29.html">
   Progressive Layered Extraction (PLE)
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Cases
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="L749932_MTL_for_Related_Products_Recommendations_at_Pinterest.html">
   MTL for Related Products Recommendations at Pinterest
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L792752_UberEats.html">
   UberEats
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L106645_Etsy.html">
   Etsy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L149938_Airbnb_Experiences.html">
   Airbnb Experiences
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="T929652_Efficient_Frontier.html">
   Efficient Frontier
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T854417_Linear_Optimization_with_OR_Tools.html">
   Linear Optimization with OR-Tools
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T209908_Stein%27s_Paradox.html">
   Stein’s Paradox
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T423887_Solving_MOO_with_SMT_Toolkit.html">
   Solving MOO with SMT Toolkit
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T277640_Efficient_Continuous_Pareto_Exploration_in_MTL_on_UCI_Census.html">
   Efficient Continuous Pareto Exploration in MTL on UCI Census
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T851476_Multi_Task_Learning.html">
   Multi-Task Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T307666_Pareto_Efficient_algorithm_for_MOO.html">
   Pareto-Efficient algorithm for MOO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T465017_Shared_Bottom_in_Tensorflow.html">
   Shared Bottom in Tensorflow
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T175823_MoE_in_Tensorflow.html">
   MoE in Tensorflow
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T580632_MMoE_on_Census_income_data_in_Tensorflow.html">
   MMoE on Census income data in Tensorflow
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T948705_Sparsely_gated_Mixture_of_Experts.html">
   Sparsely-gated Mixture-of-Experts
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Multi-Objective Ranking using Constrained Optimization in GBTs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T145475_Multi_objective_Optimization_using_Pymoo_Library.html">
   Multi-objective Optimization using Pymoo Library
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T892775_Exploring_Multi_Objective_Hyperparameter_Optimization.html">
   Exploring Multi-Objective Hyperparameter Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T660394_Implicit_Hybrid_Movie_Recommender_using_Collie_Library.html">
   Implicit Hybrid Movie Recommender using Collie Library
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T559084_Multi_task_Learning_on_ML_100k_in_TFRS.html">
   Multi-task Learning on ML-100k in TFRS
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T368830_Multi_Task_Recommender_on_Olist_dataset_using_TFRS_Library.html">
   Multi-Task Recommender on Olist dataset using TFRS Library
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T671443_MAMO_Framework.html">
   MAMO Framework
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T952247_MKR.html">
   MKR
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/sparsh-ai/multiobjective-optimizations/main?urlpath=tree/docs/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/sparsh-ai/multiobjective-optimizations/blob/main/docs/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-procedure">
   Training Procedure
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#utils">
   Utils
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-boosting-losses">
   Gradient Boosting Losses
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-boosted-regression-trees">
   Gradient Boosted Regression Trees
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#expedia-dataset">
   Expedia Dataset
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-preparation">
   Data Preparation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#unconstrained-gbt-on-primary-objective">
   Unconstrained GBT on Primary Objective
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#constrained-gbt">
   Constrained GBT
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#changing-hyperparameters-b-and-mu">
   Changing hyperparameters b and mu
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#scalarization">
   Scalarization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#primary-objective-unconstrained-gbt">
   Primary Objective Unconstrained GBT
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#takeaways">
   Takeaways
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="multi-objective-ranking-using-constrained-optimization-in-gbts">
<h1>Multi-Objective Ranking using Constrained Optimization in GBTs<a class="headerlink" href="#multi-objective-ranking-using-constrained-optimization-in-gbts" title="Permalink to this headline">¶</a></h1>
<p>This method is inspired by <a class="reference external" href="https://arxiv.org/pdf/2002.05753.pdf">Multi-objective Ranking via Constrained Optimization</a> which introduces multi-objective optimization in Search by incorporating constrained optimization in LambdaMART, a popular LTR algorithm.</p>
<p>For recommendations in general, in a pointwise modeling approach, user-product pairs are scored based on previous interactions between users and products. The outcome of this interaction (say, click or conversion) becomes the target variable and is typically the primary objective of the ML model. However, in most cases, there are other objectives (or constraints) that will need to be handled. For example, cost-of-delivery, time-to-delivery, fairness-to-partners, diversity, repeat-rate, etc. Furthermore, the importance of these additional constraints can vary subject to other factors. Let’s call these constraints sub-objectives.</p>
<tr>
<td>
<p><center><figure><img src='https://s3.us-west-2.amazonaws.com/secure.notion-static.com/55a5b31a-ddf2-45a2-b8a2-4ccd52ee8227/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20211027%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20211027T065415Z&X-Amz-Expires=86400&X-Amz-Signature=de1b268ce06270b6bf4d4cba28a581b704b490adb7c99d27ef5c6c9fd984a7e4&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22' width=400><figcaption>Scatter with respect to the primary objective of whether there was a click on a restaurant ad.</figcaption></figure></center></p>
</td>
<td>
<p><center><figure><img src='https://s3.us-west-2.amazonaws.com/secure.notion-static.com/1a619e07-297a-4e99-a973-15b59de1fbaa/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20211027%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20211027T065611Z&X-Amz-Expires=86400&X-Amz-Signature=048844c3b9cfd9ff8553acd8bb0a29397689105157f9f1beb78e133d4d43a18f&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22' width=400><figcaption>Scatter with respect to the sub-objective which is the normalized SLA of delivery from the restaurant.</figcaption></figure></center></p>
</td>
</tr><p>Consider the plots above, representing two target variables with respect to two features f1 and f2. We can model them individually with simple NNs but how to optimize them together? Our goal is to develop a model which optimizes for both the primary objective (say, clicks) and the sub-objective (say, SLA).</p>
<p>Several methods have been explored to handle MOO. Broadly, there are (at least) 3–4 classes of methods:</p>
<ol class="simple">
<li><p>scalarization or post processing (where the additional objectives are not really part of the learning process but are handled post facto). Construct a convex linear combination — λ * <em>S1</em> + (1-λ) * S2, where S1 and S2 are scores from individual models trained on different objectives and λ is the weight learned from a Pareto analysis. This method works surprisingly well in practice and is a very credible baseline.</p></li>
<li><p>changing the training data (where the model is built on data that is <a class="reference external" href="https://arxiv.org/pdf/2008.10277.pdf">sampled</a> from the ‘desired’ region).</p></li>
<li><p>changing the way labels are aggregated (for example, <a class="reference external" href="https://assets.amazon.science/4d/9c/69cbef8346408349385c780cac48/scipub-1195.pdf">stochastic label aggregation</a> method of David Carmel, et al.).</p></li>
<li><p>changing the loss function itself. To solve for multiple objectives we modify the algorithm to include constraints in it’s loss function, which are specified as upper bound on the cost value for the sub-objectives.</p></li>
</ol>
<p>In this tutorial, we will modify the Gradient Boosted Trees (GBT) algorithm to include constraints in it’s loss function. We combine GBT with the <a class="reference external" href="https://en.wikipedia.org/wiki/Augmented_Lagrangian_method">Augmented Lagrangian method</a>, which is a class of algorithms to solve constrained optimization problems. We define the constrained optimization problem as:</p>
<div class="math notranslate nohighlight">
\[\min_s C^{pm} (s)\ s.t.\ C^t (s) \le b^t, t=1,...,T.\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(C^{pm}\)</span> represents the cost of the primary objective</p></li>
<li><p><span class="math notranslate nohighlight">\(C^t\)</span> represents the cost of sub-objective <span class="math notranslate nohighlight">\(t\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(T\)</span> is total number of sub-objectives</p></li>
<li><p><span class="math notranslate nohighlight">\(b\)</span> represents the upper bound specified for the sub-objective cost during training</p></li>
</ul>
<p>The Lagrangian is written as:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(s,\alpha) = C^{pm}(s) + \sum_t^T\alpha^t(C^t(s)-b^t)\]</div>
<p>where <span class="math notranslate nohighlight">\(α = [α1, α2, α3…..]\)</span> is a vector of dual variables. The Lagrangian is solved by minimizing with respect to the primary variables <span class="math notranslate nohighlight">\(s\)</span> and maximizing with respect to the dual variables <span class="math notranslate nohighlight">\(α\)</span>.</p>
<p>Augmented Lagrangian (AL) has an additional penalty term that iteratively solves the constraint optimization problem:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(s,\alpha) = C^{pm}(s) + \sum_t^T\alpha^t(C^t(s)-b^t) - \sum_t^T\dfrac{1}{2\mu_k}(\alpha^t-\alpha_{k-1}^t)^2\]</div>
<p>where <span class="math notranslate nohighlight">\(α_{k−1}^t\)</span> is a solution in the previous iteration and a constant in the current iteration <span class="math notranslate nohighlight">\(k\)</span>. <span class="math notranslate nohighlight">\(μ\)</span> is a sufficiently large constant associated with each dual variable <span class="math notranslate nohighlight">\(α\)</span>.</p>
<p>We have modified the loss function of scikit-learn <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html">GBT Classifier</a> and <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor">GBT Regressor</a> to optimize for multiple objectives and combine the iterations of GBT (building trees) with the iterative constrained optimization framework (AL) defined in the previous section.</p>
<p>Below is the GBT algorithm for Classification/Regression and how we modified it to serve for multiple objectives. GBT requires a differentiable loss function. We modify a traditional loss function to include the constraint terms and the AL term which is constant for an iteration.</p>
<p>We will be expanding <a class="reference external" href="https://github.com/Swiggy/Moo-GBT">https://github.com/Swiggy/Moo-GBT</a> modules in this section. So be ready for 1Ks code lines. We are doing it this way instead of installing the package because 1) we are learning, not working, 2) codebase is 1Ks, not 10K, so our notebook can easily handle it for us, and most importantly, 3) It is optional part because we can directly jump to runs, present in the last quarter of notebook.</p>
<div class="section" id="training-procedure">
<h2>Training Procedure<a class="headerlink" href="#training-procedure" title="Permalink to this headline">¶</a></h2>
<p><center><img src='https://s3.us-west-2.amazonaws.com/secure.notion-static.com/4113c08f-4a87-4399-92bd-042209eb405a/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20211027%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20211027T065846Z&X-Amz-Expires=86400&X-Amz-Signature=2b88f10fcba30c171bee2e28eb2d13c8512dc70a8e496a7f9137b434c192c946&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22' width=50%></center></p>
<ol class="simple">
<li><p>GBT uses residuals as target values to build the current tree. Calculation of these residuals changes as we modify the GBT loss function to include sub-objective constraints and the AL term (constant for an iteration)</p></li>
<li><p>For each leaf we calculate an output value, gamma, such that it minimizes the loss function</p></li>
<li><p>Modify the dual variables, alphas, which are used in the loss function based on the respective constraint being met or not</p></li>
</ol>
</div>
<div class="section" id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABCMeta</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>
<span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">effective_n_jobs</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">clone</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">is_classifier</span><span class="p">,</span> <span class="n">is_regressor</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">MetaEstimatorMixin</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">Bunch</span><span class="p">,</span> <span class="n">_print_elapsed_time</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.metaestimators</span> <span class="kn">import</span> <span class="n">_BaseComposition</span>
 
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">ClassifierMixin</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">RegressorMixin</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">is_classifier</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">deprecated</span>

<span class="kn">from</span> <span class="nn">sklearn.ensemble._gradient_boosting</span> <span class="kn">import</span> <span class="n">predict_stages</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble._gradient_boosting</span> <span class="kn">import</span> <span class="n">predict_stage</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble._gradient_boosting</span> <span class="kn">import</span> <span class="n">_random_sample_mask</span>

<span class="kn">import</span> <span class="nn">numbers</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">expit</span><span class="p">,</span> <span class="n">logsumexp</span>

<span class="kn">from</span> <span class="nn">sklearn.tree._tree</span> <span class="kn">import</span> <span class="n">TREE_LEAF</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.stats</span> <span class="kn">import</span> <span class="n">_weighted_percentile</span>
<span class="kn">from</span> <span class="nn">sklearn.dummy</span> <span class="kn">import</span> <span class="n">DummyClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.dummy</span> <span class="kn">import</span> <span class="n">DummyRegressor</span>

<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="kn">import</span> <span class="n">csc_matrix</span>
<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="kn">import</span> <span class="n">csr_matrix</span>
<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="kn">import</span> <span class="n">issparse</span>

<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.tree._tree</span> <span class="kn">import</span> <span class="n">DTYPE</span><span class="p">,</span> <span class="n">DOUBLE</span>

<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_array</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">column_or_1d</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.validation</span> <span class="kn">import</span> <span class="n">check_is_fitted</span><span class="p">,</span> <span class="n">_check_sample_weight</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.multiclass</span> <span class="kn">import</span> <span class="n">check_classification_targets</span>
<span class="kn">from</span> <span class="nn">sklearn.exceptions</span> <span class="kn">import</span> <span class="n">NotFittedError</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.validation</span> <span class="kn">import</span> <span class="n">_deprecate_positional_args</span>

<span class="kn">from</span> <span class="nn">sklearn.utils.validation</span> <span class="kn">import</span> <span class="n">check_X_y</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="utils">
<h2>Utils<a class="headerlink" href="#utils" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_fit_single_estimator</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">message_clsname</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Private function used to fit an estimator within a job.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">_print_elapsed_time</span><span class="p">(</span><span class="n">message_clsname</span><span class="p">,</span> <span class="n">message</span><span class="p">):</span>
                <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">exc</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;unexpected keyword argument &#39;sample_weight&#39;&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">exc</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="s2">&quot;Underlying estimator </span><span class="si">{}</span><span class="s2"> does not support sample weights.&quot;</span>
                    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">estimator</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
                <span class="p">)</span> <span class="kn">from</span> <span class="nn">exc</span>
            <span class="k">raise</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">_print_elapsed_time</span><span class="p">(</span><span class="n">message_clsname</span><span class="p">,</span> <span class="n">message</span><span class="p">):</span>
            <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">estimator</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_set_random_states</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Set fixed random_state parameters for an estimator.</span>
<span class="sd">    Finds all parameters ending ``random_state`` and sets them to integers</span>
<span class="sd">    derived from ``random_state``.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    estimator : estimator supporting get/set_params</span>
<span class="sd">        Estimator with potential randomness managed by random_state</span>
<span class="sd">        parameters.</span>
<span class="sd">    random_state : int, RandomState instance or None, default=None</span>
<span class="sd">        Pseudo-random number generator to control the generation of the random</span>
<span class="sd">        integers. Pass an int for reproducible output across multiple function</span>
<span class="sd">        calls.</span>
<span class="sd">        See :term:`Glossary &lt;random_state&gt;`.</span>
<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    This does not necessarily set *all* ``random_state`` attributes that</span>
<span class="sd">    control an estimator&#39;s randomness, only those accessible through</span>
<span class="sd">    ``estimator.get_params()``.  ``random_state``s not controlled include</span>
<span class="sd">    those belonging to:</span>
<span class="sd">        * cross-validation splitters</span>
<span class="sd">        * ``scipy.stats`` rvs</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="n">to_set</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">estimator</span><span class="o">.</span><span class="n">get_params</span><span class="p">(</span><span class="n">deep</span><span class="o">=</span><span class="kc">True</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s1">&#39;random_state&#39;</span> <span class="ow">or</span> <span class="n">key</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;__random_state&#39;</span><span class="p">):</span>
            <span class="n">to_set</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">random_state</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">to_set</span><span class="p">:</span>
        <span class="n">estimator</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">to_set</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">BaseEnsemble</span><span class="p">(</span><span class="n">MetaEstimatorMixin</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base class for all ensemble classes.</span>
<span class="sd">    Warning: This class should not be used directly. Use derived classes</span>
<span class="sd">    instead.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    base_estimator : object</span>
<span class="sd">        The base estimator from which the ensemble is built.</span>
<span class="sd">    n_estimators : int, default=10</span>
<span class="sd">        The number of estimators in the ensemble.</span>
<span class="sd">    estimator_params : list of str, default=tuple()</span>
<span class="sd">        The list of attributes to use as parameters when instantiating a</span>
<span class="sd">        new base estimator. If none are given, default parameters are used.</span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    base_estimator_ : estimator</span>
<span class="sd">        The base estimator from which the ensemble is grown.</span>
<span class="sd">    estimators_ : list of estimators</span>
<span class="sd">        The collection of fitted base estimators.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># overwrite _required_parameters from MetaEstimatorMixin</span>
    <span class="n">_required_parameters</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base_estimator</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                 <span class="n">estimator_params</span><span class="o">=</span><span class="nb">tuple</span><span class="p">()):</span>
        <span class="c1"># Set parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span> <span class="o">=</span> <span class="n">base_estimator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="n">n_estimators</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">estimator_params</span> <span class="o">=</span> <span class="n">estimator_params</span>

        <span class="c1"># Don&#39;t instantiate estimators now! Parameters of base_estimator might</span>
        <span class="c1"># still change. Eg., when grid-searching with the nested object syntax.</span>
        <span class="c1"># self.estimators_ needs to be filled by the derived classes in fit.</span>

    <span class="k">def</span> <span class="nf">_validate_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check the estimator and the n_estimator attribute.</span>
<span class="sd">        Sets the base_estimator_` attributes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;n_estimators must be an integer, &quot;</span>
                             <span class="s2">&quot;got </span><span class="si">{0}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">)))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;n_estimators must be greater than zero, &quot;</span>
                             <span class="s2">&quot;got </span><span class="si">{0}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator_</span> <span class="o">=</span> <span class="n">default</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator_</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;base_estimator cannot be None&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_make_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">append</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Make and configure a copy of the `base_estimator_` attribute.</span>
<span class="sd">        Warning: This method should be used to properly instantiate new</span>
<span class="sd">        sub-estimators.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">estimator</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_estimator_</span><span class="p">)</span>
        <span class="n">estimator</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="n">p</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
                                <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimator_params</span><span class="p">})</span>

        <span class="k">if</span> <span class="n">random_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_set_random_states</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">random_state</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">append</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">estimator</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">estimator</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the number of estimators in the ensemble.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the index&#39;th estimator in the ensemble.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return iterator over estimators in the ensemble.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_partition_estimators</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">,</span> <span class="n">n_jobs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Private function used to partition estimators between jobs.&quot;&quot;&quot;</span>
    <span class="c1"># Compute the number of jobs</span>
    <span class="n">n_jobs</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">effective_n_jobs</span><span class="p">(</span><span class="n">n_jobs</span><span class="p">),</span> <span class="n">n_estimators</span><span class="p">)</span>

    <span class="c1"># Partition estimators between jobs</span>
    <span class="n">n_estimators_per_job</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">n_estimators</span> <span class="o">//</span> <span class="n">n_jobs</span><span class="p">,</span>
                                   <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">n_estimators_per_job</span><span class="p">[:</span><span class="n">n_estimators</span> <span class="o">%</span> <span class="n">n_jobs</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">starts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">n_estimators_per_job</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">n_jobs</span><span class="p">,</span> <span class="n">n_estimators_per_job</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">starts</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">_BaseHeterogeneousEnsemble</span><span class="p">(</span><span class="n">MetaEstimatorMixin</span><span class="p">,</span> <span class="n">_BaseComposition</span><span class="p">,</span>
                                 <span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base class for heterogeneous ensemble of learners.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    estimators : list of (str, estimator) tuples</span>
<span class="sd">        The ensemble of estimators to use in the ensemble. Each element of the</span>
<span class="sd">        list is defined as a tuple of string (i.e. name of the estimator) and</span>
<span class="sd">        an estimator instance. An estimator can be set to `&#39;drop&#39;` using</span>
<span class="sd">        `set_params`.</span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    estimators_ : list of estimators</span>
<span class="sd">        The elements of the estimators parameter, having been fitted on the</span>
<span class="sd">        training data. If an estimator has been set to `&#39;drop&#39;`, it will not</span>
<span class="sd">        appear in `estimators_`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_required_parameters</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;estimators&#39;</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">named_estimators</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Bunch</span><span class="p">(</span><span class="o">**</span><span class="nb">dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators</span><span class="p">))</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">estimators</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">estimators</span> <span class="o">=</span> <span class="n">estimators</span>

    <span class="k">def</span> <span class="nf">_validate_estimators</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid &#39;estimators&#39; attribute, &#39;estimators&#39; should be a list&quot;</span>
                <span class="s2">&quot; of (string, estimator) tuples.&quot;</span>
            <span class="p">)</span>
        <span class="n">names</span><span class="p">,</span> <span class="n">estimators</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators</span><span class="p">)</span>
        <span class="c1"># defined by MetaEstimatorMixin</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_names</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>

        <span class="n">has_estimator</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">est</span> <span class="o">!=</span> <span class="s1">&#39;drop&#39;</span> <span class="k">for</span> <span class="n">est</span> <span class="ow">in</span> <span class="n">estimators</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">has_estimator</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;All estimators are dropped. At least one is required &quot;</span>
                <span class="s2">&quot;to be an estimator.&quot;</span>
            <span class="p">)</span>

        <span class="n">is_estimator_type</span> <span class="o">=</span> <span class="p">(</span><span class="n">is_classifier</span> <span class="k">if</span> <span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
                             <span class="k">else</span> <span class="n">is_regressor</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">est</span> <span class="ow">in</span> <span class="n">estimators</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">est</span> <span class="o">!=</span> <span class="s1">&#39;drop&#39;</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_estimator_type</span><span class="p">(</span><span class="n">est</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;The estimator </span><span class="si">{}</span><span class="s2"> should be a </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">est</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">is_estimator_type</span><span class="o">.</span><span class="vm">__name__</span><span class="p">[</span><span class="mi">3</span><span class="p">:]</span>
                    <span class="p">)</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="n">names</span><span class="p">,</span> <span class="n">estimators</span>

    <span class="k">def</span> <span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the parameters of an estimator from the ensemble.</span>
<span class="sd">        Valid parameter keys can be listed with `get_params()`. Note that you</span>
<span class="sd">        can directly set the parameters of the estimators contained in</span>
<span class="sd">        `estimators`.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        **params : keyword arguments</span>
<span class="sd">            Specific parameters using e.g.</span>
<span class="sd">            `set_params(parameter_name=new_value)`. In addition, to setting the</span>
<span class="sd">            parameters of the estimator, the individual estimator of the</span>
<span class="sd">            estimators can also be set, or can be removed by setting them to</span>
<span class="sd">            &#39;drop&#39;.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_set_params</span><span class="p">(</span><span class="s1">&#39;estimators&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">get_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">deep</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the parameters of an estimator from the ensemble.</span>
<span class="sd">        Returns the parameters given in the constructor as well as the</span>
<span class="sd">        estimators contained within the `estimators` parameter.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        deep : bool, default=True</span>
<span class="sd">            Setting it to True gets the various estimators and the parameters</span>
<span class="sd">            of the estimators as well.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_get_params</span><span class="p">(</span><span class="s1">&#39;estimators&#39;</span><span class="p">,</span> <span class="n">deep</span><span class="o">=</span><span class="n">deep</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">VerboseReporter</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Reports verbose output to stdout.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    verbose : int</span>
<span class="sd">        Verbosity level. If ``verbose==1`` output is printed once in a while</span>
<span class="sd">        (when iteration mod verbose_mod is zero).; if larger than 1 then output</span>
<span class="sd">        is printed for each update.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">verbose</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>

    <span class="k">def</span> <span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">est</span><span class="p">,</span> <span class="n">begin_at_stage</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize reporter</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        est : Estimator</span>
<span class="sd">            The estimator</span>
<span class="sd">        begin_at_stage : int, default=0</span>
<span class="sd">            stage at which to begin reporting</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># header fields and line format str</span>
        <span class="n">header_fields</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Iter&#39;</span><span class="p">,</span> <span class="s1">&#39;Train Loss&#39;</span><span class="p">]</span>
        <span class="n">verbose_fmt</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;</span><span class="si">{iter:&gt;10d}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">{train_score:&gt;16.4f}</span><span class="s1">&#39;</span><span class="p">]</span>
        <span class="c1"># do oob?</span>
        <span class="k">if</span> <span class="n">est</span><span class="o">.</span><span class="n">subsample</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">header_fields</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;OOB Improve&#39;</span><span class="p">)</span>
            <span class="n">verbose_fmt</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{oob_impr:&gt;16.4f}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">header_fields</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;Remaining Time&#39;</span><span class="p">)</span>
        <span class="n">verbose_fmt</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{remaining_time:&gt;16s}</span><span class="s1">&#39;</span><span class="p">)</span>

        <span class="c1"># print the header line</span>
        <span class="nb">print</span><span class="p">((</span><span class="s1">&#39;</span><span class="si">%10s</span><span class="s1"> &#39;</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">%16s</span><span class="s1"> &#39;</span> <span class="o">*</span>
               <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">header_fields</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">%</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">header_fields</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">verbose_fmt</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">verbose_fmt</span><span class="p">)</span>
        <span class="c1"># plot verbose info each time i % verbose_mod == 0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose_mod</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">begin_at_stage</span> <span class="o">=</span> <span class="n">begin_at_stage</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">est</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Update reporter with new iteration.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        j : int</span>
<span class="sd">            The new iteration.</span>
<span class="sd">        est : Estimator</span>
<span class="sd">            The estimator.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">do_oob</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">subsample</span> <span class="o">&lt;</span> <span class="mi">1</span>
        <span class="c1"># we need to take into account if we fit additional estimators.</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">j</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_at_stage</span>  <span class="c1"># iteration relative to the start iter</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose_mod</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">oob_impr</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">oob_improvement_</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="k">if</span> <span class="n">do_oob</span> <span class="k">else</span> <span class="mi">0</span>
            <span class="n">remaining_time</span> <span class="o">=</span> <span class="p">((</span><span class="n">est</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">-</span> <span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span>
                              <span class="p">(</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_time</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">remaining_time</span> <span class="o">&gt;</span> <span class="mi">60</span><span class="p">:</span>
                <span class="n">remaining_time</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{0:.2f}</span><span class="s1">m&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">remaining_time</span> <span class="o">/</span> <span class="mf">60.0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">remaining_time</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{0:.2f}</span><span class="s1">s&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">remaining_time</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose_fmt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">iter</span><span class="o">=</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                                          <span class="n">train_score</span><span class="o">=</span><span class="n">est</span><span class="o">.</span><span class="n">train_score_</span><span class="p">[</span><span class="n">j</span><span class="p">],</span>
                                          <span class="n">oob_impr</span><span class="o">=</span><span class="n">oob_impr</span><span class="p">,</span>
                                          <span class="n">remaining_time</span><span class="o">=</span><span class="n">remaining_time</span><span class="p">))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="p">((</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose_mod</span> <span class="o">*</span> <span class="mi">10</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
                <span class="c1"># adjust verbose frequency (powers of 10)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">verbose_mod</span> <span class="o">*=</span> <span class="mi">10</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="gradient-boosting-losses">
<h2>Gradient Boosting Losses<a class="headerlink" href="#gradient-boosting-losses" title="Permalink to this headline">¶</a></h2>
<p>Losses and corresponding default initial estimators for gradient boosting decision trees.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LossFunction</span><span class="p">(</span><span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Abstract base class for various loss functions.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_classes : int</span>
<span class="sd">        Number of classes.</span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    K : int</span>
<span class="sd">        The number of regression trees to be induced;</span>
<span class="sd">        1 for regression and binary classification;</span>
<span class="sd">        ``n_classes`` for multi-class classification.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">is_multi_class</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">constraints</span><span class="p">,</span> <span class="n">n_constraints</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="n">n_classes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">constraints</span> <span class="o">=</span> <span class="n">constraints</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_constraints</span> <span class="o">=</span> <span class="n">n_constraints</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alphas</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">init_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Default ``init`` estimator for loss function. &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_losses</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;primary_objective&#39;</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_constraints</span><span class="p">):</span>
            <span class="n">cols</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;sub_objective_</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">cols</span><span class="p">)</span>\
                            <span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>\
                            <span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;index&#39;</span><span class="p">:</span> <span class="s1">&#39;n_estimators&#39;</span><span class="p">})</span>

    <span class="c1"># def plot_losses(self, index=None):</span>
    <span class="c1">#     if index == None:</span>
    <span class="c1">#         losses = self.get_losses()</span>

    <span class="k">def</span> <span class="nf">get_alphas</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">cols</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_constraints</span><span class="p">):</span>
            <span class="n">cols</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;sub-objective-</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alphas</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">cols</span><span class="p">)</span>\
                            <span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>\
                            <span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;index&#39;</span><span class="p">:</span> <span class="s1">&#39;n_estimators&#39;</span><span class="p">})</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the loss.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        y : ndarray of shape (n_samples,)</span>
<span class="sd">            True labels.</span>
<span class="sd">        raw_predictions : ndarray of shape (n_samples, K)</span>
<span class="sd">            The raw predictions (i.e. values from the tree leaves).</span>
<span class="sd">        sample_weight : ndarray of shape (n_samples,), default=None</span>
<span class="sd">            Sample weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">negative_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="o">**</span><span class="n">kargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the negative gradient.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        y : ndarray of shape (n_samples,)</span>
<span class="sd">            The target labels.</span>
<span class="sd">        raw_predictions : ndarray of shape (n_samples, K)</span>
<span class="sd">            The raw predictions (i.e. values from the tree leaves) of the</span>
<span class="sd">            tree ensemble at iteration ``i - 1``.</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">update_terminal_regions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">residual</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span>
                                <span class="n">sample_weight</span><span class="p">,</span> <span class="n">sample_mask</span><span class="p">,</span>
                                <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Update the terminal regions (=leaves) of the given tree and</span>
<span class="sd">        updates the current predictions of the model. Traverses tree</span>
<span class="sd">        and invokes template method `_update_terminal_region`.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        tree : tree.Tree</span>
<span class="sd">            The tree object.</span>
<span class="sd">        X : ndarray of shape (n_samples, n_features)</span>
<span class="sd">            The data array.</span>
<span class="sd">        y : ndarray of shape (n_samples,)</span>
<span class="sd">            The target labels.</span>
<span class="sd">        residual : ndarray of shape (n_samples,)</span>
<span class="sd">            The residuals (usually the negative gradient).</span>
<span class="sd">        raw_predictions : ndarray of shape (n_samples, K)</span>
<span class="sd">            The raw predictions (i.e. values from the tree leaves) of the</span>
<span class="sd">            tree ensemble at iteration ``i - 1``.</span>
<span class="sd">        sample_weight : ndarray of shape (n_samples,)</span>
<span class="sd">            The weight of each sample.</span>
<span class="sd">        sample_mask : ndarray of shape (n_samples,)</span>
<span class="sd">            The sample mask to be used.</span>
<span class="sd">        learning_rate : float, default=0.1</span>
<span class="sd">            Learning rate shrinks the contribution of each tree by</span>
<span class="sd">             ``learning_rate``.</span>
<span class="sd">        k : int, default=0</span>
<span class="sd">            The index of the estimator being updated.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># compute leaf for each sample in ``X``.</span>
        <span class="n">terminal_regions</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># mask all which are not in sample mask.</span>
        <span class="n">masked_terminal_regions</span> <span class="o">=</span> <span class="n">terminal_regions</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">masked_terminal_regions</span><span class="p">[</span><span class="o">~</span><span class="n">sample_mask</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

        <span class="c1"># update each leaf (= perform line search)</span>
        <span class="k">for</span> <span class="n">leaf</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">children_left</span> <span class="o">==</span> <span class="n">TREE_LEAF</span><span class="p">)[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_update_terminal_region</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">masked_terminal_regions</span><span class="p">,</span>
                                         <span class="n">leaf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">residual</span><span class="p">,</span>
                                         <span class="n">raw_predictions</span><span class="p">[:,</span> <span class="n">k</span><span class="p">],</span> <span class="n">sample_weight</span><span class="p">)</span>

        <span class="c1"># update predictions (both in-bag and out-of-bag)</span>
        <span class="n">raw_predictions</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span> <span class="o">+=</span> \
            <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">tree</span><span class="o">.</span><span class="n">value</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_regions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># update dual variables in augmentend lagrangian</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_constraints</span><span class="p">:</span>
            <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">alphas</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">y_0</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
            
            <span class="n">primary_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_loss_function_value</span><span class="p">(</span><span class="n">y_0</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">)</span>
            <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">primary_loss</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">constraint</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">constraints</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">y_i</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>

                <span class="n">sub_loss_i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_loss_function_value</span><span class="p">(</span><span class="n">y_i</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">)</span>

                <span class="n">alpha_i</span> <span class="o">=</span> <span class="n">constraint</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">]</span>
                <span class="n">b_i</span> <span class="o">=</span> <span class="n">constraint</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span>
                <span class="n">mu_i</span> <span class="o">=</span> <span class="n">constraint</span><span class="p">[</span><span class="s1">&#39;mu&#39;</span><span class="p">]</span>

                <span class="k">if</span> <span class="n">sub_loss_i</span> <span class="o">&lt;</span> <span class="n">b_i</span><span class="p">:</span>
                    <span class="n">alpha_i</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">alpha_i</span> <span class="o">=</span> <span class="n">mu_i</span> <span class="o">*</span> <span class="p">(</span><span class="n">sub_loss_i</span> <span class="o">-</span> <span class="n">b_i</span><span class="p">)</span> <span class="o">+</span> <span class="n">alpha_i</span>

                <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sub_loss_i</span><span class="p">)</span>
                <span class="n">alphas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">alpha_i</span><span class="p">)</span>
                
                <span class="bp">self</span><span class="o">.</span><span class="n">constraints</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;alpha&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">alpha_i</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">alphas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">alphas</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">primary_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_loss_function_value</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">primary_loss</span><span class="p">])</span>


    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_get_loss_function_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Template method to return loss function value of GBT at current step&quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_update_terminal_region</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">terminal_regions</span><span class="p">,</span> <span class="n">leaf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                <span class="n">residual</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Template method for updating terminal regions (i.e., leaves).&quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">get_init_raw_predictions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">estimator</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the initial raw predictions.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : ndarray of shape (n_samples, n_features)</span>
<span class="sd">            The data array.</span>
<span class="sd">        estimator : object</span>
<span class="sd">            The estimator to use to compute the predictions.</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        raw_predictions : ndarray of shape (n_samples, K)</span>
<span class="sd">            The initial raw predictions. K is equal to 1 for binary</span>
<span class="sd">            classification and regression, and equal to the number of classes</span>
<span class="sd">            for multiclass classification. ``raw_predictions`` is casted</span>
<span class="sd">            into float64.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RegressionLossFunction</span><span class="p">(</span><span class="n">LossFunction</span><span class="p">,</span> <span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base class for regression loss functions.&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">constraints</span><span class="p">,</span> <span class="n">n_constraints</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">n_classes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">constraints</span><span class="o">=</span><span class="n">constraints</span><span class="p">,</span> <span class="n">n_constraints</span><span class="o">=</span><span class="n">n_constraints</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">check_init_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">estimator</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Make sure estimator has the required fit and predict methods.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        estimator : object</span>
<span class="sd">            The init estimator to check.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="s1">&#39;fit&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="s1">&#39;predict&#39;</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The init parameter must be a valid estimator and &quot;</span>
                <span class="s2">&quot;support both fit and predict.&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_init_raw_predictions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">estimator</span><span class="p">):</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">predictions</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LeastSquaresError</span><span class="p">(</span><span class="n">RegressionLossFunction</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Loss function for least squares (LS) estimation.</span>
<span class="sd">    Terminal regions do not need to be updated for least squares.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_classes : int</span>
<span class="sd">        Number of classes.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">init_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">DummyRegressor</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the least squares loss.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        y : ndarray of shape (n_samples,)</span>
<span class="sd">            True labels.</span>
<span class="sd">        raw_predictions : ndarray of shape (n_samples, K)</span>
<span class="sd">            The raw predictions (i.e. values from the tree leaves).</span>
<span class="sd">        sample_weight : ndarray of shape (n_samples,), default=None</span>
<span class="sd">            Sample weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
                <span class="n">sample_weight</span> <span class="o">*</span> <span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">negative_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="o">**</span><span class="n">kargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute half of the negative gradient.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        y : ndarray of shape (n_samples,)</span>
<span class="sd">            The target labels.</span>
<span class="sd">        raw_predictions : ndarray of shape (n_samples,)</span>
<span class="sd">            The raw predictions (i.e. values from the tree leaves) of the</span>
<span class="sd">            tree ensemble at iteration ``i - 1``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_constraints</span><span class="p">:</span>
            <span class="n">y_0</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="n">y_0</span> <span class="o">-</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">constraint</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">constraints</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">y_i</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>
                <span class="n">alpha_i</span> <span class="o">=</span> <span class="n">constraint</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">]</span>
                <span class="n">residual</span> <span class="o">+=</span> <span class="n">alpha_i</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_i</span> <span class="o">-</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
            <span class="k">return</span> <span class="n">residual</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Y</span> <span class="o">-</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="c1"># def update_terminal_regions(self, tree, X, Y, residual, raw_predictions,</span>
    <span class="c1">#                             sample_weight, sample_mask,</span>
    <span class="c1">#                             learning_rate=0.1, k=0):</span>
    <span class="c1">#     &quot;&quot;&quot;Least squares does not need to update terminal regions.</span>

    <span class="c1">#     But it has to update the predictions.</span>

    <span class="c1">#     Parameters</span>
    <span class="c1">#     ----------</span>
    <span class="c1">#     tree : tree.Tree</span>
    <span class="c1">#         The tree object.</span>
    <span class="c1">#     X : ndarray of shape (n_samples, n_features)</span>
    <span class="c1">#         The data array.</span>
    <span class="c1">#     y : ndarray of shape (n_samples,)</span>
    <span class="c1">#         The target labels.</span>
    <span class="c1">#     residual : ndarray of shape (n_samples,)</span>
    <span class="c1">#         The residuals (usually the negative gradient).</span>
    <span class="c1">#     raw_predictions : ndarray of shape (n_samples, K)</span>
    <span class="c1">#         The raw predictions (i.e. values from the tree leaves) of the</span>
    <span class="c1">#         tree ensemble at iteration ``i - 1``.</span>
    <span class="c1">#     sample_weight : ndarray of shape (n,)</span>
    <span class="c1">#         The weight of each sample.</span>
    <span class="c1">#     sample_mask : ndarray of shape (n,)</span>
    <span class="c1">#         The sample mask to be used.</span>
    <span class="c1">#     learning_rate : float, default=0.1</span>
    <span class="c1">#         Learning rate shrinks the contribution of each tree by</span>
    <span class="c1">#          ``learning_rate``.</span>
    <span class="c1">#     k : int, default=0</span>
    <span class="c1">#         The index of the estimator being updated.</span>
    <span class="c1">#     &quot;&quot;&quot;</span>
    <span class="c1">#     if self.n_constraints:</span>
    <span class="c1">#         terminal_regions = tree.apply(X)</span>
    <span class="c1">#         y_0 = Y[:, 0]</span>
    <span class="c1">#         y_0 = y_0.take(terminal_region, axis=0)</span>
    <span class="c1">#     else:</span>
    <span class="c1">#         # update predictions</span>
    <span class="c1">#         raw_predictions[:, k] += learning_rate * tree.predict(X).ravel()</span>

    <span class="k">def</span> <span class="nf">_update_terminal_region</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">terminal_regions</span><span class="p">,</span> <span class="n">leaf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span>
                                <span class="n">residual</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
        
        <span class="n">terminal_region</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">terminal_regions</span> <span class="o">==</span> <span class="n">leaf</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">raw_pred</span> <span class="o">=</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_constraints</span><span class="p">:</span>
            <span class="n">y_0</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
            <span class="n">y_0</span> <span class="o">=</span> <span class="n">y_0</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

            <span class="n">leaf_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_0</span> <span class="o">-</span> <span class="n">raw_pred</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">constraint</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">constraints</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">alpha_i</span> <span class="o">=</span> <span class="n">constraint</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">]</span>
                
                <span class="n">y_i</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>
                <span class="n">y_i</span> <span class="o">=</span> <span class="n">y_i</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                
                <span class="n">leaf_val</span> <span class="o">+=</span> <span class="n">alpha_i</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_i</span> <span class="o">-</span> <span class="n">raw_pred</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">leaf_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">raw_pred</span><span class="p">)</span>
        
        <span class="n">tree</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="n">leaf</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">leaf_val</span>
        


    <span class="k">def</span> <span class="nf">_get_loss_function_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;return binomial deviance value based on current prediction&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># return -np.mean(y * raw_predictions.ravel() - \</span>
        <span class="c1">#     np.log(1 + np.exp(raw_predictions.ravel())))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LeastAbsoluteError</span><span class="p">(</span><span class="n">RegressionLossFunction</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Loss function for least absolute deviation (LAD) regression.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_classes : int</span>
<span class="sd">        Number of classes</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">init_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">DummyRegressor</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;quantile&#39;</span><span class="p">,</span> <span class="n">quantile</span><span class="o">=</span><span class="mf">.5</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the least absolute error.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        y : ndarray of shape (n_samples,)</span>
<span class="sd">            True labels.</span>
<span class="sd">        raw_predictions : ndarray of shape (n_samples, K)</span>
<span class="sd">            The raw predictions (i.e. values from the tree leaves).</span>
<span class="sd">        sample_weight : ndarray of shape (n_samples,), default=None</span>
<span class="sd">            Sample weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
                <span class="n">sample_weight</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">())))</span>

    <span class="k">def</span> <span class="nf">negative_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="o">**</span><span class="n">kargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the negative gradient.</span>
<span class="sd">        1.0 if y - raw_predictions &gt; 0.0 else -1.0</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        y : ndarray of shape (n_samples,)</span>
<span class="sd">            The target labels.</span>
<span class="sd">        raw_predictions : ndarray of shape (n_samples, K)</span>
<span class="sd">            The raw predictions (i.e. values from the tree leaves) of the</span>
<span class="sd">            tree ensemble at iteration ``i - 1``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">raw_predictions</span> <span class="o">=</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">raw_predictions</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">_update_terminal_region</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">terminal_regions</span><span class="p">,</span> <span class="n">leaf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                <span class="n">residual</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;LAD updates terminal regions to median estimates.&quot;&quot;&quot;</span>
        <span class="n">terminal_region</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">terminal_regions</span> <span class="o">==</span> <span class="n">leaf</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span>
                <span class="n">raw_predictions</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
        <span class="n">tree</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="n">leaf</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">_weighted_percentile</span><span class="p">(</span><span class="n">diff</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span>
                                                      <span class="n">percentile</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">HuberLossFunction</span><span class="p">(</span><span class="n">RegressionLossFunction</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Huber loss function for robust regression.</span>
<span class="sd">    M-Regression proposed in Friedman 2001.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    alpha : float, default=0.9</span>
<span class="sd">        Percentile at which to extract score.</span>
<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    J. Friedman, Greedy Function Approximation: A Gradient Boosting</span>
<span class="sd">    Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">init_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">DummyRegressor</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;quantile&#39;</span><span class="p">,</span> <span class="n">quantile</span><span class="o">=</span><span class="mf">.5</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the Huber loss.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        y : ndarray of shape (n_samples,)</span>
<span class="sd">            True labels.</span>
<span class="sd">        raw_predictions : ndarray of shape (n_samples, K)</span>
<span class="sd">            The raw predictions (i.e. values from the tree leaves) of the</span>
<span class="sd">            tree ensemble.</span>
<span class="sd">        sample_weight : ndarray of shape (n_samples,), default=None</span>
<span class="sd">            Sample weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">raw_predictions</span> <span class="o">=</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">raw_predictions</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span>
        <span class="k">if</span> <span class="n">gamma</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">gamma</span> <span class="o">=</span> <span class="n">_weighted_percentile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">),</span> <span class="n">sample_weight</span><span class="p">,</span>
                                             <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span>

        <span class="n">gamma_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">gamma</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sq_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">diff</span><span class="p">[</span><span class="n">gamma_mask</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">lin_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">gamma</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">[</span><span class="o">~</span><span class="n">gamma_mask</span><span class="p">])</span> <span class="o">-</span>
                                       <span class="n">gamma</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">sq_loss</span> <span class="o">+</span> <span class="n">lin_loss</span><span class="p">)</span> <span class="o">/</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sq_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">sample_weight</span><span class="p">[</span><span class="n">gamma_mask</span><span class="p">]</span> <span class="o">*</span>
                             <span class="n">diff</span><span class="p">[</span><span class="n">gamma_mask</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">lin_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">sample_weight</span><span class="p">[</span><span class="o">~</span><span class="n">gamma_mask</span><span class="p">]</span> <span class="o">*</span>
                              <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">[</span><span class="o">~</span><span class="n">gamma_mask</span><span class="p">])</span> <span class="o">-</span> <span class="n">gamma</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">sq_loss</span> <span class="o">+</span> <span class="n">lin_loss</span><span class="p">)</span> <span class="o">/</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">negative_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="o">**</span><span class="n">kargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the negative gradient.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        y : ndarray of shape (n_samples,)</span>
<span class="sd">            The target labels.</span>
<span class="sd">        raw_predictions : ndarray of shape (n_samples, K)</span>
<span class="sd">            The raw predictions (i.e. values from the tree leaves) of the</span>
<span class="sd">            tree ensemble at iteration ``i - 1``.</span>
<span class="sd">        sample_weight : ndarray of shape (n_samples,), default=None</span>
<span class="sd">            Sample weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">raw_predictions</span> <span class="o">=</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">raw_predictions</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">gamma</span> <span class="o">=</span> <span class="n">_weighted_percentile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">),</span> <span class="n">sample_weight</span><span class="p">,</span>
                                         <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span>
        <span class="n">gamma_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">gamma</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">residual</span><span class="p">[</span><span class="n">gamma_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">diff</span><span class="p">[</span><span class="n">gamma_mask</span><span class="p">]</span>
        <span class="n">residual</span><span class="p">[</span><span class="o">~</span><span class="n">gamma_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">diff</span><span class="p">[</span><span class="o">~</span><span class="n">gamma_mask</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="k">return</span> <span class="n">residual</span>

    <span class="k">def</span> <span class="nf">_update_terminal_region</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">terminal_regions</span><span class="p">,</span> <span class="n">leaf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                <span class="n">residual</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
        <span class="n">terminal_region</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">terminal_regions</span> <span class="o">==</span> <span class="n">leaf</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="o">-</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
        <span class="n">median</span> <span class="o">=</span> <span class="n">_weighted_percentile</span><span class="p">(</span><span class="n">diff</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">percentile</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
        <span class="n">diff_minus_median</span> <span class="o">=</span> <span class="n">diff</span> <span class="o">-</span> <span class="n">median</span>
        <span class="n">tree</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="n">leaf</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">median</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">diff_minus_median</span><span class="p">)</span> <span class="o">*</span>
            <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff_minus_median</span><span class="p">),</span> <span class="n">gamma</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">QuantileLossFunction</span><span class="p">(</span><span class="n">RegressionLossFunction</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Loss function for quantile regression.</span>
<span class="sd">    Quantile regression allows to estimate the percentiles</span>
<span class="sd">    of the conditional distribution of the target.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    alpha : float, default=0.9</span>
<span class="sd">        The percentile.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">percentile</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="mi">100</span>

    <span class="k">def</span> <span class="nf">init_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">DummyRegressor</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;quantile&#39;</span><span class="p">,</span> <span class="n">quantile</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the Quantile loss.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        y : ndarray of shape (n_samples,)</span>
<span class="sd">            True labels.</span>
<span class="sd">        raw_predictions : ndarray of shape (n_samples, K)</span>
<span class="sd">            The raw predictions (i.e. values from the tree leaves) of the</span>
<span class="sd">            tree ensemble.</span>
<span class="sd">        sample_weight : ndarray of shape (n_samples,), default=None</span>
<span class="sd">            Sample weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">raw_predictions</span> <span class="o">=</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">raw_predictions</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span>

        <span class="n">mask</span> <span class="o">=</span> <span class="n">y</span> <span class="o">&gt;</span> <span class="n">raw_predictions</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">diff</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">-</span>
                    <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">diff</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span> <span class="o">/</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="p">((</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">*</span> <span class="n">diff</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span> <span class="o">-</span>
                    <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">]</span> <span class="o">*</span>
                                         <span class="n">diff</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">]))</span> <span class="o">/</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">negative_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="o">**</span><span class="n">kargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the negative gradient.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        y : ndarray of shape (n_samples,)</span>
<span class="sd">            The target labels.</span>
<span class="sd">        raw_predictions : ndarray of shape (n_samples, K)</span>
<span class="sd">            The raw predictions (i.e. values from the tree leaves) of the</span>
<span class="sd">            tree ensemble at iteration ``i - 1``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span>
        <span class="n">raw_predictions</span> <span class="o">=</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">y</span> <span class="o">&gt;</span> <span class="n">raw_predictions</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">mask</span><span class="p">)</span> <span class="o">-</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="o">~</span><span class="n">mask</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_update_terminal_region</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">terminal_regions</span><span class="p">,</span> <span class="n">leaf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                <span class="n">residual</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
        <span class="n">terminal_region</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">terminal_regions</span> <span class="o">==</span> <span class="n">leaf</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="o">-</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">val</span> <span class="o">=</span> <span class="n">_weighted_percentile</span><span class="p">(</span><span class="n">diff</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">percentile</span><span class="p">)</span>
        <span class="n">tree</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="n">leaf</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ClassificationLossFunction</span><span class="p">(</span><span class="n">LossFunction</span><span class="p">,</span> <span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base class for classification loss functions. &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_raw_prediction_to_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Template method to convert raw predictions into probabilities.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        raw_predictions : ndarray of shape (n_samples, K)</span>
<span class="sd">            The raw predictions (i.e. values from the tree leaves) of the</span>
<span class="sd">            tree ensemble.</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        probas : ndarray of shape (n_samples, K)</span>
<span class="sd">            The predicted probabilities.</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_raw_prediction_to_decision</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Template method to convert raw predictions to decisions.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        raw_predictions : ndarray of shape (n_samples, K)</span>
<span class="sd">            The raw predictions (i.e. values from the tree leaves) of the</span>
<span class="sd">            tree ensemble.</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        encoded_predictions : ndarray of shape (n_samples, K)</span>
<span class="sd">            The predicted encoded labels.</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">check_init_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">estimator</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Make sure estimator has fit and predict_proba methods.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        estimator : object</span>
<span class="sd">            The init estimator to check.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="s1">&#39;fit&#39;</span><span class="p">)</span> <span class="ow">and</span>
                <span class="nb">hasattr</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="s1">&#39;predict_proba&#39;</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The init parameter must be a valid estimator &quot;</span>
                <span class="s2">&quot;and support both fit and predict_proba.&quot;</span>
            <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">BinomialDeviance</span><span class="p">(</span><span class="n">ClassificationLossFunction</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Binomial deviance loss function for binary classification.</span>
<span class="sd">    Binary classification is a special case; here, we only need to</span>
<span class="sd">    fit one tree instead of ``n_classes`` trees.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_classes : int</span>
<span class="sd">        Number of classes.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">constraints</span><span class="p">,</span> <span class="n">n_constraints</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">n_classes</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{0:s}</span><span class="s2"> requires 2 classes; got </span><span class="si">{1:d}</span><span class="s2"> class(es)&quot;</span>
                             <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">))</span>
        <span class="c1"># we only need to fit one tree for binary clf.</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">n_classes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">constraints</span><span class="o">=</span><span class="n">constraints</span><span class="p">,</span> <span class="n">n_constraints</span><span class="o">=</span><span class="n">n_constraints</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">init_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># return the most common class, taking into account the samples</span>
        <span class="c1"># weights</span>
        <span class="k">return</span> <span class="n">DummyClassifier</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;prior&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the deviance (= 2 * negative log-likelihood).</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        y : ndarray of shape (n_samples,)</span>
<span class="sd">            True labels.</span>
<span class="sd">        raw_predictions : ndarray of shape (n_samples, K)</span>
<span class="sd">            The raw predictions (i.e. values from the tree leaves) of the</span>
<span class="sd">            tree ensemble.</span>
<span class="sd">        sample_weight : ndarray of shape (n_samples,), default=None</span>
<span class="sd">            Sample weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># logaddexp(0, v) == log(1.0 + exp(v))</span>
        <span class="n">raw_predictions</span> <span class="o">=</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span> <span class="o">*</span> <span class="n">raw_predictions</span><span class="p">)</span> <span class="o">-</span>
                                <span class="n">np</span><span class="o">.</span><span class="n">logaddexp</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">/</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
                <span class="n">sample_weight</span> <span class="o">*</span> <span class="p">((</span><span class="n">y</span> <span class="o">*</span> <span class="n">raw_predictions</span><span class="p">)</span> <span class="o">-</span>
                                 <span class="n">np</span><span class="o">.</span><span class="n">logaddexp</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">))))</span>

    <span class="k">def</span> <span class="nf">negative_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="o">**</span><span class="n">kargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute half of the negative gradient.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        y : ndarray of shape (n_samples,)</span>
<span class="sd">            True labels.</span>
<span class="sd">        raw_predictions : ndarray of shape (n_samples, K)</span>
<span class="sd">            The raw predictions (i.e. values from the tree leaves) of the</span>
<span class="sd">            tree ensemble at iteration ``i - 1``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_constraints</span><span class="p">:</span>
            <span class="n">y_0</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="n">y_0</span> <span class="o">-</span> <span class="n">expit</span><span class="p">(</span><span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">constraint</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">constraints</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">y_i</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>
                <span class="n">alpha_i</span> <span class="o">=</span> <span class="n">constraint</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">]</span>
                <span class="n">residual</span> <span class="o">+=</span> <span class="n">alpha_i</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_i</span> <span class="o">-</span> <span class="n">expit</span><span class="p">(</span><span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">()))</span>
            <span class="k">return</span> <span class="n">residual</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Y</span> <span class="o">-</span> <span class="n">expit</span><span class="p">(</span><span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">_update_terminal_region</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">terminal_regions</span><span class="p">,</span> <span class="n">leaf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span>
                                <span class="n">residual</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Make a single Newton-Raphson step.</span>
<span class="sd">        our node estimate is given by:</span>
<span class="sd">            sum(w * (y - prob)) / sum(w * prob * (1 - prob))</span>
<span class="sd">        we take advantage that: y - prob = residual</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">terminal_region</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">terminal_regions</span> <span class="o">==</span> <span class="n">leaf</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_constraints</span><span class="p">:</span>
            <span class="n">raw_pred</span> <span class="o">=</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">y_0</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
            <span class="n">y_0</span> <span class="o">=</span> <span class="n">y_0</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

            <span class="c1"># residual for primary objective</span>
            <span class="n">numerator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_0</span> <span class="o">-</span> <span class="n">expit</span><span class="p">(</span><span class="n">raw_pred</span><span class="o">.</span><span class="n">ravel</span><span class="p">()))</span>

            <span class="c1"># submission (1 - p) * p</span>
            <span class="n">den</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span> <span class="n">expit</span><span class="p">(</span><span class="n">raw_pred</span><span class="o">.</span><span class="n">ravel</span><span class="p">()))</span> <span class="o">*</span> <span class="n">expit</span><span class="p">(</span><span class="n">raw_pred</span><span class="o">.</span><span class="n">ravel</span><span class="p">()))</span>
            
            <span class="n">denominator</span> <span class="o">=</span> <span class="n">den</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">constraint</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">constraints</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">alpha_i</span> <span class="o">=</span> <span class="n">constraint</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">]</span>
                <span class="n">y_i</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>
                <span class="n">y_i</span> <span class="o">=</span> <span class="n">y_i</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

                <span class="n">numerator</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">alpha_i</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_i</span> <span class="o">-</span> <span class="n">expit</span><span class="p">(</span><span class="n">raw_pred</span><span class="o">.</span><span class="n">ravel</span><span class="p">())))</span>
                <span class="n">denominator</span> <span class="o">+=</span> <span class="n">alpha_i</span> <span class="o">*</span> <span class="n">den</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="n">residual</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

            <span class="n">numerator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span> <span class="o">*</span> <span class="n">residual</span><span class="p">)</span>
            <span class="n">denominator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span> <span class="o">*</span>
                                <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">residual</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Y</span> <span class="o">+</span> <span class="n">residual</span><span class="p">))</span>

        <span class="c1"># prevents overflow and division by zero </span>
        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">denominator</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-150</span><span class="p">:</span>
            <span class="n">tree</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="n">leaf</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tree</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="n">leaf</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>
    
    <span class="k">def</span> <span class="nf">_get_loss_function_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;return binomial deviance value based on current prediction&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="o">-</span> \
            <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">())))</span>

    <span class="k">def</span> <span class="nf">_raw_prediction_to_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">):</span>
        <span class="n">proba</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">raw_predictions</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">proba</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">expit</span><span class="p">(</span><span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
        <span class="n">proba</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-=</span> <span class="n">proba</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">proba</span>

    <span class="k">def</span> <span class="nf">_raw_prediction_to_decision</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">):</span>
        <span class="n">proba</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_prediction_to_proba</span><span class="p">(</span><span class="n">raw_predictions</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">proba</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_init_raw_predictions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">estimator</span><span class="p">):</span>
        <span class="n">probas</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">proba_pos_class</span> <span class="o">=</span> <span class="n">probas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
        <span class="n">proba_pos_class</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">proba_pos_class</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">eps</span><span class="p">)</span>
        <span class="c1"># log(x / (1 - x)) is the inverse of the sigmoid (expit) function</span>
        <span class="n">raw_predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">proba_pos_class</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">proba_pos_class</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MultinomialDeviance</span><span class="p">(</span><span class="n">ClassificationLossFunction</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Multinomial deviance loss function for multi-class classification.</span>
<span class="sd">    For multi-class classification we need to fit ``n_classes`` trees at</span>
<span class="sd">    each stage.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_classes : int</span>
<span class="sd">        Number of classes.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">is_multi_class</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">n_classes</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{0:s}</span><span class="s2"> requires more than 2 classes.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">n_classes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">init_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">DummyClassifier</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;prior&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the Multinomial deviance.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        y : ndarray of shape (n_samples,)</span>
<span class="sd">            True labels.</span>
<span class="sd">        raw_predictions : ndarray of shape (n_samples, K)</span>
<span class="sd">            The raw predictions (i.e. values from the tree leaves) of the</span>
<span class="sd">            tree ensemble.</span>
<span class="sd">        sample_weight : ndarray of shape (n_samples,), default=None</span>
<span class="sd">            Sample weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># create one-hot label encoding</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">):</span>
            <span class="n">Y</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span> <span class="o">==</span> <span class="n">k</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span>
            <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="p">(</span><span class="n">Y</span> <span class="o">*</span> <span class="n">raw_predictions</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span>
            <span class="n">logsumexp</span><span class="p">(</span><span class="n">raw_predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">weights</span><span class="o">=</span><span class="n">sample_weight</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">negative_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute negative gradient for the ``k``-th class.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        y : ndarray of shape (n_samples,)</span>
<span class="sd">            The target labels.</span>
<span class="sd">        raw_predictions : ndarray of shape (n_samples, K)</span>
<span class="sd">            The raw predictions (i.e. values from the tree leaves) of the</span>
<span class="sd">            tree ensemble at iteration ``i - 1``.</span>
<span class="sd">        k : int, default=0</span>
<span class="sd">            The index of the class.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">raw_predictions</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span> <span class="o">-</span>
                                        <span class="n">logsumexp</span><span class="p">(</span><span class="n">raw_predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">_update_terminal_region</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">terminal_regions</span><span class="p">,</span> <span class="n">leaf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                <span class="n">residual</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Make a single Newton-Raphson step. &quot;&quot;&quot;</span>
        <span class="n">terminal_region</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">terminal_regions</span> <span class="o">==</span> <span class="n">leaf</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">residual</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">numerator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span> <span class="o">*</span> <span class="n">residual</span><span class="p">)</span>
        <span class="n">numerator</span> <span class="o">*=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span>

        <span class="n">denominator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">residual</span><span class="p">)</span> <span class="o">*</span>
                             <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span> <span class="o">+</span> <span class="n">residual</span><span class="p">))</span>

        <span class="c1"># prevents overflow and division by zero</span>
        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">denominator</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-150</span><span class="p">:</span>
            <span class="n">tree</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="n">leaf</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tree</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="n">leaf</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>

    <span class="k">def</span> <span class="nf">_raw_prediction_to_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">raw_predictions</span> <span class="o">-</span>
                   <span class="p">(</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">raw_predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])))</span>

    <span class="k">def</span> <span class="nf">_raw_prediction_to_decision</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">):</span>
        <span class="n">proba</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_prediction_to_proba</span><span class="p">(</span><span class="n">raw_predictions</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">proba</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_init_raw_predictions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">estimator</span><span class="p">):</span>
        <span class="n">probas</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
        <span class="n">probas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">probas</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">eps</span><span class="p">)</span>
        <span class="n">raw_predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">probas</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">raw_predictions</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ExponentialLoss</span><span class="p">(</span><span class="n">ClassificationLossFunction</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Exponential loss function for binary classification.</span>
<span class="sd">    Same loss as AdaBoost.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_classes : int</span>
<span class="sd">        Number of classes.</span>
<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Greg Ridgeway, Generalized Boosted Models: A guide to the gbm package, 2007</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">n_classes</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{0:s}</span><span class="s2"> requires 2 classes; got </span><span class="si">{1:d}</span><span class="s2"> class(es)&quot;</span>
                             <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">))</span>
        <span class="c1"># we only need to fit one tree for binary clf.</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">n_classes</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">init_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">DummyClassifier</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;prior&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the exponential loss</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        y : ndarray of shape (n_samples,)</span>
<span class="sd">            True labels.</span>
<span class="sd">        raw_predictions : ndarray of shape (n_samples, K)</span>
<span class="sd">            The raw predictions (i.e. values from the tree leaves) of the</span>
<span class="sd">            tree ensemble.</span>
<span class="sd">        sample_weight : ndarray of shape (n_samples,), default=None</span>
<span class="sd">            Sample weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">raw_predictions</span> <span class="o">=</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mf">2.</span> <span class="o">*</span> <span class="n">y</span> <span class="o">-</span> <span class="mf">1.</span><span class="p">)</span> <span class="o">*</span> <span class="n">raw_predictions</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
                <span class="n">sample_weight</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">y</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">raw_predictions</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">negative_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="o">**</span><span class="n">kargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the residual (= negative gradient).</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        y : ndarray of shape (n_samples,)</span>
<span class="sd">            True labels.</span>
<span class="sd">        raw_predictions : ndarray of shape (n_samples, K)</span>
<span class="sd">            The raw predictions (i.e. values from the tree leaves) of the</span>
<span class="sd">            tree ensemble at iteration ``i - 1``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">y_</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="mf">2.</span> <span class="o">*</span> <span class="n">y</span> <span class="o">-</span> <span class="mf">1.</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y_</span> <span class="o">*</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">_update_terminal_region</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">terminal_regions</span><span class="p">,</span> <span class="n">leaf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                <span class="n">residual</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
        <span class="n">terminal_region</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">terminal_regions</span> <span class="o">==</span> <span class="n">leaf</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">raw_predictions</span> <span class="o">=</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">y_</span> <span class="o">=</span> <span class="mf">2.</span> <span class="o">*</span> <span class="n">y</span> <span class="o">-</span> <span class="mf">1.</span>

        <span class="n">numerator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_</span> <span class="o">*</span> <span class="n">sample_weight</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">y_</span> <span class="o">*</span> <span class="n">raw_predictions</span><span class="p">))</span>
        <span class="n">denominator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">y_</span> <span class="o">*</span> <span class="n">raw_predictions</span><span class="p">))</span>

        <span class="c1"># prevents overflow and division by zero</span>
        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">denominator</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-150</span><span class="p">:</span>
            <span class="n">tree</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="n">leaf</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tree</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="n">leaf</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>

    <span class="k">def</span> <span class="nf">_raw_prediction_to_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">):</span>
        <span class="n">proba</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">raw_predictions</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">proba</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">expit</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
        <span class="n">proba</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-=</span> <span class="n">proba</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">proba</span>

    <span class="k">def</span> <span class="nf">_raw_prediction_to_decision</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_init_raw_predictions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">estimator</span><span class="p">):</span>
        <span class="n">probas</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">proba_pos_class</span> <span class="o">=</span> <span class="n">probas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
        <span class="n">proba_pos_class</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">proba_pos_class</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">eps</span><span class="p">)</span>
        <span class="c1"># according to The Elements of Statistical Learning sec. 10.5, the</span>
        <span class="c1"># minimizer of the exponential loss is .5 * log odds ratio. So this is</span>
        <span class="c1"># the equivalent to .5 * binomial_deviance.get_init_raw_predictions()</span>
        <span class="n">raw_predictions</span> <span class="o">=</span> <span class="mf">.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">proba_pos_class</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">proba_pos_class</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">LOSS_FUNCTIONS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;ls&#39;</span><span class="p">:</span> <span class="n">LeastSquaresError</span><span class="p">,</span>
    <span class="s1">&#39;lad&#39;</span><span class="p">:</span> <span class="n">LeastAbsoluteError</span><span class="p">,</span>
    <span class="s1">&#39;huber&#39;</span><span class="p">:</span> <span class="n">HuberLossFunction</span><span class="p">,</span>
    <span class="s1">&#39;quantile&#39;</span><span class="p">:</span> <span class="n">QuantileLossFunction</span><span class="p">,</span>
    <span class="s1">&#39;deviance&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># for both, multinomial and binomial</span>
    <span class="s1">&#39;exponential&#39;</span><span class="p">:</span> <span class="n">ExponentialLoss</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="gradient-boosted-regression-trees">
<h2>Gradient Boosted Regression Trees<a class="headerlink" href="#gradient-boosted-regression-trees" title="Permalink to this headline">¶</a></h2>
<p>This module contains methods for fitting gradient boosted regression trees for both classification and regression.</p>
<p>The module structure is the following:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">BaseGradientBoosting</span></code> base class implements a common <code class="docutils literal notranslate"><span class="pre">fit</span></code> method
for all the estimators in the module. Regression and classification
only differ in the concrete <code class="docutils literal notranslate"><span class="pre">LossFunction</span></code> used.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MooGBTClassifier</span></code> implements gradient boosting for
classification problems.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MooGBTRegressor</span></code> implements gradient boosting for
regression problems.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">BaseGradientBoosting</span><span class="p">(</span><span class="n">BaseEnsemble</span><span class="p">,</span> <span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Abstract base class for Gradient Boosting.&quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">n_estimators</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span>
                 <span class="n">min_samples_split</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="p">,</span> <span class="n">min_weight_fraction_leaf</span><span class="p">,</span>
                 <span class="n">max_depth</span><span class="p">,</span> <span class="n">min_impurity_decrease</span><span class="p">,</span> <span class="n">min_impurity_split</span><span class="p">,</span>
                 <span class="n">init</span><span class="p">,</span> <span class="n">subsample</span><span class="p">,</span> <span class="n">max_features</span><span class="p">,</span> <span class="n">ccp_alpha</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">n_iter_no_change</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">constraints</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="n">n_estimators</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span> <span class="o">=</span> <span class="n">min_samples_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">=</span> <span class="n">min_weight_fraction_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">subsample</span> <span class="o">=</span> <span class="n">subsample</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">=</span> <span class="n">max_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_decrease</span> <span class="o">=</span> <span class="n">min_impurity_decrease</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_split</span> <span class="o">=</span> <span class="n">min_impurity_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ccp_alpha</span> <span class="o">=</span> <span class="n">ccp_alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">=</span> <span class="n">init</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="n">max_leaf_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="o">=</span> <span class="n">warm_start</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validation_fraction</span> <span class="o">=</span> <span class="n">validation_fraction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_no_change</span> <span class="o">=</span> <span class="n">n_iter_no_change</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        
        <span class="n">constraints_</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">constraint</span> <span class="ow">in</span> <span class="n">constraints</span><span class="p">:</span>
            <span class="n">constraint</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">constraints_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">constraint</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">constraints</span> <span class="o">=</span> <span class="n">constraints_</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_constraints</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">constraints_</span><span class="p">)</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_validate_y</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Called by fit to validate y.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_fit_stage</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">sample_mask</span><span class="p">,</span>
                   <span class="n">random_state</span><span class="p">,</span> <span class="n">X_csc</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">X_csr</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit another stage of ``_n_classes`` trees to the boosting model.&quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="n">sample_mask</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="nb">bool</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_constraints</span><span class="p">:</span>
            <span class="n">y_0</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">y_0</span> <span class="o">=</span> <span class="n">y</span>
        <span class="n">original_y</span> <span class="o">=</span> <span class="n">y_0</span>

        <span class="c1"># Need to pass a copy of raw_predictions to negative_gradient()</span>
        <span class="c1"># because raw_predictions is partially updated at the end of the loop</span>
        <span class="c1"># in update_terminal_regions(), and gradients need to be evaluated at</span>
        <span class="c1"># iteration i - 1.</span>
        <span class="n">raw_predictions_copy</span> <span class="o">=</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">K</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">loss</span><span class="o">.</span><span class="n">is_multi_class</span><span class="p">:</span>
                <span class="n">y_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">original_y</span> <span class="o">==</span> <span class="n">k</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

            <span class="n">residual</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">negative_gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions_copy</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span>
                                              <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>

            <span class="c1"># induce regression tree on residuals</span>
            <span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span>
                <span class="n">criterion</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">,</span>
                <span class="n">splitter</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">,</span>
                <span class="n">max_depth</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span><span class="p">,</span>
                <span class="n">min_samples_split</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span><span class="p">,</span>
                <span class="n">min_samples_leaf</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span><span class="p">,</span>
                <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span><span class="p">,</span>
                <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_decrease</span><span class="p">,</span>
                <span class="n">min_impurity_split</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_split</span><span class="p">,</span>
                <span class="n">max_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_features</span><span class="p">,</span>
                <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
                <span class="n">ccp_alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ccp_alpha</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">subsample</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
                <span class="c1"># no inplace multiplication!</span>
                <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span> <span class="o">*</span> <span class="n">sample_mask</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

            <span class="n">X</span> <span class="o">=</span> <span class="n">X_csr</span> <span class="k">if</span> <span class="n">X_csr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">X</span>
            <span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">residual</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                     <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

            <span class="c1"># update tree leaves</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">update_terminal_regions</span><span class="p">(</span>
                <span class="n">tree</span><span class="o">.</span><span class="n">tree_</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">residual</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span>
                <span class="n">sample_mask</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>

            <span class="c1"># add tree to ensemble</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">tree</span>

        <span class="k">return</span> <span class="n">raw_predictions</span>

    <span class="k">def</span> <span class="nf">_check_n_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">reset</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Set the `n_features_in_` attribute, or check against it.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {ndarray, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples.</span>
<span class="sd">        reset : bool</span>
<span class="sd">            If True, the `n_features_in_` attribute is set to `X.shape[1]`.</span>
<span class="sd">            If False and the attribute exists, then check that it is equal to</span>
<span class="sd">            `X.shape[1]`. If False and the attribute does *not* exist, then</span>
<span class="sd">            the check is skipped.</span>
<span class="sd">            .. note::</span>
<span class="sd">               It is recommended to call reset=True in `fit` and in the first</span>
<span class="sd">               call to `partial_fit`. All other methods that validate `X`</span>
<span class="sd">               should set `reset=False`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">reset</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_features_in_</span> <span class="o">=</span> <span class="n">n_features</span>
            <span class="k">return</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;n_features_in_&quot;</span><span class="p">):</span>
            <span class="c1"># Skip this check if the expected number of expected input features</span>
            <span class="c1"># was not recorded by calling fit first. This is typically the case</span>
            <span class="c1"># for stateless transformers.</span>
            <span class="k">return</span>

        <span class="k">if</span> <span class="n">n_features</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features_in_</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;X has </span><span class="si">{</span><span class="n">n_features</span><span class="si">}</span><span class="s2"> features, but </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;is expecting </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features_in_</span><span class="si">}</span><span class="s2"> features as input.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validate_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;no_validation&#39;</span><span class="p">,</span> <span class="n">reset</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                       <span class="n">validate_separately</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">check_params</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Validate input data and set or check the `n_features_in_` attribute.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix, dataframe} of shape \</span>
<span class="sd">                (n_samples, n_features)</span>
<span class="sd">            The input samples.</span>
<span class="sd">        y : array-like of shape (n_samples,), default=&#39;no_validation&#39;</span>
<span class="sd">            The targets.</span>
<span class="sd">            - If `None`, `check_array` is called on `X`. If the estimator&#39;s</span>
<span class="sd">              requires_y tag is True, then an error will be raised.</span>
<span class="sd">            - If `&#39;no_validation&#39;`, `check_array` is called on `X` and the</span>
<span class="sd">              estimator&#39;s requires_y tag is ignored. This is a default</span>
<span class="sd">              placeholder and is never meant to be explicitly set.</span>
<span class="sd">            - Otherwise, both `X` and `y` are checked with either `check_array`</span>
<span class="sd">              or `check_X_y` depending on `validate_separately`.</span>
<span class="sd">        reset : bool, default=True</span>
<span class="sd">            Whether to reset the `n_features_in_` attribute.</span>
<span class="sd">            If False, the input will be checked for consistency with data</span>
<span class="sd">            provided when reset was last True.</span>
<span class="sd">            .. note::</span>
<span class="sd">               It is recommended to call reset=True in `fit` and in the first</span>
<span class="sd">               call to `partial_fit`. All other methods that validate `X`</span>
<span class="sd">               should set `reset=False`.</span>
<span class="sd">        validate_separately : False or tuple of dicts, default=False</span>
<span class="sd">            Only used if y is not None.</span>
<span class="sd">            If False, call validate_X_y(). Else, it must be a tuple of kwargs</span>
<span class="sd">            to be used for calling check_array() on X and y respectively.</span>
<span class="sd">        **check_params : kwargs</span>
<span class="sd">            Parameters passed to :func:`sklearn.utils.check_array` or</span>
<span class="sd">            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately</span>
<span class="sd">            is not False.</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        out : {ndarray, sparse matrix} or tuple of these</span>
<span class="sd">            The validated input. A tuple is returned if `y` is not None.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_tags</span><span class="p">()[</span><span class="s1">&#39;requires_y&#39;</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;This </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> estimator &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;requires y to be passed, but the target y is None.&quot;</span>
                <span class="p">)</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="o">**</span><span class="n">check_params</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">X</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="n">y</span> <span class="o">==</span> <span class="s1">&#39;no_validation&#39;</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="o">**</span><span class="n">check_params</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">X</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">validate_separately</span><span class="p">:</span>
                <span class="c1"># We need this because some estimators validate X and y</span>
                <span class="c1"># separately, and in general, separately calling check_array()</span>
                <span class="c1"># on X and y isn&#39;t equivalent to just calling check_X_y()</span>
                <span class="c1"># :(</span>
                <span class="n">check_X_params</span><span class="p">,</span> <span class="n">check_y_params</span> <span class="o">=</span> <span class="n">validate_separately</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="o">**</span><span class="n">check_X_params</span><span class="p">)</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="n">check_y_params</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="n">check_params</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

        <span class="k">if</span> <span class="n">check_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;ensure_2d&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_check_n_features</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">reset</span><span class="o">=</span><span class="n">reset</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">_check_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check validity of parameters and raise ValueError if not valid.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;n_estimators must be greater than 0 but &quot;</span>
                             <span class="s2">&quot;was </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">&lt;=</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;learning_rate must be greater than 0 but &quot;</span>
                             <span class="s2">&quot;was </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_SUPPORTED_LOSS</span>
                <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">LOSS_FUNCTIONS</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Loss &#39;</span><span class="si">{0:s}</span><span class="s2">&#39; not supported. &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;deviance&#39;</span><span class="p">:</span>
            <span class="n">loss_class</span> <span class="o">=</span> <span class="p">(</span><span class="n">MultinomialDeviance</span>
                          <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span>
                          <span class="k">else</span> <span class="n">BinomialDeviance</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">loss_class</span> <span class="o">=</span> <span class="n">LOSS_FUNCTIONS</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span> <span class="o">=</span> <span class="n">loss_class</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">constraints</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_constraints</span><span class="p">)</span> <span class="c1"># change</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;huber&quot;</span><span class="p">,</span> <span class="s2">&quot;quantile&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span> <span class="o">=</span> <span class="n">loss_class</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span> <span class="o">=</span> <span class="n">loss_class</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">constraints</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_constraints</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="mf">0.0</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">subsample</span> <span class="o">&lt;=</span> <span class="mf">1.0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;subsample must be in (0,1] but &quot;</span>
                             <span class="s2">&quot;was </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">subsample</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># init must be an estimator or &#39;zero&#39;</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">check_init_estimator</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">)</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">==</span> <span class="s1">&#39;zero&#39;</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;The init parameter must be an estimator or &#39;zero&#39;. &quot;</span>
                    <span class="s2">&quot;Got init=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">)</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="mf">0.0</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;alpha must be in (0.0, 1.0) but &quot;</span>
                             <span class="s2">&quot;was </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_features</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                    <span class="n">max_features</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">)))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">max_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">==</span> <span class="s2">&quot;sqrt&quot;</span><span class="p">:</span>
                <span class="n">max_features</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">)))</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">==</span> <span class="s2">&quot;log2&quot;</span><span class="p">:</span>
                <span class="n">max_features</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">)))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid value for max_features: </span><span class="si">%r</span><span class="s2">. &quot;</span>
                                 <span class="s2">&quot;Allowed string values are &#39;auto&#39;, &#39;sqrt&#39; &quot;</span>
                                 <span class="s2">&quot;or &#39;log2&#39;.&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">max_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_features</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">):</span>
            <span class="n">max_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># float</span>
            <span class="k">if</span> <span class="mf">0.</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">&lt;=</span> <span class="mf">1.</span><span class="p">:</span>
                <span class="n">max_features</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">*</span>
                                       <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;max_features must be in (0, n_features]&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_features_</span> <span class="o">=</span> <span class="n">max_features</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter_no_change</span><span class="p">,</span>
                          <span class="p">(</span><span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">))):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;n_iter_no_change should either be None or an &quot;</span>
                             <span class="s2">&quot;integer. </span><span class="si">%r</span><span class="s2"> was passed&quot;</span>
                             <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_no_change</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_init_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize model state and allocate model state data structures. &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">init_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">init_estimator</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">K</span><span class="p">),</span>
                                    <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_score_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="c1"># do oob?</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">subsample</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">oob_improvement_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">),</span>
                                             <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_clear_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Clear the state of the gradient boosting model. &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;estimators_&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;train_score_&#39;</span><span class="p">):</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_score_</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;oob_improvement_&#39;</span><span class="p">):</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">oob_improvement_</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;init_&#39;</span><span class="p">):</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_rng&#39;</span><span class="p">):</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rng</span>

    <span class="k">def</span> <span class="nf">_resize_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Add additional ``n_estimators`` entries to all attributes.&quot;&quot;&quot;</span>
        <span class="c1"># self.n_estimators is the number of additional est to fit</span>
        <span class="n">total_n_estimators</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span>
        <span class="k">if</span> <span class="n">total_n_estimators</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;resize with smaller n_estimators </span><span class="si">%d</span><span class="s1"> &lt; </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span>
                             <span class="p">(</span><span class="n">total_n_estimators</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">,</span>
                                     <span class="p">(</span><span class="n">total_n_estimators</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">K</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_score_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_score_</span><span class="p">,</span> <span class="n">total_n_estimators</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">subsample</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;oob_improvement_&#39;</span><span class="p">)):</span>
            <span class="c1"># if do oob resize arrays or create new if not available</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;oob_improvement_&#39;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">oob_improvement_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">oob_improvement_</span><span class="p">,</span>
                                                  <span class="n">total_n_estimators</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">oob_improvement_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">total_n_estimators</span><span class="p">,),</span>
                                                 <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_is_initialized</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;estimators_&#39;</span><span class="p">,</span> <span class="p">[]))</span> <span class="o">&gt;</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">_check_initialized</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check that the estimator is initialized, raising an error if not.&quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_warn_mae_for_criterion</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit the gradient boosting model.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>
<span class="sd">        y : array-like of shape (n_samples,)</span>
<span class="sd">            Target values (strings or integers in classification, real numbers</span>
<span class="sd">            in regression)</span>
<span class="sd">            For classification, labels must correspond to classes.</span>
<span class="sd">        sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">            Sample weights. If None, then samples are equally weighted. Splits</span>
<span class="sd">            that would create child nodes with net zero or negative weight are</span>
<span class="sd">            ignored while searching for a split in each node. In the case of</span>
<span class="sd">            classification, splits are also ignored if they would result in any</span>
<span class="sd">            single class carrying a negative weight in either child node.</span>
<span class="sd">        monitor : callable, default=None</span>
<span class="sd">            The monitor is called after each iteration with the current</span>
<span class="sd">            iteration, a reference to the estimator and the local variables of</span>
<span class="sd">            ``_fit_stages`` as keyword arguments ``callable(i, self,</span>
<span class="sd">            locals())``. If the callable returns ``True`` the fitting procedure</span>
<span class="sd">            is stopped. The monitor can be used for various things such as</span>
<span class="sd">            computing held-out estimates, early stopping, model introspect, and</span>
<span class="sd">            snapshoting.</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">==</span> <span class="s1">&#39;mae&#39;</span><span class="p">:</span>
            <span class="c1"># TODO: This should raise an error from 1.1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_warn_mae_for_criterion</span><span class="p">()</span>

        <span class="c1"># if not warmstart - clear the estimator state</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_clear_state</span><span class="p">()</span>

        <span class="c1"># Check input</span>
        <span class="c1"># Since check_array converts both X and y to the same dtype, but the</span>
        <span class="c1"># trees use different types for X and y, checking them separately.</span>

        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="s1">&#39;csc&#39;</span><span class="p">,</span> <span class="s1">&#39;coo&#39;</span><span class="p">],</span>
                                   <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">,</span> <span class="n">multi_output</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">sample_weight_is_none</span> <span class="o">=</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span>

        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">_check_sample_weight</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_constraints</span><span class="p">:</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
            <span class="n">y_0</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
            <span class="n">maxi</span> <span class="o">=</span> <span class="n">y_0</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
            <span class="n">mini</span> <span class="o">=</span> <span class="n">y_0</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>

            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">constraint</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">constraints</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">y_i</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>
                <span class="n">c_maxi</span> <span class="o">=</span> <span class="n">y_i</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
                <span class="n">c_mini</span> <span class="o">=</span> <span class="n">y_i</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>

                <span class="k">if</span> <span class="n">c_maxi</span> <span class="o">!=</span> <span class="n">maxi</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;&quot;&quot;Multi Objectives have different scale. </span>
<span class="s2">                        Sub-objective at index </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> has different scale than the Primary Objective(index=0)</span>
<span class="s2">                        Make sure minimum and maximum values of objectives are same</span>
<span class="s2">                        &quot;&quot;&quot;</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="n">c_mini</span> <span class="o">!=</span> <span class="n">mini</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;&quot;&quot;Multi Objectives have different scale. </span>
<span class="s2">                        Sub-objective at index </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> has different scale than the Primary Objective(index=0)</span>
<span class="s2">                        Make sure minimum and maximum values of objectives are same</span>
<span class="s2">                        &quot;&quot;&quot;</span>
                    <span class="p">)</span>



            <span class="c1"># shape = np.shape(y)</span>
            <span class="c1"># if shape[1] != self.n_constraints + 1:</span>
            <span class="c1">#     raise ValueError(&quot;Number of additional y variables\</span>
            <span class="c1">#         and constrains not same&quot;) </span>
            <span class="n">y_0</span> <span class="o">=</span> <span class="n">column_or_1d</span><span class="p">(</span><span class="n">y</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">warn</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            
        <span class="k">else</span><span class="p">:</span>
            <span class="n">y_0</span> <span class="o">=</span> <span class="n">column_or_1d</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">warn</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


        <span class="k">if</span> <span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="n">y_0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_y</span><span class="p">(</span><span class="n">y_0</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">y_0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_y</span><span class="p">(</span><span class="n">y_0</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_no_change</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">stratify</span> <span class="o">=</span> <span class="n">y_0</span> <span class="k">if</span> <span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_0</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">sample_weight_val</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_0</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span>
                                <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
                                <span class="n">test_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">validation_fraction</span><span class="p">,</span>
                                <span class="n">stratify</span><span class="o">=</span><span class="n">stratify</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_classes</span> <span class="o">!=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_0</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                    <span class="c1"># We choose to error here. The problem is that the init</span>
                    <span class="c1"># estimator would be trained on y_0, which has some missing</span>
                    <span class="c1"># classes now, so its predictions would not have the</span>
                    <span class="c1"># correct shape.</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s1">&#39;The training data after the early stopping split &#39;</span>
                        <span class="s1">&#39;is missing some classes. Try using another random &#39;</span>
                        <span class="s1">&#39;seed.&#39;</span>
                    <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">X_val</span> <span class="o">=</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">sample_weight_val</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_check_params</span><span class="p">()</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_initialized</span><span class="p">():</span>
            <span class="c1"># init state</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_state</span><span class="p">()</span>

            <span class="c1"># fit initial model and initialize raw predictions</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_</span> <span class="o">==</span> <span class="s1">&#39;zero&#39;</span><span class="p">:</span>
                <span class="n">raw_predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">K</span><span class="p">),</span>
                                           <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># XXX clean this once we have a support_sample_weight tag</span>
                <span class="k">if</span> <span class="n">sample_weight_is_none</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">init_</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_0</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;The initial estimator </span><span class="si">{}</span><span class="s2"> does not support sample &quot;</span>
                           <span class="s2">&quot;weights.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">init_</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_0</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
                    <span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                        <span class="c1"># regular estimator without SW support</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>
                    <span class="k">except</span> <span class="ne">ValueError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                        <span class="k">if</span> <span class="s2">&quot;pass parameters to specific steps of &quot;</span>\
                           <span class="s2">&quot;your pipeline using the &quot;</span>\
                           <span class="s2">&quot;stepname__parameter&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">):</span>  <span class="c1"># pipeline</span>
                            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>
                        <span class="k">else</span><span class="p">:</span>  <span class="c1"># regular estimator whose input checking failed</span>
                            <span class="k">raise</span>

                <span class="n">raw_predictions</span> <span class="o">=</span> \
                    <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">get_init_raw_predictions</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_</span><span class="p">)</span>

            <span class="n">begin_at_stage</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="c1"># The rng state must be preserved if warm_start is True</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># add more estimators to fitted model</span>
            <span class="c1"># invariant: warm_start = True</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;n_estimators=</span><span class="si">%d</span><span class="s1"> must be larger or equal to &#39;</span>
                                 <span class="s1">&#39;estimators_.shape[0]=</span><span class="si">%d</span><span class="s1"> when &#39;</span>
                                 <span class="s1">&#39;warm_start==True&#39;</span>
                                 <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
            <span class="n">begin_at_stage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="c1"># The requirements of _decision_function (called in two lines</span>
            <span class="c1"># below) are more constrained than fit. It accepts only CSR</span>
            <span class="c1"># matrices.</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>
            <span class="n">raw_predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_resize_state</span><span class="p">()</span>

        <span class="c1"># fit the boosting stages</span>
        <span class="n">n_stages</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit_stages</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rng</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span>
            <span class="n">sample_weight_val</span><span class="p">,</span> <span class="n">begin_at_stage</span><span class="p">,</span> <span class="n">monitor</span><span class="p">)</span>

        <span class="c1"># change shape of arrays after fit (early-stopping or additional ests)</span>
        <span class="k">if</span> <span class="n">n_stages</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[:</span><span class="n">n_stages</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_score_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_score_</span><span class="p">[:</span><span class="n">n_stages</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;oob_improvement_&#39;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">oob_improvement_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">oob_improvement_</span><span class="p">[:</span><span class="n">n_stages</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators_</span> <span class="o">=</span> <span class="n">n_stages</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">_fit_stages</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">random_state</span><span class="p">,</span>
                    <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">sample_weight_val</span><span class="p">,</span>
                    <span class="n">begin_at_stage</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Iteratively fits the stages.</span>
<span class="sd">        For each stage it computes the progress (OOB, train score)</span>
<span class="sd">        and delegates to ``_fit_stage``.</span>
<span class="sd">        Returns the number of stages fit; might differ from ``n_estimators``</span>
<span class="sd">        due to early stopping.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">do_oob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">subsample</span> <span class="o">&lt;</span> <span class="mf">1.0</span>
        <span class="n">sample_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
        <span class="n">n_inbag</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">subsample</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span>
        <span class="n">loss_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_constraints</span><span class="p">:</span>
            <span class="n">y_0</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">y_0</span> <span class="o">=</span> <span class="n">y</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="n">verbose_reporter</span> <span class="o">=</span> <span class="n">VerboseReporter</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>
            <span class="n">verbose_reporter</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">begin_at_stage</span><span class="p">)</span>

        <span class="n">X_csc</span> <span class="o">=</span> <span class="n">csc_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">if</span> <span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">X_csr</span> <span class="o">=</span> <span class="n">csr_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">if</span> <span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_no_change</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter_no_change</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
            <span class="c1"># We create a generator to get the predictions for X_val after</span>
            <span class="c1"># the addition of each successive stage</span>
            <span class="n">y_val_pred_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_staged_raw_predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>

        <span class="c1"># perform boosting iterations</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">begin_at_stage</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">begin_at_stage</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">):</span>

            <span class="c1"># subsampling</span>
            <span class="k">if</span> <span class="n">do_oob</span><span class="p">:</span>
                <span class="n">sample_mask</span> <span class="o">=</span> <span class="n">_random_sample_mask</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_inbag</span><span class="p">,</span>
                                                  <span class="n">random_state</span><span class="p">)</span>
                <span class="c1"># OOB score before adding this stage</span>
                <span class="n">old_oob_score</span> <span class="o">=</span> <span class="n">loss_</span><span class="p">(</span><span class="n">y_0</span><span class="p">[</span><span class="o">~</span><span class="n">sample_mask</span><span class="p">],</span>
                                      <span class="n">raw_predictions</span><span class="p">[</span><span class="o">~</span><span class="n">sample_mask</span><span class="p">],</span>
                                      <span class="n">sample_weight</span><span class="p">[</span><span class="o">~</span><span class="n">sample_mask</span><span class="p">])</span>

            <span class="c1"># fit next stage of trees</span>
            <span class="n">raw_predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit_stage</span><span class="p">(</span>
                <span class="n">i</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">sample_mask</span><span class="p">,</span>
                <span class="n">random_state</span><span class="p">,</span> <span class="n">X_csc</span><span class="p">,</span> <span class="n">X_csr</span><span class="p">)</span>

            <span class="c1"># track deviance (= loss)</span>
            <span class="k">if</span> <span class="n">do_oob</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_score_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss_</span><span class="p">(</span><span class="n">y_0</span><span class="p">[</span><span class="n">sample_mask</span><span class="p">],</span>
                                             <span class="n">raw_predictions</span><span class="p">[</span><span class="n">sample_mask</span><span class="p">],</span>
                                             <span class="n">sample_weight</span><span class="p">[</span><span class="n">sample_mask</span><span class="p">])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">oob_improvement_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">old_oob_score</span> <span class="o">-</span> <span class="n">loss_</span><span class="p">(</span><span class="n">y_0</span><span class="p">[</span><span class="o">~</span><span class="n">sample_mask</span><span class="p">],</span>
                                          <span class="n">raw_predictions</span><span class="p">[</span><span class="o">~</span><span class="n">sample_mask</span><span class="p">],</span>
                                          <span class="n">sample_weight</span><span class="p">[</span><span class="o">~</span><span class="n">sample_mask</span><span class="p">]))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># no need to fancy index w/ no subsampling</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_score_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss_</span><span class="p">(</span><span class="n">y_0</span><span class="p">,</span> <span class="n">raw_predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">verbose_reporter</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">early_stopping</span> <span class="o">=</span> <span class="n">monitor</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="nb">locals</span><span class="p">())</span>
                <span class="k">if</span> <span class="n">early_stopping</span><span class="p">:</span>
                    <span class="k">break</span>

            <span class="c1"># We also provide an early stopping based on the score from</span>
            <span class="c1"># validation set (X_val, y_val), if n_iter_no_change is set</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_no_change</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># By calling next(y_val_pred_iter), we get the predictions</span>
                <span class="c1"># for X_val after the addition of the current stage</span>
                <span class="n">validation_loss</span> <span class="o">=</span> <span class="n">loss_</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="nb">next</span><span class="p">(</span><span class="n">y_val_pred_iter</span><span class="p">),</span>
                                        <span class="n">sample_weight_val</span><span class="p">)</span>

                <span class="c1"># Require validation_score to be better (less) than at least</span>
                <span class="c1"># one of the last n_iter_no_change evaluations</span>
                <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">validation_loss</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">&lt;</span> <span class="n">loss_history</span><span class="p">):</span>
                    <span class="n">loss_history</span><span class="p">[</span><span class="n">i</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">loss_history</span><span class="p">)]</span> <span class="o">=</span> <span class="n">validation_loss</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">break</span>

        <span class="k">return</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">_make_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">append</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="c1"># we don&#39;t need _make_estimator</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_raw_predict_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check input and compute raw predictions of the init estimator.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_initialized</span><span class="p">()</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;X.shape[1] should be </span><span class="si">{0:d}</span><span class="s2">, not </span><span class="si">{1:d}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_</span> <span class="o">==</span> <span class="s1">&#39;zero&#39;</span><span class="p">:</span>
            <span class="n">raw_predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">K</span><span class="p">),</span>
                                       <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">raw_predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">get_init_raw_predictions</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">raw_predictions</span>

    <span class="k">def</span> <span class="nf">_raw_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the sum of the trees raw predictions (+ init estimator).&quot;&quot;&quot;</span>
        <span class="n">raw_predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_predict_init</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">predict_stages</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span>
                       <span class="n">raw_predictions</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">raw_predictions</span>

    <span class="k">def</span> <span class="nf">_staged_raw_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute raw predictions of ``X`` for each iteration.</span>
<span class="sd">        This method allows monitoring (i.e. determine error on testing set)</span>
<span class="sd">        after each stage.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        raw_predictions : generator of ndarray of shape (n_samples, k)</span>
<span class="sd">            The raw predictions of the input samples. The order of the</span>
<span class="sd">            classes corresponds to that in the attribute :term:`classes_`.</span>
<span class="sd">            Regression and binary classification are special cases with</span>
<span class="sd">            ``k == 1``, otherwise ``k==n_classes``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>
        <span class="n">raw_predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_predict_init</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">predict_stage</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span>
                          <span class="n">raw_predictions</span><span class="p">)</span>
            <span class="k">yield</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">feature_importances_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;The impurity-based feature importances.</span>
<span class="sd">        The higher, the more important the feature.</span>
<span class="sd">        The importance of a feature is computed as the (normalized)</span>
<span class="sd">        total reduction of the criterion brought by that feature.  It is also</span>
<span class="sd">        known as the Gini importance.</span>
<span class="sd">        Warning: impurity-based feature importances can be misleading for</span>
<span class="sd">        high cardinality features (many unique values). See</span>
<span class="sd">        :func:`sklearn.inspection.permutation_importance` as an alternative.</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        feature_importances_ : ndarray of shape (n_features,)</span>
<span class="sd">            The values of this array sum to 1, unless all trees are single node</span>
<span class="sd">            trees consisting of only the root node, in which case it will be an</span>
<span class="sd">            array of zeros.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_initialized</span><span class="p">()</span>

        <span class="n">relevant_trees</span> <span class="o">=</span> <span class="p">[</span><span class="n">tree</span>
                          <span class="k">for</span> <span class="n">stage</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="n">stage</span>
                          <span class="k">if</span> <span class="n">tree</span><span class="o">.</span><span class="n">tree_</span><span class="o">.</span><span class="n">node_count</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">relevant_trees</span><span class="p">:</span>
            <span class="c1"># degenerate case where all trees have only one node</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

        <span class="n">relevant_feature_importances</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">tree</span><span class="o">.</span><span class="n">tree_</span><span class="o">.</span><span class="n">compute_feature_importances</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="n">relevant_trees</span>
        <span class="p">]</span>
        <span class="n">avg_feature_importances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">relevant_feature_importances</span><span class="p">,</span>
                                          <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">avg_feature_importances</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">avg_feature_importances</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_compute_partial_dependence_recursion</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">target_features</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fast partial dependence computation.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        grid : ndarray of shape (n_samples, n_target_features)</span>
<span class="sd">            The grid points on which the partial dependence should be</span>
<span class="sd">            evaluated.</span>
<span class="sd">        target_features : ndarray of shape (n_target_features,)</span>
<span class="sd">            The set of target features for which the partial dependence</span>
<span class="sd">            should be evaluated.</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        averaged_predictions : ndarray of shape \</span>
<span class="sd">                (n_trees_per_iteration, n_samples)</span>
<span class="sd">            The value of the partial dependence function on each grid point.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s1">&#39;Using recursion method with a non-constant init predictor &#39;</span>
                <span class="s1">&#39;will lead to incorrect partial dependence values. &#39;</span>
                <span class="s1">&#39;Got init=</span><span class="si">%s</span><span class="s1">.&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">,</span>
                <span class="ne">UserWarning</span>
            <span class="p">)</span>
        <span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="p">)</span>
        <span class="n">n_estimators</span><span class="p">,</span> <span class="n">n_trees_per_stage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">averaged_predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_trees_per_stage</span><span class="p">,</span> <span class="n">grid</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
                                        <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">stage</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trees_per_stage</span><span class="p">):</span>
                <span class="n">tree</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="n">stage</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">tree_</span>
                <span class="n">tree</span><span class="o">.</span><span class="n">compute_partial_dependence</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">target_features</span><span class="p">,</span>
                                                <span class="n">averaged_predictions</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
        <span class="n">averaged_predictions</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span>

        <span class="k">return</span> <span class="n">averaged_predictions</span>

    <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Apply trees in the ensemble to X, return leaf indices.</span>
<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, its dtype will be converted to</span>
<span class="sd">            ``dtype=np.float32``. If a sparse matrix is provided, it will</span>
<span class="sd">            be converted to a sparse ``csr_matrix``.</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X_leaves : array-like of shape (n_samples, n_estimators, n_classes)</span>
<span class="sd">            For each datapoint x in X and for each tree in the ensemble,</span>
<span class="sd">            return the index of the leaf x ends up in each estimator.</span>
<span class="sd">            In the case of binary classification n_classes is 1.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_check_initialized</span><span class="p">()</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># n_classes will be equal to 1 in the binary classification or the</span>
        <span class="c1"># regression case.</span>
        <span class="n">n_estimators</span><span class="p">,</span> <span class="n">n_classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">leaves</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_estimators</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_classes</span><span class="p">):</span>
                <span class="n">estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
                <span class="n">leaves</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">leaves</span>

    <span class="k">def</span> <span class="nf">get_loss_function_values</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get Loss function values for objectives and subobjectives</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : 2d list</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">losses</span>
    
    <span class="k">def</span> <span class="nf">get_alpha_values</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get Alpha values for subobjectives</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : 2d list</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">alphas</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MooGBTClassifier</span><span class="p">(</span><span class="n">ClassifierMixin</span><span class="p">,</span> <span class="n">BaseGradientBoosting</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gradient Boosting for classification.</span>
<span class="sd">    GB builds an additive model in a</span>
<span class="sd">    forward stage-wise fashion; it allows for the optimization of</span>
<span class="sd">    arbitrary differentiable loss functions. In each stage ``n_classes_``</span>
<span class="sd">    regression trees are fit on the negative gradient of the</span>
<span class="sd">    binomial or multinomial deviance loss function. Binary classification</span>
<span class="sd">    is a special case where only a single regression tree is induced.</span>
<span class="sd">    Read more in the :ref:`User Guide &lt;gradient_boosting&gt;`.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    loss : {&#39;deviance&#39;, &#39;exponential&#39;}, default=&#39;deviance&#39;</span>
<span class="sd">        The loss function to be optimized. &#39;deviance&#39; refers to</span>
<span class="sd">        deviance (= logistic regression) for classification</span>
<span class="sd">        with probabilistic outputs. For loss &#39;exponential&#39; gradient</span>
<span class="sd">        boosting recovers the AdaBoost algorithm.</span>
<span class="sd">    learning_rate : float, default=0.1</span>
<span class="sd">        Learning rate shrinks the contribution of each tree by `learning_rate`.</span>
<span class="sd">        There is a trade-off between learning_rate and n_estimators.</span>
<span class="sd">    n_estimators : int, default=100</span>
<span class="sd">        The number of boosting stages to perform. Gradient boosting</span>
<span class="sd">        is fairly robust to over-fitting so a large number usually</span>
<span class="sd">        results in better performance.</span>
<span class="sd">    subsample : float, default=1.0</span>
<span class="sd">        The fraction of samples to be used for fitting the individual base</span>
<span class="sd">        learners. If smaller than 1.0 this results in Stochastic Gradient</span>
<span class="sd">        Boosting. `subsample` interacts with the parameter `n_estimators`.</span>
<span class="sd">        Choosing `subsample &lt; 1.0` leads to a reduction of variance</span>
<span class="sd">        and an increase in bias.</span>
<span class="sd">    criterion : {&#39;friedman_mse&#39;, &#39;mse&#39;, &#39;mae&#39;}, default=&#39;friedman_mse&#39;</span>
<span class="sd">        The function to measure the quality of a split. Supported criteria</span>
<span class="sd">        are &#39;friedman_mse&#39; for the mean squared error with improvement</span>
<span class="sd">        score by Friedman, &#39;mse&#39; for mean squared error, and &#39;mae&#39; for</span>
<span class="sd">        the mean absolute error. The default value of &#39;friedman_mse&#39; is</span>
<span class="sd">        generally the best as it can provide a better approximation in</span>
<span class="sd">        some cases.</span>
<span class="sd">        .. versionadded:: 0.18</span>
<span class="sd">        .. deprecated:: 0.24</span>
<span class="sd">            `criterion=&#39;mae&#39;` is deprecated and will be removed in version</span>
<span class="sd">            1.1 (renaming of 0.26). Use `criterion=&#39;friedman_mse&#39;` or `&#39;mse&#39;`</span>
<span class="sd">            instead, as trees should use a least-square criterion in</span>
<span class="sd">            Gradient Boosting.</span>
<span class="sd">    min_samples_split : int or float, default=2</span>
<span class="sd">        The minimum number of samples required to split an internal node:</span>
<span class="sd">        - If int, then consider `min_samples_split` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_split` is a fraction and</span>
<span class="sd">          `ceil(min_samples_split * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each split.</span>
<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>
<span class="sd">    min_samples_leaf : int or float, default=1</span>
<span class="sd">        The minimum number of samples required to be at a leaf node.</span>
<span class="sd">        A split point at any depth will only be considered if it leaves at</span>
<span class="sd">        least ``min_samples_leaf`` training samples in each of the left and</span>
<span class="sd">        right branches.  This may have the effect of smoothing the model,</span>
<span class="sd">        especially in regression.</span>
<span class="sd">        - If int, then consider `min_samples_leaf` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_leaf` is a fraction and</span>
<span class="sd">          `ceil(min_samples_leaf * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each node.</span>
<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>
<span class="sd">    min_weight_fraction_leaf : float, default=0.0</span>
<span class="sd">        The minimum weighted fraction of the sum total of weights (of all</span>
<span class="sd">        the input samples) required to be at a leaf node. Samples have</span>
<span class="sd">        equal weight when sample_weight is not provided.</span>
<span class="sd">    max_depth : int, default=3</span>
<span class="sd">        The maximum depth of the individual regression estimators. The maximum</span>
<span class="sd">        depth limits the number of nodes in the tree. Tune this parameter</span>
<span class="sd">        for best performance; the best value depends on the interaction</span>
<span class="sd">        of the input variables.</span>
<span class="sd">    min_impurity_decrease : float, default=0.0</span>
<span class="sd">        A node will be split if this split induces a decrease of the impurity</span>
<span class="sd">        greater than or equal to this value.</span>
<span class="sd">        The weighted impurity decrease equation is the following::</span>
<span class="sd">            N_t / N * (impurity - N_t_R / N_t * right_impurity</span>
<span class="sd">                                - N_t_L / N_t * left_impurity)</span>
<span class="sd">        where ``N`` is the total number of samples, ``N_t`` is the number of</span>
<span class="sd">        samples at the current node, ``N_t_L`` is the number of samples in the</span>
<span class="sd">        left child, and ``N_t_R`` is the number of samples in the right child.</span>
<span class="sd">        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,</span>
<span class="sd">        if ``sample_weight`` is passed.</span>
<span class="sd">        .. versionadded:: 0.19</span>
<span class="sd">    min_impurity_split : float, default=None</span>
<span class="sd">        Threshold for early stopping in tree growth. A node will split</span>
<span class="sd">        if its impurity is above the threshold, otherwise it is a leaf.</span>
<span class="sd">        .. deprecated:: 0.19</span>
<span class="sd">           ``min_impurity_split`` has been deprecated in favor of</span>
<span class="sd">           ``min_impurity_decrease`` in 0.19. The default value of</span>
<span class="sd">           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it</span>
<span class="sd">           will be removed in 1.0 (renaming of 0.25).</span>
<span class="sd">           Use ``min_impurity_decrease`` instead.</span>
<span class="sd">    init : estimator or &#39;zero&#39;, default=None</span>
<span class="sd">        An estimator object that is used to compute the initial predictions.</span>
<span class="sd">        ``init`` has to provide :meth:`fit` and :meth:`predict_proba`. If</span>
<span class="sd">        &#39;zero&#39;, the initial raw predictions are set to zero. By default, a</span>
<span class="sd">        ``DummyEstimator`` predicting the classes priors is used.</span>
<span class="sd">    random_state : int, RandomState instance or None, default=None</span>
<span class="sd">        Controls the random seed given to each Tree estimator at each</span>
<span class="sd">        boosting iteration.</span>
<span class="sd">        In addition, it controls the random permutation of the features at</span>
<span class="sd">        each split (see Notes for more details).</span>
<span class="sd">        It also controls the random spliting of the training data to obtain a</span>
<span class="sd">        validation set if `n_iter_no_change` is not None.</span>
<span class="sd">        Pass an int for reproducible output across multiple function calls.</span>
<span class="sd">        See :term:`Glossary &lt;random_state&gt;`.</span>
<span class="sd">    max_features : {&#39;auto&#39;, &#39;sqrt&#39;, &#39;log2&#39;}, int or float, default=None</span>
<span class="sd">        The number of features to consider when looking for the best split:</span>
<span class="sd">        - If int, then consider `max_features` features at each split.</span>
<span class="sd">        - If float, then `max_features` is a fraction and</span>
<span class="sd">          `int(max_features * n_features)` features are considered at each</span>
<span class="sd">          split.</span>
<span class="sd">        - If &#39;auto&#39;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">        - If &#39;sqrt&#39;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">        - If &#39;log2&#39;, then `max_features=log2(n_features)`.</span>
<span class="sd">        - If None, then `max_features=n_features`.</span>
<span class="sd">        Choosing `max_features &lt; n_features` leads to a reduction of variance</span>
<span class="sd">        and an increase in bias.</span>
<span class="sd">        Note: the search for a split does not stop until at least one</span>
<span class="sd">        valid partition of the node samples is found, even if it requires to</span>
<span class="sd">        effectively inspect more than ``max_features`` features.</span>
<span class="sd">    verbose : int, default=0</span>
<span class="sd">        Enable verbose output. If 1 then it prints progress and performance</span>
<span class="sd">        once in a while (the more trees the lower the frequency). If greater</span>
<span class="sd">        than 1 then it prints progress and performance for every tree.</span>
<span class="sd">    max_leaf_nodes : int, default=None</span>
<span class="sd">        Grow trees with ``max_leaf_nodes`` in best-first fashion.</span>
<span class="sd">        Best nodes are defined as relative reduction in impurity.</span>
<span class="sd">        If None then unlimited number of leaf nodes.</span>
<span class="sd">    warm_start : bool, default=False</span>
<span class="sd">        When set to ``True``, reuse the solution of the previous call to fit</span>
<span class="sd">        and add more estimators to the ensemble, otherwise, just erase the</span>
<span class="sd">        previous solution. See :term:`the Glossary &lt;warm_start&gt;`.</span>
<span class="sd">    validation_fraction : float, default=0.1</span>
<span class="sd">        The proportion of training data to set aside as validation set for</span>
<span class="sd">        early stopping. Must be between 0 and 1.</span>
<span class="sd">        Only used if ``n_iter_no_change`` is set to an integer.</span>
<span class="sd">        .. versionadded:: 0.20</span>
<span class="sd">    n_iter_no_change : int, default=None</span>
<span class="sd">        ``n_iter_no_change`` is used to decide if early stopping will be used</span>
<span class="sd">        to terminate training when validation score is not improving. By</span>
<span class="sd">        default it is set to None to disable early stopping. If set to a</span>
<span class="sd">        number, it will set aside ``validation_fraction`` size of the training</span>
<span class="sd">        data as validation and terminate training when validation score is not</span>
<span class="sd">        improving in all of the previous ``n_iter_no_change`` numbers of</span>
<span class="sd">        iterations. The split is stratified.</span>
<span class="sd">        .. versionadded:: 0.20</span>
<span class="sd">    tol : float, default=1e-4</span>
<span class="sd">        Tolerance for the early stopping. When the loss is not improving</span>
<span class="sd">        by at least tol for ``n_iter_no_change`` iterations (if set to a</span>
<span class="sd">        number), the training stops.</span>
<span class="sd">        .. versionadded:: 0.20</span>
<span class="sd">    ccp_alpha : non-negative float, default=0.0</span>
<span class="sd">        Complexity parameter used for Minimal Cost-Complexity Pruning. The</span>
<span class="sd">        subtree with the largest cost complexity that is smaller than</span>
<span class="sd">        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See</span>
<span class="sd">        :ref:`minimal_cost_complexity_pruning` for details.</span>
<span class="sd">        .. versionadded:: 0.22</span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    n_estimators_ : int</span>
<span class="sd">        The number of estimators as selected by early stopping (if</span>
<span class="sd">        ``n_iter_no_change`` is specified). Otherwise it is set to</span>
<span class="sd">        ``n_estimators``.</span>
<span class="sd">        .. versionadded:: 0.20</span>
<span class="sd">    feature_importances_ : ndarray of shape (n_features,)</span>
<span class="sd">        The impurity-based feature importances.</span>
<span class="sd">        The higher, the more important the feature.</span>
<span class="sd">        The importance of a feature is computed as the (normalized)</span>
<span class="sd">        total reduction of the criterion brought by that feature.  It is also</span>
<span class="sd">        known as the Gini importance.</span>
<span class="sd">        Warning: impurity-based feature importances can be misleading for</span>
<span class="sd">        high cardinality features (many unique values). See</span>
<span class="sd">        :func:`sklearn.inspection.permutation_importance` as an alternative.</span>
<span class="sd">    oob_improvement_ : ndarray of shape (n_estimators,)</span>
<span class="sd">        The improvement in loss (= deviance) on the out-of-bag samples</span>
<span class="sd">        relative to the previous iteration.</span>
<span class="sd">        ``oob_improvement_[0]`` is the improvement in</span>
<span class="sd">        loss of the first stage over the ``init`` estimator.</span>
<span class="sd">        Only available if ``subsample &lt; 1.0``</span>
<span class="sd">    train_score_ : ndarray of shape (n_estimators,)</span>
<span class="sd">        The i-th score ``train_score_[i]`` is the deviance (= loss) of the</span>
<span class="sd">        model at iteration ``i`` on the in-bag sample.</span>
<span class="sd">        If ``subsample == 1`` this is the deviance on the training data.</span>
<span class="sd">    loss_ : LossFunction</span>
<span class="sd">        The concrete ``LossFunction`` object.</span>
<span class="sd">    init_ : estimator</span>
<span class="sd">        The estimator that provides the initial predictions.</span>
<span class="sd">        Set via the ``init`` argument or ``loss.init_estimator``.</span>
<span class="sd">    estimators_ : ndarray of DecisionTreeRegressor of \</span>
<span class="sd">shape (n_estimators, ``loss_.K``)</span>
<span class="sd">        The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary</span>
<span class="sd">        classification, otherwise n_classes.</span>
<span class="sd">    classes_ : ndarray of shape (n_classes,)</span>
<span class="sd">        The classes labels.</span>
<span class="sd">    n_features_ : int</span>
<span class="sd">        The number of data features.</span>
<span class="sd">    n_classes_ : int</span>
<span class="sd">        The number of classes.</span>
<span class="sd">    max_features_ : int</span>
<span class="sd">        The inferred value of max_features.</span>
<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    HistGradientBoostingClassifier : Histogram-based Gradient Boosting</span>
<span class="sd">        Classification Tree.</span>
<span class="sd">    sklearn.tree.DecisionTreeClassifier : A decision tree classifier.</span>
<span class="sd">    RandomForestClassifier : A meta-estimator that fits a number of decision</span>
<span class="sd">        tree classifiers on various sub-samples of the dataset and uses</span>
<span class="sd">        averaging to improve the predictive accuracy and control over-fitting.</span>
<span class="sd">    AdaBoostClassifier : A meta-estimator that begins by fitting a classifier</span>
<span class="sd">        on the original dataset and then fits additional copies of the</span>
<span class="sd">        classifier on the same dataset where the weights of incorrectly</span>
<span class="sd">        classified instances are adjusted such that subsequent classifiers</span>
<span class="sd">        focus more on difficult cases.</span>
<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The features are always randomly permuted at each split. Therefore,</span>
<span class="sd">    the best found split may vary, even with the same training data and</span>
<span class="sd">    ``max_features=n_features``, if the improvement of the criterion is</span>
<span class="sd">    identical for several splits enumerated during the search of the best</span>
<span class="sd">    split. To obtain a deterministic behaviour during fitting,</span>
<span class="sd">    ``random_state`` has to be fixed.</span>
<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    J. Friedman, Greedy Function Approximation: A Gradient Boosting</span>
<span class="sd">    Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</span>
<span class="sd">    J. Friedman, Stochastic Gradient Boosting, 1999</span>
<span class="sd">    T. Hastie, R. Tibshirani and J. Friedman.</span>
<span class="sd">    Elements of Statistical Learning Ed. 2, Springer, 2009.</span>
<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    The following example shows how to fit a gradient boosting classifier with</span>
<span class="sd">    100 decision stumps as weak learners.</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.datasets import make_hastie_10_2</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.ensemble import MooGBTClassifier</span>
<span class="sd">    &gt;&gt;&gt; X, y = make_hastie_10_2(random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; X_train, X_test = X[:2000], X[2000:]</span>
<span class="sd">    &gt;&gt;&gt; y_train, y_test = y[:2000], y[2000:]</span>
<span class="sd">    &gt;&gt;&gt; clf = MooGBTClassifier(n_estimators=100, learning_rate=1.0,</span>
<span class="sd">    ...     max_depth=1, random_state=0).fit(X_train, y_train)</span>
<span class="sd">    &gt;&gt;&gt; clf.score(X_test, y_test)</span>
<span class="sd">    0.913...</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_SUPPORTED_LOSS</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;deviance&#39;</span><span class="p">,</span> <span class="s1">&#39;exponential&#39;</span><span class="p">)</span>

    <span class="nd">@_deprecate_positional_args</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;deviance&#39;</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">subsample</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;friedman_mse&#39;</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">min_impurity_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_iter_no_change</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
                 <span class="n">ccp_alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">constraints</span><span class="o">=</span><span class="p">[]):</span>
        
        <span class="c1"># Check constraints</span>
        <span class="c1"># for constraint in constraints:</span>
        <span class="c1">#     print (constraint)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">criterion</span><span class="o">=</span><span class="n">criterion</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="n">min_samples_split</span><span class="p">,</span>
            <span class="n">min_samples_leaf</span><span class="o">=</span><span class="n">min_samples_leaf</span><span class="p">,</span>
            <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="n">min_weight_fraction_leaf</span><span class="p">,</span>
            <span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">,</span> <span class="n">subsample</span><span class="o">=</span><span class="n">subsample</span><span class="p">,</span>
            <span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="n">max_leaf_nodes</span><span class="p">,</span>
            <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="n">min_impurity_decrease</span><span class="p">,</span>
            <span class="n">min_impurity_split</span><span class="o">=</span><span class="n">min_impurity_split</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="n">validation_fraction</span><span class="p">,</span>
            <span class="n">n_iter_no_change</span><span class="o">=</span><span class="n">n_iter_no_change</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span> <span class="n">ccp_alpha</span><span class="o">=</span><span class="n">ccp_alpha</span><span class="p">,</span>
            <span class="n">constraints</span><span class="o">=</span><span class="n">constraints</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validate_y</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
        <span class="n">check_classification_targets</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">n_trim_classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">n_trim_classes</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;y contains </span><span class="si">%d</span><span class="s2"> class after sample_weight &quot;</span>
                             <span class="s2">&quot;trimmed classes with zero weights, while a &quot;</span>
                             <span class="s2">&quot;minimum of 2 classes are required.&quot;</span>
                             <span class="o">%</span> <span class="n">n_trim_classes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
        <span class="c1"># expose n_classes_ attribute</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_classes</span>
        <span class="k">return</span> <span class="n">y</span>

    <span class="k">def</span> <span class="nf">_warn_mae_for_criterion</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># TODO: This should raise an error from 1.1</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;criterion=&#39;mae&#39; was deprecated in version 0.24 and &quot;</span>
                      <span class="s2">&quot;will be removed in version 1.1 (renaming of 0.26). Use &quot;</span>
                      <span class="s2">&quot;criterion=&#39;friedman_mse&#39; or &#39;mse&#39; instead, as trees &quot;</span>
                      <span class="s2">&quot;should use a least-square criterion in Gradient &quot;</span>
                      <span class="s2">&quot;Boosting.&quot;</span><span class="p">,</span> <span class="ne">FutureWarning</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the decision function of ``X``.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        score : ndarray of shape (n_samples, n_classes) or (n_samples,)</span>
<span class="sd">            The decision function of the input samples, which corresponds to</span>
<span class="sd">            the raw values predicted from the trees of the ensemble . The</span>
<span class="sd">            order of the classes corresponds to that in the attribute</span>
<span class="sd">            :term:`classes_`. Regression and binary classification produce an</span>
<span class="sd">            array of shape (n_samples,).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>
        <span class="n">raw_predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">raw_predictions</span>

    <span class="k">def</span> <span class="nf">staged_decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute decision function of ``X`` for each iteration.</span>
<span class="sd">        This method allows monitoring (i.e. determine error on testing set)</span>
<span class="sd">        after each stage.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        score : generator of ndarray of shape (n_samples, k)</span>
<span class="sd">            The decision function of the input samples, which corresponds to</span>
<span class="sd">            the raw values predicted from the trees of the ensemble . The</span>
<span class="sd">            classes corresponds to that in the attribute :term:`classes_`.</span>
<span class="sd">            Regression and binary classification are special cases with</span>
<span class="sd">            ``k == 1``, otherwise ``k==n_classes``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">yield from</span> <span class="bp">self</span><span class="o">.</span><span class="n">_staged_raw_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict class for X.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : ndarray of shape (n_samples,)</span>
<span class="sd">            The predicted values.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">raw_predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">encoded_labels</span> <span class="o">=</span> \
            <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">_raw_prediction_to_decision</span><span class="p">(</span><span class="n">raw_predictions</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">encoded_labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">staged_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict class at each stage for X.</span>
<span class="sd">        This method allows monitoring (i.e. determine error on testing set)</span>
<span class="sd">        after each stage.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : generator of ndarray of shape (n_samples,)</span>
<span class="sd">            The predicted value of the input samples.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">raw_predictions</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_staged_raw_predict</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="n">encoded_labels</span> <span class="o">=</span> \
                <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">_raw_prediction_to_decision</span><span class="p">(</span><span class="n">raw_predictions</span><span class="p">)</span>
            <span class="k">yield</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">encoded_labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict class probabilities for X.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>
<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        AttributeError</span>
<span class="sd">            If the ``loss`` does not support probabilities.</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        p : ndarray of shape (n_samples, n_classes)</span>
<span class="sd">            The class probabilities of the input samples. The order of the</span>
<span class="sd">            classes corresponds to that in the attribute :term:`classes_`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">raw_predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">_raw_prediction_to_proba</span><span class="p">(</span><span class="n">raw_predictions</span><span class="p">)</span>
        <span class="k">except</span> <span class="n">NotFittedError</span><span class="p">:</span>
            <span class="k">raise</span>
        <span class="k">except</span> <span class="ne">AttributeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;loss=</span><span class="si">%r</span><span class="s1"> does not support predict_proba&#39;</span> <span class="o">%</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>

    <span class="k">def</span> <span class="nf">predict_log_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict class log-probabilities for X.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>
<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        AttributeError</span>
<span class="sd">            If the ``loss`` does not support probabilities.</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        p : ndarray of shape (n_samples, n_classes)</span>
<span class="sd">            The class log-probabilities of the input samples. The order of the</span>
<span class="sd">            classes corresponds to that in the attribute :term:`classes_`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">proba</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">proba</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">staged_predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict class probabilities at each stage for X.</span>
<span class="sd">        This method allows monitoring (i.e. determine error on testing set)</span>
<span class="sd">        after each stage.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : generator of ndarray of shape (n_samples,)</span>
<span class="sd">            The predicted value of the input samples.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">raw_predictions</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_staged_raw_predict</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
                <span class="k">yield</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">_raw_prediction_to_proba</span><span class="p">(</span><span class="n">raw_predictions</span><span class="p">)</span>
        <span class="k">except</span> <span class="n">NotFittedError</span><span class="p">:</span>
            <span class="k">raise</span>
        <span class="k">except</span> <span class="ne">AttributeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;loss=</span><span class="si">%r</span><span class="s1"> does not support predict_proba&#39;</span> <span class="o">%</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MooGBTRegressor</span><span class="p">(</span><span class="n">RegressorMixin</span><span class="p">,</span> <span class="n">BaseGradientBoosting</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gradient Boosting for regression.</span>
<span class="sd">    GB builds an additive model in a forward stage-wise fashion;</span>
<span class="sd">    it allows for the optimization of arbitrary differentiable loss functions.</span>
<span class="sd">    In each stage a regression tree is fit on the negative gradient of the</span>
<span class="sd">    given loss function.</span>
<span class="sd">    Read more in the :ref:`User Guide &lt;gradient_boosting&gt;`.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    loss : {&#39;ls&#39;, &#39;lad&#39;, &#39;huber&#39;, &#39;quantile&#39;}, default=&#39;ls&#39;</span>
<span class="sd">        Loss function to be optimized. &#39;ls&#39; refers to least squares</span>
<span class="sd">        regression. &#39;lad&#39; (least absolute deviation) is a highly robust</span>
<span class="sd">        loss function solely based on order information of the input</span>
<span class="sd">        variables. &#39;huber&#39; is a combination of the two. &#39;quantile&#39;</span>
<span class="sd">        allows quantile regression (use `alpha` to specify the quantile).</span>
<span class="sd">    learning_rate : float, default=0.1</span>
<span class="sd">        Learning rate shrinks the contribution of each tree by `learning_rate`.</span>
<span class="sd">        There is a trade-off between learning_rate and n_estimators.</span>
<span class="sd">    n_estimators : int, default=100</span>
<span class="sd">        The number of boosting stages to perform. Gradient boosting</span>
<span class="sd">        is fairly robust to over-fitting so a large number usually</span>
<span class="sd">        results in better performance.</span>
<span class="sd">    subsample : float, default=1.0</span>
<span class="sd">        The fraction of samples to be used for fitting the individual base</span>
<span class="sd">        learners. If smaller than 1.0 this results in Stochastic Gradient</span>
<span class="sd">        Boosting. `subsample` interacts with the parameter `n_estimators`.</span>
<span class="sd">        Choosing `subsample &lt; 1.0` leads to a reduction of variance</span>
<span class="sd">        and an increase in bias.</span>
<span class="sd">    criterion : {&#39;friedman_mse&#39;, &#39;mse&#39;, &#39;mae&#39;}, default=&#39;friedman_mse&#39;</span>
<span class="sd">        The function to measure the quality of a split. Supported criteria</span>
<span class="sd">        are &quot;friedman_mse&quot; for the mean squared error with improvement</span>
<span class="sd">        score by Friedman, &quot;mse&quot; for mean squared error, and &quot;mae&quot; for</span>
<span class="sd">        the mean absolute error. The default value of &quot;friedman_mse&quot; is</span>
<span class="sd">        generally the best as it can provide a better approximation in</span>
<span class="sd">        some cases.</span>
<span class="sd">        .. versionadded:: 0.18</span>
<span class="sd">        .. deprecated:: 0.24</span>
<span class="sd">            `criterion=&#39;mae&#39;` is deprecated and will be removed in version</span>
<span class="sd">            1.1 (renaming of 0.26). The correct way of minimizing the absolute</span>
<span class="sd">            error is to use `loss=&#39;lad&#39;` instead.</span>
<span class="sd">    min_samples_split : int or float, default=2</span>
<span class="sd">        The minimum number of samples required to split an internal node:</span>
<span class="sd">        - If int, then consider `min_samples_split` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_split` is a fraction and</span>
<span class="sd">          `ceil(min_samples_split * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each split.</span>
<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>
<span class="sd">    min_samples_leaf : int or float, default=1</span>
<span class="sd">        The minimum number of samples required to be at a leaf node.</span>
<span class="sd">        A split point at any depth will only be considered if it leaves at</span>
<span class="sd">        least ``min_samples_leaf`` training samples in each of the left and</span>
<span class="sd">        right branches.  This may have the effect of smoothing the model,</span>
<span class="sd">        especially in regression.</span>
<span class="sd">        - If int, then consider `min_samples_leaf` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_leaf` is a fraction and</span>
<span class="sd">          `ceil(min_samples_leaf * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each node.</span>
<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>
<span class="sd">    min_weight_fraction_leaf : float, default=0.0</span>
<span class="sd">        The minimum weighted fraction of the sum total of weights (of all</span>
<span class="sd">        the input samples) required to be at a leaf node. Samples have</span>
<span class="sd">        equal weight when sample_weight is not provided.</span>
<span class="sd">    max_depth : int, default=3</span>
<span class="sd">        Maximum depth of the individual regression estimators. The maximum</span>
<span class="sd">        depth limits the number of nodes in the tree. Tune this parameter</span>
<span class="sd">        for best performance; the best value depends on the interaction</span>
<span class="sd">        of the input variables.</span>
<span class="sd">    min_impurity_decrease : float, default=0.0</span>
<span class="sd">        A node will be split if this split induces a decrease of the impurity</span>
<span class="sd">        greater than or equal to this value.</span>
<span class="sd">        The weighted impurity decrease equation is the following::</span>
<span class="sd">            N_t / N * (impurity - N_t_R / N_t * right_impurity</span>
<span class="sd">                                - N_t_L / N_t * left_impurity)</span>
<span class="sd">        where ``N`` is the total number of samples, ``N_t`` is the number of</span>
<span class="sd">        samples at the current node, ``N_t_L`` is the number of samples in the</span>
<span class="sd">        left child, and ``N_t_R`` is the number of samples in the right child.</span>
<span class="sd">        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,</span>
<span class="sd">        if ``sample_weight`` is passed.</span>
<span class="sd">        .. versionadded:: 0.19</span>
<span class="sd">    min_impurity_split : float, default=None</span>
<span class="sd">        Threshold for early stopping in tree growth. A node will split</span>
<span class="sd">        if its impurity is above the threshold, otherwise it is a leaf.</span>
<span class="sd">        .. deprecated:: 0.19</span>
<span class="sd">           ``min_impurity_split`` has been deprecated in favor of</span>
<span class="sd">           ``min_impurity_decrease`` in 0.19. The default value of</span>
<span class="sd">           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it</span>
<span class="sd">           will be removed in 1.0 (renaming of 0.25).</span>
<span class="sd">           Use ``min_impurity_decrease`` instead.</span>
<span class="sd">    init : estimator or &#39;zero&#39;, default=None</span>
<span class="sd">        An estimator object that is used to compute the initial predictions.</span>
<span class="sd">        ``init`` has to provide :term:`fit` and :term:`predict`. If &#39;zero&#39;, the</span>
<span class="sd">        initial raw predictions are set to zero. By default a</span>
<span class="sd">        ``DummyEstimator`` is used, predicting either the average target value</span>
<span class="sd">        (for loss=&#39;ls&#39;), or a quantile for the other losses.</span>
<span class="sd">    random_state : int, RandomState instance or None, default=None</span>
<span class="sd">        Controls the random seed given to each Tree estimator at each</span>
<span class="sd">        boosting iteration.</span>
<span class="sd">        In addition, it controls the random permutation of the features at</span>
<span class="sd">        each split (see Notes for more details).</span>
<span class="sd">        It also controls the random spliting of the training data to obtain a</span>
<span class="sd">        validation set if `n_iter_no_change` is not None.</span>
<span class="sd">        Pass an int for reproducible output across multiple function calls.</span>
<span class="sd">        See :term:`Glossary &lt;random_state&gt;`.</span>
<span class="sd">    max_features : {&#39;auto&#39;, &#39;sqrt&#39;, &#39;log2&#39;}, int or float, default=None</span>
<span class="sd">        The number of features to consider when looking for the best split:</span>
<span class="sd">        - If int, then consider `max_features` features at each split.</span>
<span class="sd">        - If float, then `max_features` is a fraction and</span>
<span class="sd">          `int(max_features * n_features)` features are considered at each</span>
<span class="sd">          split.</span>
<span class="sd">        - If &quot;auto&quot;, then `max_features=n_features`.</span>
<span class="sd">        - If &quot;sqrt&quot;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">        - If &quot;log2&quot;, then `max_features=log2(n_features)`.</span>
<span class="sd">        - If None, then `max_features=n_features`.</span>
<span class="sd">        Choosing `max_features &lt; n_features` leads to a reduction of variance</span>
<span class="sd">        and an increase in bias.</span>
<span class="sd">        Note: the search for a split does not stop until at least one</span>
<span class="sd">        valid partition of the node samples is found, even if it requires to</span>
<span class="sd">        effectively inspect more than ``max_features`` features.</span>
<span class="sd">    alpha : float, default=0.9</span>
<span class="sd">        The alpha-quantile of the huber loss function and the quantile</span>
<span class="sd">        loss function. Only if ``loss=&#39;huber&#39;`` or ``loss=&#39;quantile&#39;``.</span>
<span class="sd">    verbose : int, default=0</span>
<span class="sd">        Enable verbose output. If 1 then it prints progress and performance</span>
<span class="sd">        once in a while (the more trees the lower the frequency). If greater</span>
<span class="sd">        than 1 then it prints progress and performance for every tree.</span>
<span class="sd">    max_leaf_nodes : int, default=None</span>
<span class="sd">        Grow trees with ``max_leaf_nodes`` in best-first fashion.</span>
<span class="sd">        Best nodes are defined as relative reduction in impurity.</span>
<span class="sd">        If None then unlimited number of leaf nodes.</span>
<span class="sd">    warm_start : bool, default=False</span>
<span class="sd">        When set to ``True``, reuse the solution of the previous call to fit</span>
<span class="sd">        and add more estimators to the ensemble, otherwise, just erase the</span>
<span class="sd">        previous solution. See :term:`the Glossary &lt;warm_start&gt;`.</span>
<span class="sd">    validation_fraction : float, default=0.1</span>
<span class="sd">        The proportion of training data to set aside as validation set for</span>
<span class="sd">        early stopping. Must be between 0 and 1.</span>
<span class="sd">        Only used if ``n_iter_no_change`` is set to an integer.</span>
<span class="sd">        .. versionadded:: 0.20</span>
<span class="sd">    n_iter_no_change : int, default=None</span>
<span class="sd">        ``n_iter_no_change`` is used to decide if early stopping will be used</span>
<span class="sd">        to terminate training when validation score is not improving. By</span>
<span class="sd">        default it is set to None to disable early stopping. If set to a</span>
<span class="sd">        number, it will set aside ``validation_fraction`` size of the training</span>
<span class="sd">        data as validation and terminate training when validation score is not</span>
<span class="sd">        improving in all of the previous ``n_iter_no_change`` numbers of</span>
<span class="sd">        iterations.</span>
<span class="sd">        .. versionadded:: 0.20</span>
<span class="sd">    tol : float, default=1e-4</span>
<span class="sd">        Tolerance for the early stopping. When the loss is not improving</span>
<span class="sd">        by at least tol for ``n_iter_no_change`` iterations (if set to a</span>
<span class="sd">        number), the training stops.</span>
<span class="sd">        .. versionadded:: 0.20</span>
<span class="sd">    ccp_alpha : non-negative float, default=0.0</span>
<span class="sd">        Complexity parameter used for Minimal Cost-Complexity Pruning. The</span>
<span class="sd">        subtree with the largest cost complexity that is smaller than</span>
<span class="sd">        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See</span>
<span class="sd">        :ref:`minimal_cost_complexity_pruning` for details.</span>
<span class="sd">        .. versionadded:: 0.22</span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    feature_importances_ : ndarray of shape (n_features,)</span>
<span class="sd">        The impurity-based feature importances.</span>
<span class="sd">        The higher, the more important the feature.</span>
<span class="sd">        The importance of a feature is computed as the (normalized)</span>
<span class="sd">        total reduction of the criterion brought by that feature.  It is also</span>
<span class="sd">        known as the Gini importance.</span>
<span class="sd">        Warning: impurity-based feature importances can be misleading for</span>
<span class="sd">        high cardinality features (many unique values). See</span>
<span class="sd">        :func:`sklearn.inspection.permutation_importance` as an alternative.</span>
<span class="sd">    oob_improvement_ : ndarray of shape (n_estimators,)</span>
<span class="sd">        The improvement in loss (= deviance) on the out-of-bag samples</span>
<span class="sd">        relative to the previous iteration.</span>
<span class="sd">        ``oob_improvement_[0]`` is the improvement in</span>
<span class="sd">        loss of the first stage over the ``init`` estimator.</span>
<span class="sd">        Only available if ``subsample &lt; 1.0``</span>
<span class="sd">    train_score_ : ndarray of shape (n_estimators,)</span>
<span class="sd">        The i-th score ``train_score_[i]`` is the deviance (= loss) of the</span>
<span class="sd">        model at iteration ``i`` on the in-bag sample.</span>
<span class="sd">        If ``subsample == 1`` this is the deviance on the training data.</span>
<span class="sd">    loss_ : LossFunction</span>
<span class="sd">        The concrete ``LossFunction`` object.</span>
<span class="sd">    init_ : estimator</span>
<span class="sd">        The estimator that provides the initial predictions.</span>
<span class="sd">        Set via the ``init`` argument or ``loss.init_estimator``.</span>
<span class="sd">    estimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, 1)</span>
<span class="sd">        The collection of fitted sub-estimators.</span>
<span class="sd">    n_classes_ : int</span>
<span class="sd">        The number of classes, set to 1 for regressors.</span>
<span class="sd">        .. deprecated:: 0.24</span>
<span class="sd">            Attribute ``n_classes_`` was deprecated in version 0.24 and</span>
<span class="sd">            will be removed in 1.1 (renaming of 0.26).</span>
<span class="sd">    n_estimators_ : int</span>
<span class="sd">        The number of estimators as selected by early stopping (if</span>
<span class="sd">        ``n_iter_no_change`` is specified). Otherwise it is set to</span>
<span class="sd">        ``n_estimators``.</span>
<span class="sd">    n_features_ : int</span>
<span class="sd">        The number of data features.</span>
<span class="sd">    max_features_ : int</span>
<span class="sd">        The inferred value of max_features.</span>
<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    HistGradientBoostingRegressor : Histogram-based Gradient Boosting</span>
<span class="sd">        Classification Tree.</span>
<span class="sd">    sklearn.tree.DecisionTreeRegressor : A decision tree regressor.</span>
<span class="sd">    sklearn.tree.RandomForestRegressor : A random forest regressor.</span>
<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The features are always randomly permuted at each split. Therefore,</span>
<span class="sd">    the best found split may vary, even with the same training data and</span>
<span class="sd">    ``max_features=n_features``, if the improvement of the criterion is</span>
<span class="sd">    identical for several splits enumerated during the search of the best</span>
<span class="sd">    split. To obtain a deterministic behaviour during fitting,</span>
<span class="sd">    ``random_state`` has to be fixed.</span>
<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.datasets import make_regression</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.ensemble import MooGBTRegressor</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.model_selection import train_test_split</span>
<span class="sd">    &gt;&gt;&gt; X, y = make_regression(random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(</span>
<span class="sd">    ...     X, y, random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; reg = MooGBTRegressor(random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; reg.fit(X_train, y_train)</span>
<span class="sd">    MooGBTRegressor(random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; reg.predict(X_test[1:2])</span>
<span class="sd">    array([-61...])</span>
<span class="sd">    &gt;&gt;&gt; reg.score(X_test, y_test)</span>
<span class="sd">    0.4...</span>
<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    J. Friedman, Greedy Function Approximation: A Gradient Boosting</span>
<span class="sd">    Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</span>
<span class="sd">    J. Friedman, Stochastic Gradient Boosting, 1999</span>
<span class="sd">    T. Hastie, R. Tibshirani and J. Friedman.</span>
<span class="sd">    Elements of Statistical Learning Ed. 2, Springer, 2009.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_SUPPORTED_LOSS</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;ls&#39;</span><span class="p">,</span> <span class="s1">&#39;lad&#39;</span><span class="p">,</span> <span class="s1">&#39;huber&#39;</span><span class="p">,</span> <span class="s1">&#39;quantile&#39;</span><span class="p">)</span>

    <span class="nd">@_deprecate_positional_args</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;ls&#39;</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">subsample</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;friedman_mse&#39;</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">min_impurity_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">max_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">n_iter_no_change</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">ccp_alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">constraints</span><span class="o">=</span><span class="p">[]):</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">criterion</span><span class="o">=</span><span class="n">criterion</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="n">min_samples_split</span><span class="p">,</span>
            <span class="n">min_samples_leaf</span><span class="o">=</span><span class="n">min_samples_leaf</span><span class="p">,</span>
            <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="n">min_weight_fraction_leaf</span><span class="p">,</span>
            <span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">,</span> <span class="n">subsample</span><span class="o">=</span><span class="n">subsample</span><span class="p">,</span>
            <span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">,</span>
            <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="n">min_impurity_decrease</span><span class="p">,</span>
            <span class="n">min_impurity_split</span><span class="o">=</span><span class="n">min_impurity_split</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="n">max_leaf_nodes</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
            <span class="n">validation_fraction</span><span class="o">=</span><span class="n">validation_fraction</span><span class="p">,</span>
            <span class="n">n_iter_no_change</span><span class="o">=</span><span class="n">n_iter_no_change</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span> <span class="n">ccp_alpha</span><span class="o">=</span><span class="n">ccp_alpha</span><span class="p">,</span>
            <span class="n">constraints</span><span class="o">=</span><span class="n">constraints</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validate_y</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">kind</span> <span class="o">==</span> <span class="s1">&#39;O&#39;</span><span class="p">:</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">DOUBLE</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y</span>

    <span class="k">def</span> <span class="nf">_warn_mae_for_criterion</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># TODO: This should raise an error from 1.1</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;criterion=&#39;mae&#39; was deprecated in version 0.24 and &quot;</span>
                      <span class="s2">&quot;will be removed in version 1.1 (renaming of 0.26). The &quot;</span>
                      <span class="s2">&quot;correct way of minimizing the absolute error is to use &quot;</span>
                      <span class="s2">&quot; loss=&#39;lad&#39; instead.&quot;</span><span class="p">,</span> <span class="ne">FutureWarning</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict regression target for X.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : ndarray of shape (n_samples,)</span>
<span class="sd">            The predicted values.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>
        <span class="c1"># In regression we can directly return the raw value from the trees.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">staged_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict regression target at each stage for X.</span>
<span class="sd">        This method allows monitoring (i.e. determine error on testing set)</span>
<span class="sd">        after each stage.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : generator of ndarray of shape (n_samples,)</span>
<span class="sd">            The predicted value of the input samples.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">raw_predictions</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_staged_raw_predict</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="k">yield</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Apply trees in the ensemble to X, return leaf indices.</span>
<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, its dtype will be converted to</span>
<span class="sd">            ``dtype=np.float32``. If a sparse matrix is provided, it will</span>
<span class="sd">            be converted to a sparse ``csr_matrix``.</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X_leaves : array-like of shape (n_samples, n_estimators)</span>
<span class="sd">            For each datapoint x in X and for each tree in the ensemble,</span>
<span class="sd">            return the index of the leaf x ends up in each estimator.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">leaves</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">leaves</span> <span class="o">=</span> <span class="n">leaves</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">leaves</span>

    <span class="c1"># FIXME: to be removed in 1.1</span>
    <span class="c1"># mypy error: Decorated property not supported</span>
    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;Attribute n_classes_ was deprecated &quot;</span>  <span class="c1"># type: ignore</span>
                <span class="s2">&quot;in version 0.24 and will be removed in 1.1 &quot;</span>
                <span class="s2">&quot;(renaming of 0.26).&quot;</span><span class="p">)</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">n_classes_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">except</span> <span class="n">NotFittedError</span> <span class="k">as</span> <span class="n">nfe</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> object has no n_classes_ attribute.&quot;</span>
                <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
            <span class="p">)</span> <span class="kn">from</span> <span class="nn">nfe</span>
        <span class="k">return</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="expedia-dataset">
<h2>Expedia Dataset<a class="headerlink" href="#expedia-dataset" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!wget -q --show-progress https://github.com/Swiggy/Moo-GBT/raw/master/examples/data/train_data_sample_2.csv
!head train_data_sample_2.csv
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train_data_sample_2 100%[===================&gt;]  41.47M   159MB/s    in 0.3s    
orig_destination_distance,is_mobile,is_package,srch_adults_cnt,srch_children_cnt,srch_rm_cnt,is_booking,cnt,d1,d2,d3,d4,d5,d6,d7,d8,d9,d10,d11,d12,d13,d14,d15,d16,d17,d18,d19,d20,d21,d22,d23,d24,d25,d26,d27,d28,d29,d30,d31,d32,d33,d34,d35,d36,d37,d38,d39,d40,d41,d42,d43,d44,d45,d46,d47,d48,d49,d50,d51,d52,d53,d54,d55,d56,d57,d58,d59,d60,d61,d62,d63,d64,d65,d66,d67,d68,d69,d70,d71,d72,d73,d74,d75,d76,d77,d78,d79,d80,d81,d82,d83,d84,d85,d86,d87,d88,d89,d90,d91,d92,d93,d94,d95,d96,d97,d98,d99,d100,d101,d102,d103,d104,d105,d106,d107,d108,d109,d110,d111,d112,d113,d114,d115,d116,d117,d118,d119,d120,d121,d122,d123,d124,d125,d126,d127,d128,d129,d130,d131,d132,d133,d134,d135,d136,d137,d138,d139,d140,d141,d142,d143,d144,d145,d146,d147,d148,d149
173.3135,0,0,2,1,1,0,1,-2.18730798456,-2.19921365688,-2.19921365688,-2.19921365688,-2.19921365688,-2.05652701607,-2.19921365688,-2.19921365688,-2.14147178951,-2.19921365688,-2.09449283689,-2.19921365688,-2.11262022307,-2.1937419145700003,-2.19921365688,-2.1892697358400004,-2.19880077561,-2.19789654871,-2.19720657457,-2.19921365688,-2.01842837791,-2.19921365688,-2.19720657457,-2.19921365688,-2.19921365688,-2.19921365688,-2.19921365688,-2.19921365688,-2.18515569902,-2.19921365688,-2.19921365688,-2.19720657457,-2.19921365688,-2.19921365688,-2.19921365688,-2.19921365688,-1.93192146961,-2.19124038877,-2.19921365688,-2.19921365688,-2.19888860595,-2.19921365688,-2.12529171935,-2.14923447977,-2.19921365688,-2.19921365688,-2.19124038877,-2.19921365688,-2.10311662401,-2.19921365688,-2.19921365688,-2.19921365688,-2.07693725914,-2.19921365688,-2.14270320246,-2.15706839465,-2.19921365688,-1.92127979726,-2.19921365688,-2.19921365688,-2.19921365688,-2.1834108678,-2.19921365688,-2.19921365688,-2.19921365688,-2.19921365688,-2.19921365688,-2.19921365688,-2.18147534541,-2.19921365688,-1.8000427139099997,-2.19921365688,-2.18147534541,-2.19841566449,-2.1935662146,-2.19718897512,-2.19899137302,-2.19921365688,-2.19921365688,-2.19921365688,-2.19921365688,-2.19921365688,-2.19921365688,-2.1990721026,-2.19921365688,-2.19921365688,-2.19921365688,-2.18665269041,-2.19720657457,-2.19921365688,-2.11177026934,-2.19921365688,-1.94808266923,-2.19921365688,-2.1932200245,-2.1879566876400003,-2.19720657457,-2.18573696088,-2.19921365688,-2.1952087253,-2.1588979194,-2.19921365688,-2.19921365688,-2.19921365688,-2.19921365688,-2.19720657457,-2.19921365688,-2.13139301227,-2.1981772743400003,-2.19921365688,-2.1932200245,-2.19921365688,-2.19921365688,-2.17344575826,-2.19124038877,-2.19921365688,-2.19921365688,-2.19921365688,-2.19921365688,-2.19921365688,-2.0428280295700003,-2.18118849358,-2.19921365688,-2.19921365688,-2.19921365688,-2.1954205464900003,-2.18730798456,-2.06080227709,-2.19921365688,-2.19921365688,-2.17381838005,-2.12200885943,-2.19921365688,-2.19921365688,-2.19921365688,-2.19921365688,-2.1892697358400004,-2.19921365688,-2.15450666473,-2.1588979194,-2.19921365688,-2.19921365688,-2.19921365688,-2.19921365688,-2.14803462242,-2.19921365688,-2.19921365688,-2.19720657457,-2.19921365688
36.7112,0,0,2,0,1,0,1,-2.29225147315,-2.29943723091,-2.29951639018,-2.08111774245,-2.01272641649,-1.84631088289,-2.25694293284,-2.2419289198500003,-2.29661889811,-2.27289357876,-2.08944597431,-2.29951639018,-2.07369341787,-2.12623021823,-2.29951639018,-1.90708238883,-2.22496898239,-2.13035167283,-2.29951639018,-2.29951249471,-2.28622599442,-2.2949371716900004,-2.29951639018,-2.24878320275,-2.25627922625,-2.27106252833,-2.07758088047,-2.2789245532900004,-2.01919896939,-2.17033430979,-2.29951639018,-2.29951639018,-2.29868226243,-2.13449074923,-2.1817876442200004,-2.29891987723,-1.51031359274,-2.29951638843,-2.29951639018,-2.2727538704700003,-2.12233363475,-2.2646069145200003,-1.861096587,-2.29951049379,-2.24979614841,-2.2599437095200003,-2.29234253518,-2.16916087384,-1.78586420185,-2.28423710917,-2.29355618508,-2.2880450341900005,-2.05052621808,-2.29951639018,-2.19273195665,-2.0559748470500003,-2.28706322107,-1.83595586077,-2.2717180871,-2.29951639018,-2.28358439181,-2.29951639018,-2.29951636728,-2.2711891898400003,-2.29951639018,-2.16604704624,-2.26580484652,-2.290142878,-2.28879572602,-2.29951639018,-2.27611532069,-2.2912463406400003,-2.16569226028,-2.09273216087,-2.12119035029,-2.29875027631,-2.26711502493,-1.87582618779,-2.231525479,-1.89300982035,-2.26256545701,-2.1593817667,-2.26187281285,-2.20483923822,-2.27674009638,-2.29951639018,-2.29943943004,-2.26871127854,-2.29701879781,-2.29951639018,-2.04654483581,-2.29951612093,-1.5615411274899995,-2.0834170963,-2.29951639018,-2.25161524538,-2.16701027975,-2.29951639018,-2.2863131288900003,-2.29951639018,-2.21803808202,-2.25840690625,-2.26918226622,-2.29443436627,-2.29129713595,-2.28877150463,-2.29951639018,-1.88060999312,-2.17263219298,-2.29826579857,-2.29784973368,-2.2505303639400003,-2.29951639018,-2.2883029679,-2.29951639018,-2.29951639018,-2.29868226243,-2.29951639018,-2.29868226243,-2.28223375548,-2.2167933579900003,-1.92421166225,-2.29951639018,-2.29851003895,-2.29907738072,-2.24186831008,-2.29867046062,-2.13543615299,-2.2422272593400003,-1.9868763298700003,-2.29720113035,-1.85824233386,-2.27904131926,-2.2980915226400005,-2.29951639018,-2.07470816125,-2.1293160654400003,-2.29951639018,-1.7357019870400003,-2.29826579857,-2.14536209421,-2.28940517509,-2.29951639018,-2.29340208978,-2.29868226243,-2.29951639018,-2.29322328031,-2.29951639018,-2.21700747965
6027.9528,0,0,5,0,3,0,1,-2.18601998041,-2.28056825772,-2.29019720014,-2.12694256006,-2.09254428558,-1.93225836938,-2.2567418740900003,-2.27487783382,-1.69029654535,-2.2488744229,-2.17512143322,-2.29019720014,-2.04966722954,-2.0227828052400003,-2.29019720014,-2.03677768041,-2.23518049895,-2.25631380196,-2.28890956744,-2.18252697187,-2.28462095615,-2.29019720014,-2.28942416249,-2.27279456232,-2.29019720014,-2.27396892582,-2.2840924795599995,-2.27982640015,-1.98104432747,-2.20639665304,-2.28705979197,-2.286757079,-2.29019720014,-2.2314529776,-2.26233758585,-2.22614170912,-1.56662906645,-2.22430515038,-2.2478515335,-2.28005736508,-2.24519749671,-2.28315036127,-1.80241789844,-2.1100185649200003,-2.23474558019,-2.28443163592,-2.22079500675,-2.27005806999,-1.85442858091,-2.2666461782,-2.284805427,-2.27658628843,-2.21489993542,-2.29019720014,-2.10908043078,-2.04952286912,-2.28629731876,-1.87229237492,-2.28910149153,-2.28644001392,-2.27994462023,-2.29019534473,-2.2886694270400003,-2.28984971827,-2.29019720014,-2.04720731695,-2.28494549908,-2.28672455042,-2.24530755781,-2.29019720014,-2.26459047619,-2.28813881633,-2.2815105658,-2.16383018935,-2.13027684252,-2.12625210231,-2.25149493219,-2.24141905049,-2.27383427636,-2.15056442037,-2.28673884572,-2.28742881559,-2.22781979171,-2.18590453579,-2.13743442502,-2.29019720014,-2.28981906095,-2.08699900326,-2.28788220297,-2.29019720014,-2.05108971688,-2.2896341009,-1.64000539234,-2.28600335514,-2.28150682337,-2.2414787426400005,-2.28890956744,-2.29019720014,-2.28434446096,-2.2659568711,-2.08856323903,-2.22253795707,-1.83087085792,-2.24471361274,-2.22972275521,-2.28019327215,-2.28984196847,-1.8786253951,-2.26484916062,-2.28861425564,-2.26930162243,-2.11915724866,-2.29019720014,-2.06992947498,-2.2638191311400004,-2.29019720014,-2.29019720014,-2.29019720014,-2.28993936801,-2.28243803241,-2.12567656646,-2.0240233856,-2.29019720014,-2.23965883005,-2.28718665775,-2.24254121466,-2.21539626656,-2.08143046069,-2.29019720014,-2.20915053632,-2.20602296922,-2.1397147609,-2.27964232683,-2.10483461346,-2.29019720014,-2.2119380686200003,-2.0461717291,-2.29019720014,-1.82075990551,-2.25360192787,-2.2819954742,-2.28559052961,-2.19656938464,-2.23702637874,-2.25694365822,-2.25443851095,-2.27929366193,-2.29019720014,-2.03120316058
15000.0,0,0,4,0,2,0,1,-2.22419599385,-2.26119948801,-2.26185367538,-2.03289354844,-1.95143161453,-1.97346730388,-2.26185367538,-2.26185367538,-2.18992285201,-2.25660580135,-2.18650921861,-2.26185367538,-2.1790719945,-2.09469335232,-2.26185367538,-2.12907935289,-2.06133554754,-2.24811878996,-2.26185367538,-2.26125806819,-2.13904037137,-2.0924999578400003,-2.26185367538,-2.19460566658,-2.26135594427,-2.25304637363,-2.08565131713,-2.1991328305,-2.10783978263,-2.25642969249,-2.26185367538,-2.23161301391,-2.26185367538,-2.0781141044,-2.07183231045,-2.26174418822,-1.6339833762399998,-2.22734094963,-2.26185367538,-2.2590971845,-2.21168778343,-2.18839856144,-1.9382503167400005,-2.26121959865,-2.20427953229,-2.16791678936,-2.26151415643,-2.2259776664400004,-1.93475785471,-2.24966082651,-2.26180328715,-2.25872922597,-2.05382155346,-2.26184996044,-2.2458394790700003,-2.22037380564,-2.22903781367,-1.87617021491,-2.26158184817,-2.26179605657,-2.26112926603,-2.26185367538,-2.26185367401,-2.2617403302900003,-2.26185367538,-2.16254026064,-2.2148434793000003,-2.26130201546,-2.2463181961200003,-2.26185367538,-2.1148846266900003,-2.2607229762400003,-2.18679474851,-2.0808057481,-2.21818861188,-2.26163731884,-2.07475550982,-2.0501402255400003,-2.26182002503,-1.91806686483,-2.1834712680900004,-2.26140104241,-2.2457616817700004,-2.2364959727,-2.21201035067,-2.26185367538,-2.1568034114,-2.17279381302,-2.26185367538,-2.26180328715,-2.08252457729,-2.26185367538,-1.71205669693,-2.0754534601,-2.26185367538,-2.03961031569,-2.24670467738,-2.1902277602,-2.2611733977,-2.26185367538,-2.26073813092,-2.12210487043,-2.24835331605,-2.26061130004,-2.26063170896,-2.26083591378,-2.26185367538,-1.9099648839400003,-2.09394340511,-2.26185367538,-2.2617404729,-2.2216511064,-2.26185367538,-2.22825211883,-2.26185367538,-2.26185367538,-2.26185367538,-2.26185367538,-2.26185367538,-2.24994713758,-2.20486259346,-2.14997456852,-2.26185367538,-2.20337557574,-2.26185367538,-2.23614135354,-2.26185367538,-2.14471127695,-2.26113680092,-2.06278851398,-2.07524827963,-1.91616339078,-2.25770936008,-2.23591570758,-2.26129268181,-2.22435719139,-2.1004484108000003,-2.26185230556,-1.97361835043,-2.26151415643,-2.1030315709000003,-2.2613776887,-2.26185367538,-2.26185367538,-2.26162729993,-2.26185367538,-2.26185366412,-2.26185367538,-2.24909093915
3856.6132,0,0,1,0,1,0,1,-2.24696072756,-2.17808779039,-2.2506122251,-2.24788585081,-2.13394526832,-1.81520229165,-2.2506122251,-2.2506122251,-2.2506122251,-2.23056465855,-2.22697803961,-2.2506122251,-2.1806387976,-2.04830249715,-2.2506122251,-2.00599560546,-2.09167893128,-2.24388029705,-2.2506122251,-2.24255676581,-2.24745466479,-2.20094287435,-2.2506122251,-2.22306966411,-2.2506122251,-2.2026569847,-2.1377428161900003,-2.24158961085,-1.9965524995400004,-2.15964689283,-2.2506122251,-2.2506122251,-2.2506122251,-2.13890904099,-2.2005211754400005,-2.2503643607900004,-1.5851139117200002,-2.2506122251,-2.2506122251,-2.24768388907,-2.2236386446200003,-2.24755981256,-1.97743107426,-2.2506122251,-2.17808779039,-2.24888801991,-2.20094287435,-2.24818921934,-1.89230405069,-2.17992942942,-2.2506122251,-2.2485794080000003,-1.98723049691,-2.2506122251,-2.2418857945900004,-2.24768388907,-2.2506122251,-1.95577696286,-2.2506122251,-2.2506122251,-2.2506122251,-2.2506122251,-2.2506122251,-2.2506122251,-2.2506122251,-2.2506122251,-2.2506122251,-2.2506122251,-2.2506122251,-2.2506122251,-2.1873000808,-2.2506122251,-2.17709675991,-2.09159474049,-2.09225128957,-2.2506122251,-2.21122849347,-2.14404466629,-2.2506122251,-1.8118717338,-2.1873000808,-2.2506122251,-2.18642032498,-2.06761651765,-2.18234992186,-2.2506122251,-2.1873000808,-2.24596350136,-2.2506122251,-2.2506122251,-2.12086692442,-2.2506122251,-1.61740275031,-2.2506122251,-2.2506122251,-2.24145953397,-2.2506122251,-2.2506122251,-2.25020608952,-2.2506122251,-2.24622709231,-2.2506122251,-2.2506122251,-2.2506122251,-2.24789045241,-2.2506122251,-2.2506122251,-1.80608972824,-2.18595277872,-2.2491455889700003,-2.2506122251,-2.24970750856,-2.2506122251,-2.2506122251,-2.2506122251,-2.2506122251,-2.2506122251,-2.2506122251,-2.2506122251,-2.2506122251,-2.14494828945,-2.04132993987,-2.2506122251,-2.19770592014,-2.2506122251,-2.2506122251,-2.2506122251,-2.22720726721,-2.2506122251,-2.03422129054,-2.2506122251,-2.00919420278,-2.24321612491,-2.2506122251,-2.2506122251,-2.24140529963,-2.11299017593,-2.2506122251,-1.9224319860900003,-2.2491455889700003,-2.25059682252,-2.2506122251,-2.1873000808,-2.1873000808,-2.2506122251,-2.2506122251,-2.2506122251,-2.2506122251,-2.2506122251
6464.1636,0,0,1,0,1,0,1,-2.13894477873,-2.23823095257,-2.23823095257,-2.01365607922,-1.99315180817,-2.00293745867,-2.23823095257,-2.23823095257,-2.14162918085,-2.18356833564,-2.10756584128,-2.23823095257,-2.20639215006,-2.1584428464200003,-2.23823095257,-2.0979625944,-2.19961060013,-2.18356833564,-2.23823095257,-2.2187979019,-2.23766254095,-2.19841423061,-2.23823095257,-2.17806418146,-2.23823095257,-2.2377934548800003,-2.16326005876,-2.23823095257,-2.13787175382,-2.23154648673,-2.23823095257,-2.23769413888,-2.2328924429900003,-2.01316699478,-2.14286112395,-2.2068916126,-1.81294032672,-2.1558196195,-2.23823095257,-2.23555349496,-2.15507688415,-2.22456644542,-2.12213459312,-2.23448711677,-2.20336579166,-2.23823095257,-2.2068916126,-2.20753816791,-1.9888114159,-2.23823095257,-2.23823095257,-2.23782950305,-2.12294950786,-2.2068916126,-2.22111508746,-2.10994298444,-2.07107526028,-2.04163108471,-2.23823095257,-2.23823095257,-2.12005388168,-2.2068916126,-2.23823095257,-2.18356833564,-2.23823095257,-2.16015123966,-2.17153932171,-2.23823095257,-2.18356833564,-2.23823095257,-2.17884787989,-2.23823095257,-2.21894281254,-2.14076445921,-2.22122270757,-2.2373010301900003,-2.0168560923900003,-2.08792106357,-1.9780545591900005,-1.9891689473400005,-2.23823095257,-2.17511596581,-2.2068916126,-2.17244232998,-2.0684877166900004,-2.23823095257,-2.23823095257,-2.18356833564,-2.1544887473700003,-2.23823095257,-2.11138073407,-2.23823095257,-1.83197159787,-2.1783181761700003,-2.22456644542,-2.18808256429,-2.23823095257,-2.23823095257,-2.23823095257,-2.23823095257,-2.23769413888,-2.11957238481,-2.20639215006,-2.155841653,-2.17530529865,-2.23555349496,-2.23823095257,-2.02166890836,-2.2343503792400004,-2.10964623473,-2.23448711677,-2.23823095257,-2.23823095257,-2.23823095257,-2.23823095257,-2.23823095257,-2.20044297872,-2.23715798791,-2.23715798791,-2.23823095257,-2.17851423997,-2.07754945988,-2.23823095257,-2.18356833564,-2.23823095257,-2.23681388938,-2.15537554215,-2.12167871241,-2.23823095257,-2.147532042,-2.22085733774,-2.02171406729,-2.23823095257,-2.23823095257,-2.23823095257,-2.23808005575,-2.13057625921,-2.2068916126,-2.05938663621,-2.23823095257,-2.23823095257,-2.23823095257,-2.23823095257,-2.23823095257,-2.23823095257,-2.07444112384,-2.23823095257,-2.23823095257,-2.2068703214
221.6766,0,0,4,0,1,0,1,-2.25860995379,-2.18757463381,-2.26083456066,-2.1082382951400005,-2.0326764726,-1.86656872548,-2.26083456066,-2.26083456066,-2.18564153032,-1.99821535692,-2.05738851807,-2.26083456066,-2.06930526485,-2.03273607588,-2.26083456066,-2.02440036705,-2.04623527971,-2.23331571379,-2.26083456066,-2.25469462294,-2.07903689021,-2.22563581719,-2.26083456066,-2.17952743751,-2.26083456066,-2.2586411206,-2.08933612024,-2.2577742031400003,-2.00816278107,-2.2101948406200003,-2.26083456066,-2.25750190901,-2.26083456066,-2.20031006385,-2.22072977693,-2.26083456066,-1.68622160281,-2.26083456066,-2.26083456066,-2.23967981644,-2.18348518302,-2.2494171623900003,-2.0311440146,-2.19489806832,-2.26083456066,-2.25972658069,-2.2361818004,-2.2603395353,-1.96846778352,-2.24587267133,-2.25079124804,-2.25752666272,-1.87541417237,-2.26083456066,-2.21778076661,-2.23085361112,-2.26083456066,-1.92130249258,-2.26083456066,-2.26083456066,-2.17325099973,-2.26083456066,-2.26083456066,-2.26083456066,-2.26083456066,-2.12338754266,-2.240019611,-2.26083456066,-2.23384417716,-2.26083456066,-2.16381705449,-2.26083456066,-2.10914160622,-2.17368053444,-2.14425792981,-2.25072689394,-2.18886909545,-2.11675790155,-2.23179403325,-1.98752164401,-2.24587267133,-2.25972083282,-2.21881235885,-2.17574982571,-2.25060148659,-2.26083456066,-2.19588690524,-1.9859171935,-2.25972083282,-2.26083456066,-2.14801835704,-2.26053305869,-1.71927203712,-2.23206936222,-2.26083456066,-2.17046927808,-2.26083456066,-2.18353295482,-2.25697920234,-2.26083456066,-2.2380563876700004,-2.25559989155,-2.17619824333,-2.26083456066,-2.25454829555,-2.25419463656,-2.26083456066,-1.87473371565,-2.1443221145400004,-2.26083456066,-2.25972083282,-2.25969685273,-2.25972083282,-2.18170757977,-2.25972083282,-2.26083456066,-2.26083456066,-2.26083456066,-2.25972083282,-2.24267812405,-2.15790955011,-2.10011143089,-2.26083456066,-2.24960023477,-2.25961317095,-2.24011300618,-2.25860995379,-2.20926147322,-2.24267812405,-2.1739378996700003,-1.97932026964,-1.9182458585,-2.22100062215,-2.2462706316,-2.26083456066,-2.25457833315,-2.18743558381,-2.26083456066,-1.8177207826,-2.25309778567,-2.26083456066,-2.26083456066,-2.26083456066,-2.24267812405,-2.25309778567,-2.26083456066,-2.26083456066,-2.26083456066,-2.24036250207
10372.0472,0,0,2,0,1,0,1,-2.27256440389,-2.24100193624,-2.37491252216,-2.22245847729,-1.86982920934,-1.7093744971200002,-2.23137276,-2.19658794872,-2.2112851104599995,-2.2454380836900003,-2.06336630383,-2.37491252216,-2.09352188407,-1.97185029025,-2.37491252216,-1.8692211117,-2.06822829755,-2.36327425637,-2.37491252216,-2.3744229426,-2.2805486485900004,-2.34307583125,-2.37491252216,-2.31361589755,-2.30239885158,-2.24446757268,-2.22286547286,-2.33913053072,-1.78981808075,-2.28421345222,-2.37491252216,-2.37491252216,-2.37491252216,-2.20811714173,-2.13021044289,-2.37414058032,-1.46941693843,-2.37491252216,-2.37491252216,-2.36654734392,-2.22346383783,-2.36605371706,-1.81115831501,-2.14092186377,-2.20328894288,-2.32789770397,-2.28059756293,-2.318578677,-1.7294176819599998,-2.27819983307,-2.30814414065,-2.35041487403,-2.06403532909,-2.24135309198,-2.32603770048,-2.21820580651,-2.21610575685,-1.82907128382,-2.3727134504400005,-2.37491252216,-2.37007360598,-2.37491252216,-2.37478404221,-2.23994995993,-2.37491252216,-2.1062085635,-2.28540271477,-2.24892130733,-2.37104159232,-2.37491252216,-2.20706687628,-2.36292866791,-2.09505426782,-1.91445701581,-2.0037273589400004,-2.37365181844,-2.23439595801,-2.12088081875,-2.30243523601,-1.84613099912,-2.2361832994,-2.37442676488,-2.21104806614,-2.11612584665,-2.2635003497,-2.37491252216,-2.25076905553,-2.25905413509,-2.37394155032,-2.37414548581,-2.02175237962,-2.37481555211,-1.5164219006,-2.15644460055,-2.37345687726,-2.27247546724,-2.37491252216,-2.37491252216,-2.36699004493,-2.3749083418200003,-2.36863392631,-2.18445479248,-2.16465709563,-2.37389417926,-2.27544744471,-2.37345687726,-2.37491252216,-1.70056188609,-2.15517140784,-2.27949400397,-2.37491252216,-2.06331104793,-2.37442676488,-2.27268461254,-2.37491252216,-2.37491252216,-2.37491252216,-2.37491252216,-2.37394155032,-2.26357079121,-2.28409264477,-1.92372177149,-2.37491252216,-2.37132942659,-2.3735557026,-2.21170118272,-2.37248449929,-2.3047446614,-2.29884095499,-1.7624476132,-2.33121560301,-1.85334295206,-2.30442526165,-2.3371092947400003,-2.37491252216,-2.15385046987,-1.9400424805,-2.3749098439500003,-1.5967449460799998,-2.37442676488,-2.31476077886,-2.37491252216,-2.37491252216,-2.33747962282,-2.37442676488,-2.24108159558,-2.37234869604,-2.37491252216,-2.14203606428
136.4946,0,0,2,2,1,0,1,-2.20737318671,-2.2215897967400005,-2.2215897967400005,-2.2215897967400005,-2.11667837088,-2.0717695707900003,-2.21083143722,-2.20689767464,-2.21687556572,-2.20614008127,-2.18341384982,-2.2215897967400005,-2.16173001387,-2.09620710423,-2.2215897967400005,-1.97858173194,-2.20293282437,-2.17594108839,-2.22015819319,-2.21961612095,-2.2215897967400005,-2.2215897967400005,-2.2187312932700003,-2.2146315641900003,-2.2215897967400005,-2.2169479123,-2.21654488203,-2.2215897967400005,-2.0032234742200004,-2.19607908098,-2.2215897967400005,-2.2074817898,-2.2074817898,-2.2215897967400005,-2.22056500262,-2.2215897967400005,-1.7620458276,-2.2215897967400005,-2.2215897967400005,-2.2187312932700003,-2.2115508134,-2.2215897967400005,-1.9781906343,-2.2215897967400005,-2.22149894291,-2.22094383229,-2.13629310592,-2.18256345861,-1.94929708026,-2.1952756364400003,-2.06695589,-2.20742994163,-2.15237239807,-2.18708137239,-2.16964544729,-1.99347557765,-2.2215897967400005,-2.11671791427,-2.2215897967400005,-2.2215897967400005,-2.18955939306,-2.2215897967400005,-2.2215897967400005,-2.2215897967400005,-2.2215897967400005,-2.2215897967400005,-2.2215897967400005,-2.2215897967400005,-2.21307011815,-2.2215897967400005,-2.1442388371,-2.2215897967400005,-2.20338415208,-2.19664453682,-2.15486898592,-2.2215897967400005,-2.2105417926900004,-2.2215897967400005,-2.00163642449,-2.17805446689,-2.2215897967400005,-2.0778688108,-2.07660946001,-2.19100349475,-2.21995111878,-2.2215897967400005,-2.2215897967400005,-2.2215897967400005,-2.21026696549,-2.2215897967400005,-2.03359918552,-2.2215897967400005,-1.80585197533,-2.11246038804,-2.21589148136,-2.21536034306,-2.22015819319,-2.2215897967400005,-2.2215897967400005,-2.22147363031,-2.21730906616,-2.21039924741,-2.18708137239,-2.2215897967400005,-2.21586212836,-2.2215897967400005,-2.2215897967400005,-2.02129857224,-2.18714519275,-2.16794025951,-2.2215897967400005,-2.2215897967400005,-2.21730906616,-2.20812655618,-2.2215897967400005,-2.2187312932700003,-2.21589148136,-2.21730906616,-2.17899326637,-2.22158850772,-2.1242360389900004,-2.01984806048,-2.2215897967400005,-2.2215897967400005,-2.2215897967400005,-2.2188819617,-2.2215897967400005,-2.05586775259,-2.2215897967400005,-2.1989915574900003,-2.21702568155,-2.11660190475,-2.2215897967400005,-2.2215897967400005,-2.2215897967400005,-2.13487138744,-2.19381769717,-2.2215897967400005,-2.01735460027,-2.21307011815,-2.1028227807700004,-2.21891786842,-2.2215897967400005,-2.2149546101900004,-2.2215897967400005,-2.2215897967400005,-2.2215897967400005,-2.2215897967400005,-2.21905277372
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="o">*</span>

<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span> 

<span class="kn">import</span> <span class="nn">sys</span>

<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;display.max_columns&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;axes.labelsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;axes.titlesize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;font.size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;20&quot;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;legend.fontsize&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">12</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="data-preparation">
<h2>Data Preparation<a class="headerlink" href="#data-preparation" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">train_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;train_data_sample_2.csv&#39;</span><span class="p">)</span>
<span class="n">train_data</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(18751, 157)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">train_data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>orig_destination_distance</th>
      <th>is_mobile</th>
      <th>is_package</th>
      <th>srch_adults_cnt</th>
      <th>srch_children_cnt</th>
      <th>srch_rm_cnt</th>
      <th>is_booking</th>
      <th>cnt</th>
      <th>d1</th>
      <th>d2</th>
      <th>d3</th>
      <th>d4</th>
      <th>d5</th>
      <th>d6</th>
      <th>d7</th>
      <th>d8</th>
      <th>d9</th>
      <th>d10</th>
      <th>d11</th>
      <th>d12</th>
      <th>d13</th>
      <th>d14</th>
      <th>d15</th>
      <th>d16</th>
      <th>d17</th>
      <th>d18</th>
      <th>d19</th>
      <th>d20</th>
      <th>d21</th>
      <th>d22</th>
      <th>d23</th>
      <th>d24</th>
      <th>d25</th>
      <th>d26</th>
      <th>d27</th>
      <th>d28</th>
      <th>d29</th>
      <th>d30</th>
      <th>d31</th>
      <th>d32</th>
      <th>d33</th>
      <th>d34</th>
      <th>d35</th>
      <th>d36</th>
      <th>d37</th>
      <th>d38</th>
      <th>d39</th>
      <th>d40</th>
      <th>d41</th>
      <th>d42</th>
      <th>d43</th>
      <th>d44</th>
      <th>d45</th>
      <th>d46</th>
      <th>d47</th>
      <th>d48</th>
      <th>d49</th>
      <th>d50</th>
      <th>d51</th>
      <th>d52</th>
      <th>d53</th>
      <th>d54</th>
      <th>d55</th>
      <th>d56</th>
      <th>d57</th>
      <th>d58</th>
      <th>d59</th>
      <th>d60</th>
      <th>d61</th>
      <th>d62</th>
      <th>d63</th>
      <th>d64</th>
      <th>d65</th>
      <th>d66</th>
      <th>d67</th>
      <th>d68</th>
      <th>d69</th>
      <th>d70</th>
      <th>d71</th>
      <th>d72</th>
      <th>d73</th>
      <th>d74</th>
      <th>d75</th>
      <th>d76</th>
      <th>d77</th>
      <th>d78</th>
      <th>d79</th>
      <th>d80</th>
      <th>d81</th>
      <th>d82</th>
      <th>d83</th>
      <th>d84</th>
      <th>d85</th>
      <th>d86</th>
      <th>d87</th>
      <th>d88</th>
      <th>d89</th>
      <th>d90</th>
      <th>d91</th>
      <th>d92</th>
      <th>d93</th>
      <th>d94</th>
      <th>d95</th>
      <th>d96</th>
      <th>d97</th>
      <th>d98</th>
      <th>d99</th>
      <th>d100</th>
      <th>d101</th>
      <th>d102</th>
      <th>d103</th>
      <th>d104</th>
      <th>d105</th>
      <th>d106</th>
      <th>d107</th>
      <th>d108</th>
      <th>d109</th>
      <th>d110</th>
      <th>d111</th>
      <th>d112</th>
      <th>d113</th>
      <th>d114</th>
      <th>d115</th>
      <th>d116</th>
      <th>d117</th>
      <th>d118</th>
      <th>d119</th>
      <th>d120</th>
      <th>d121</th>
      <th>d122</th>
      <th>d123</th>
      <th>d124</th>
      <th>d125</th>
      <th>d126</th>
      <th>d127</th>
      <th>d128</th>
      <th>d129</th>
      <th>d130</th>
      <th>d131</th>
      <th>d132</th>
      <th>d133</th>
      <th>d134</th>
      <th>d135</th>
      <th>d136</th>
      <th>d137</th>
      <th>d138</th>
      <th>d139</th>
      <th>d140</th>
      <th>d141</th>
      <th>d142</th>
      <th>d143</th>
      <th>d144</th>
      <th>d145</th>
      <th>d146</th>
      <th>d147</th>
      <th>d148</th>
      <th>d149</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>173.3135</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>-2.187308</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.056527</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.141472</td>
      <td>-2.199214</td>
      <td>-2.094493</td>
      <td>-2.199214</td>
      <td>-2.112620</td>
      <td>-2.193742</td>
      <td>-2.199214</td>
      <td>-2.189270</td>
      <td>-2.198801</td>
      <td>-2.197897</td>
      <td>-2.197207</td>
      <td>-2.199214</td>
      <td>-2.018428</td>
      <td>-2.199214</td>
      <td>-2.197207</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.185156</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.197207</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-1.931921</td>
      <td>-2.191240</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.198889</td>
      <td>-2.199214</td>
      <td>-2.125292</td>
      <td>-2.149234</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.191240</td>
      <td>-2.199214</td>
      <td>-2.103117</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.076937</td>
      <td>-2.199214</td>
      <td>-2.142703</td>
      <td>-2.157068</td>
      <td>-2.199214</td>
      <td>-1.921280</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.183411</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.181475</td>
      <td>-2.199214</td>
      <td>-1.800043</td>
      <td>-2.199214</td>
      <td>-2.181475</td>
      <td>-2.198416</td>
      <td>-2.193566</td>
      <td>-2.197189</td>
      <td>-2.198991</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199072</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.186653</td>
      <td>-2.197207</td>
      <td>-2.199214</td>
      <td>-2.111770</td>
      <td>-2.199214</td>
      <td>-1.948083</td>
      <td>-2.199214</td>
      <td>-2.193220</td>
      <td>-2.187957</td>
      <td>-2.197207</td>
      <td>-2.185737</td>
      <td>-2.199214</td>
      <td>-2.195209</td>
      <td>-2.158898</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.197207</td>
      <td>-2.199214</td>
      <td>-2.131393</td>
      <td>-2.198177</td>
      <td>-2.199214</td>
      <td>-2.193220</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.173446</td>
      <td>-2.191240</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.042828</td>
      <td>-2.181188</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.195421</td>
      <td>-2.187308</td>
      <td>-2.060802</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.173818</td>
      <td>-2.122009</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.189270</td>
      <td>-2.199214</td>
      <td>-2.154507</td>
      <td>-2.158898</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.148035</td>
      <td>-2.199214</td>
      <td>-2.199214</td>
      <td>-2.197207</td>
      <td>-2.199214</td>
    </tr>
    <tr>
      <th>1</th>
      <td>36.7112</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>-2.292251</td>
      <td>-2.299437</td>
      <td>-2.299516</td>
      <td>-2.081118</td>
      <td>-2.012726</td>
      <td>-1.846311</td>
      <td>-2.256943</td>
      <td>-2.241929</td>
      <td>-2.296619</td>
      <td>-2.272894</td>
      <td>-2.089446</td>
      <td>-2.299516</td>
      <td>-2.073693</td>
      <td>-2.126230</td>
      <td>-2.299516</td>
      <td>-1.907082</td>
      <td>-2.224969</td>
      <td>-2.130352</td>
      <td>-2.299516</td>
      <td>-2.299512</td>
      <td>-2.286226</td>
      <td>-2.294937</td>
      <td>-2.299516</td>
      <td>-2.248783</td>
      <td>-2.256279</td>
      <td>-2.271063</td>
      <td>-2.077581</td>
      <td>-2.278925</td>
      <td>-2.019199</td>
      <td>-2.170334</td>
      <td>-2.299516</td>
      <td>-2.299516</td>
      <td>-2.298682</td>
      <td>-2.134491</td>
      <td>-2.181788</td>
      <td>-2.298920</td>
      <td>-1.510314</td>
      <td>-2.299516</td>
      <td>-2.299516</td>
      <td>-2.272754</td>
      <td>-2.122334</td>
      <td>-2.264607</td>
      <td>-1.861097</td>
      <td>-2.299510</td>
      <td>-2.249796</td>
      <td>-2.259944</td>
      <td>-2.292343</td>
      <td>-2.169161</td>
      <td>-1.785864</td>
      <td>-2.284237</td>
      <td>-2.293556</td>
      <td>-2.288045</td>
      <td>-2.050526</td>
      <td>-2.299516</td>
      <td>-2.192732</td>
      <td>-2.055975</td>
      <td>-2.287063</td>
      <td>-1.835956</td>
      <td>-2.271718</td>
      <td>-2.299516</td>
      <td>-2.283584</td>
      <td>-2.299516</td>
      <td>-2.299516</td>
      <td>-2.271189</td>
      <td>-2.299516</td>
      <td>-2.166047</td>
      <td>-2.265805</td>
      <td>-2.290143</td>
      <td>-2.288796</td>
      <td>-2.299516</td>
      <td>-2.276115</td>
      <td>-2.291246</td>
      <td>-2.165692</td>
      <td>-2.092732</td>
      <td>-2.121190</td>
      <td>-2.298750</td>
      <td>-2.267115</td>
      <td>-1.875826</td>
      <td>-2.231525</td>
      <td>-1.893010</td>
      <td>-2.262565</td>
      <td>-2.159382</td>
      <td>-2.261873</td>
      <td>-2.204839</td>
      <td>-2.276740</td>
      <td>-2.299516</td>
      <td>-2.299439</td>
      <td>-2.268711</td>
      <td>-2.297019</td>
      <td>-2.299516</td>
      <td>-2.046545</td>
      <td>-2.299516</td>
      <td>-1.561541</td>
      <td>-2.083417</td>
      <td>-2.299516</td>
      <td>-2.251615</td>
      <td>-2.167010</td>
      <td>-2.299516</td>
      <td>-2.286313</td>
      <td>-2.299516</td>
      <td>-2.218038</td>
      <td>-2.258407</td>
      <td>-2.269182</td>
      <td>-2.294434</td>
      <td>-2.291297</td>
      <td>-2.288772</td>
      <td>-2.299516</td>
      <td>-1.880610</td>
      <td>-2.172632</td>
      <td>-2.298266</td>
      <td>-2.297850</td>
      <td>-2.250530</td>
      <td>-2.299516</td>
      <td>-2.288303</td>
      <td>-2.299516</td>
      <td>-2.299516</td>
      <td>-2.298682</td>
      <td>-2.299516</td>
      <td>-2.298682</td>
      <td>-2.282234</td>
      <td>-2.216793</td>
      <td>-1.924212</td>
      <td>-2.299516</td>
      <td>-2.298510</td>
      <td>-2.299077</td>
      <td>-2.241868</td>
      <td>-2.298670</td>
      <td>-2.135436</td>
      <td>-2.242227</td>
      <td>-1.986876</td>
      <td>-2.297201</td>
      <td>-1.858242</td>
      <td>-2.279041</td>
      <td>-2.298092</td>
      <td>-2.299516</td>
      <td>-2.074708</td>
      <td>-2.129316</td>
      <td>-2.299516</td>
      <td>-1.735702</td>
      <td>-2.298266</td>
      <td>-2.145362</td>
      <td>-2.289405</td>
      <td>-2.299516</td>
      <td>-2.293402</td>
      <td>-2.298682</td>
      <td>-2.299516</td>
      <td>-2.293223</td>
      <td>-2.299516</td>
      <td>-2.217007</td>
    </tr>
    <tr>
      <th>2</th>
      <td>6027.9528</td>
      <td>0</td>
      <td>0</td>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>1</td>
      <td>-2.186020</td>
      <td>-2.280568</td>
      <td>-2.290197</td>
      <td>-2.126943</td>
      <td>-2.092544</td>
      <td>-1.932258</td>
      <td>-2.256742</td>
      <td>-2.274878</td>
      <td>-1.690297</td>
      <td>-2.248874</td>
      <td>-2.175121</td>
      <td>-2.290197</td>
      <td>-2.049667</td>
      <td>-2.022783</td>
      <td>-2.290197</td>
      <td>-2.036778</td>
      <td>-2.235180</td>
      <td>-2.256314</td>
      <td>-2.288910</td>
      <td>-2.182527</td>
      <td>-2.284621</td>
      <td>-2.290197</td>
      <td>-2.289424</td>
      <td>-2.272795</td>
      <td>-2.290197</td>
      <td>-2.273969</td>
      <td>-2.284092</td>
      <td>-2.279826</td>
      <td>-1.981044</td>
      <td>-2.206397</td>
      <td>-2.287060</td>
      <td>-2.286757</td>
      <td>-2.290197</td>
      <td>-2.231453</td>
      <td>-2.262338</td>
      <td>-2.226142</td>
      <td>-1.566629</td>
      <td>-2.224305</td>
      <td>-2.247852</td>
      <td>-2.280057</td>
      <td>-2.245197</td>
      <td>-2.283150</td>
      <td>-1.802418</td>
      <td>-2.110019</td>
      <td>-2.234746</td>
      <td>-2.284432</td>
      <td>-2.220795</td>
      <td>-2.270058</td>
      <td>-1.854429</td>
      <td>-2.266646</td>
      <td>-2.284805</td>
      <td>-2.276586</td>
      <td>-2.214900</td>
      <td>-2.290197</td>
      <td>-2.109080</td>
      <td>-2.049523</td>
      <td>-2.286297</td>
      <td>-1.872292</td>
      <td>-2.289101</td>
      <td>-2.286440</td>
      <td>-2.279945</td>
      <td>-2.290195</td>
      <td>-2.288669</td>
      <td>-2.289850</td>
      <td>-2.290197</td>
      <td>-2.047207</td>
      <td>-2.284945</td>
      <td>-2.286725</td>
      <td>-2.245308</td>
      <td>-2.290197</td>
      <td>-2.264590</td>
      <td>-2.288139</td>
      <td>-2.281511</td>
      <td>-2.163830</td>
      <td>-2.130277</td>
      <td>-2.126252</td>
      <td>-2.251495</td>
      <td>-2.241419</td>
      <td>-2.273834</td>
      <td>-2.150564</td>
      <td>-2.286739</td>
      <td>-2.287429</td>
      <td>-2.227820</td>
      <td>-2.185905</td>
      <td>-2.137434</td>
      <td>-2.290197</td>
      <td>-2.289819</td>
      <td>-2.086999</td>
      <td>-2.287882</td>
      <td>-2.290197</td>
      <td>-2.051090</td>
      <td>-2.289634</td>
      <td>-1.640005</td>
      <td>-2.286003</td>
      <td>-2.281507</td>
      <td>-2.241479</td>
      <td>-2.288910</td>
      <td>-2.290197</td>
      <td>-2.284344</td>
      <td>-2.265957</td>
      <td>-2.088563</td>
      <td>-2.222538</td>
      <td>-1.830871</td>
      <td>-2.244714</td>
      <td>-2.229723</td>
      <td>-2.280193</td>
      <td>-2.289842</td>
      <td>-1.878625</td>
      <td>-2.264849</td>
      <td>-2.288614</td>
      <td>-2.269302</td>
      <td>-2.119157</td>
      <td>-2.290197</td>
      <td>-2.069929</td>
      <td>-2.263819</td>
      <td>-2.290197</td>
      <td>-2.290197</td>
      <td>-2.290197</td>
      <td>-2.289939</td>
      <td>-2.282438</td>
      <td>-2.125677</td>
      <td>-2.024023</td>
      <td>-2.290197</td>
      <td>-2.239659</td>
      <td>-2.287187</td>
      <td>-2.242541</td>
      <td>-2.215396</td>
      <td>-2.081430</td>
      <td>-2.290197</td>
      <td>-2.209151</td>
      <td>-2.206023</td>
      <td>-2.139715</td>
      <td>-2.279642</td>
      <td>-2.104835</td>
      <td>-2.290197</td>
      <td>-2.211938</td>
      <td>-2.046172</td>
      <td>-2.290197</td>
      <td>-1.820760</td>
      <td>-2.253602</td>
      <td>-2.281995</td>
      <td>-2.285591</td>
      <td>-2.196569</td>
      <td>-2.237026</td>
      <td>-2.256944</td>
      <td>-2.254439</td>
      <td>-2.279294</td>
      <td>-2.290197</td>
      <td>-2.031203</td>
    </tr>
    <tr>
      <th>3</th>
      <td>15000.0000</td>
      <td>0</td>
      <td>0</td>
      <td>4</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>1</td>
      <td>-2.224196</td>
      <td>-2.261199</td>
      <td>-2.261854</td>
      <td>-2.032894</td>
      <td>-1.951432</td>
      <td>-1.973467</td>
      <td>-2.261854</td>
      <td>-2.261854</td>
      <td>-2.189923</td>
      <td>-2.256606</td>
      <td>-2.186509</td>
      <td>-2.261854</td>
      <td>-2.179072</td>
      <td>-2.094693</td>
      <td>-2.261854</td>
      <td>-2.129079</td>
      <td>-2.061336</td>
      <td>-2.248119</td>
      <td>-2.261854</td>
      <td>-2.261258</td>
      <td>-2.139040</td>
      <td>-2.092500</td>
      <td>-2.261854</td>
      <td>-2.194606</td>
      <td>-2.261356</td>
      <td>-2.253046</td>
      <td>-2.085651</td>
      <td>-2.199133</td>
      <td>-2.107840</td>
      <td>-2.256430</td>
      <td>-2.261854</td>
      <td>-2.231613</td>
      <td>-2.261854</td>
      <td>-2.078114</td>
      <td>-2.071832</td>
      <td>-2.261744</td>
      <td>-1.633983</td>
      <td>-2.227341</td>
      <td>-2.261854</td>
      <td>-2.259097</td>
      <td>-2.211688</td>
      <td>-2.188399</td>
      <td>-1.938250</td>
      <td>-2.261220</td>
      <td>-2.204280</td>
      <td>-2.167917</td>
      <td>-2.261514</td>
      <td>-2.225978</td>
      <td>-1.934758</td>
      <td>-2.249661</td>
      <td>-2.261803</td>
      <td>-2.258729</td>
      <td>-2.053822</td>
      <td>-2.261850</td>
      <td>-2.245839</td>
      <td>-2.220374</td>
      <td>-2.229038</td>
      <td>-1.876170</td>
      <td>-2.261582</td>
      <td>-2.261796</td>
      <td>-2.261129</td>
      <td>-2.261854</td>
      <td>-2.261854</td>
      <td>-2.261740</td>
      <td>-2.261854</td>
      <td>-2.162540</td>
      <td>-2.214843</td>
      <td>-2.261302</td>
      <td>-2.246318</td>
      <td>-2.261854</td>
      <td>-2.114885</td>
      <td>-2.260723</td>
      <td>-2.186795</td>
      <td>-2.080806</td>
      <td>-2.218189</td>
      <td>-2.261637</td>
      <td>-2.074756</td>
      <td>-2.050140</td>
      <td>-2.261820</td>
      <td>-1.918067</td>
      <td>-2.183471</td>
      <td>-2.261401</td>
      <td>-2.245762</td>
      <td>-2.236496</td>
      <td>-2.212010</td>
      <td>-2.261854</td>
      <td>-2.156803</td>
      <td>-2.172794</td>
      <td>-2.261854</td>
      <td>-2.261803</td>
      <td>-2.082525</td>
      <td>-2.261854</td>
      <td>-1.712057</td>
      <td>-2.075453</td>
      <td>-2.261854</td>
      <td>-2.039610</td>
      <td>-2.246705</td>
      <td>-2.190228</td>
      <td>-2.261173</td>
      <td>-2.261854</td>
      <td>-2.260738</td>
      <td>-2.122105</td>
      <td>-2.248353</td>
      <td>-2.260611</td>
      <td>-2.260632</td>
      <td>-2.260836</td>
      <td>-2.261854</td>
      <td>-1.909965</td>
      <td>-2.093943</td>
      <td>-2.261854</td>
      <td>-2.261740</td>
      <td>-2.221651</td>
      <td>-2.261854</td>
      <td>-2.228252</td>
      <td>-2.261854</td>
      <td>-2.261854</td>
      <td>-2.261854</td>
      <td>-2.261854</td>
      <td>-2.261854</td>
      <td>-2.249947</td>
      <td>-2.204863</td>
      <td>-2.149975</td>
      <td>-2.261854</td>
      <td>-2.203376</td>
      <td>-2.261854</td>
      <td>-2.236141</td>
      <td>-2.261854</td>
      <td>-2.144711</td>
      <td>-2.261137</td>
      <td>-2.062789</td>
      <td>-2.075248</td>
      <td>-1.916163</td>
      <td>-2.257709</td>
      <td>-2.235916</td>
      <td>-2.261293</td>
      <td>-2.224357</td>
      <td>-2.100448</td>
      <td>-2.261852</td>
      <td>-1.973618</td>
      <td>-2.261514</td>
      <td>-2.103032</td>
      <td>-2.261378</td>
      <td>-2.261854</td>
      <td>-2.261854</td>
      <td>-2.261627</td>
      <td>-2.261854</td>
      <td>-2.261854</td>
      <td>-2.261854</td>
      <td>-2.249091</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3856.6132</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>-2.246961</td>
      <td>-2.178088</td>
      <td>-2.250612</td>
      <td>-2.247886</td>
      <td>-2.133945</td>
      <td>-1.815202</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.230565</td>
      <td>-2.226978</td>
      <td>-2.250612</td>
      <td>-2.180639</td>
      <td>-2.048302</td>
      <td>-2.250612</td>
      <td>-2.005996</td>
      <td>-2.091679</td>
      <td>-2.243880</td>
      <td>-2.250612</td>
      <td>-2.242557</td>
      <td>-2.247455</td>
      <td>-2.200943</td>
      <td>-2.250612</td>
      <td>-2.223070</td>
      <td>-2.250612</td>
      <td>-2.202657</td>
      <td>-2.137743</td>
      <td>-2.241590</td>
      <td>-1.996552</td>
      <td>-2.159647</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.138909</td>
      <td>-2.200521</td>
      <td>-2.250364</td>
      <td>-1.585114</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.247684</td>
      <td>-2.223639</td>
      <td>-2.247560</td>
      <td>-1.977431</td>
      <td>-2.250612</td>
      <td>-2.178088</td>
      <td>-2.248888</td>
      <td>-2.200943</td>
      <td>-2.248189</td>
      <td>-1.892304</td>
      <td>-2.179929</td>
      <td>-2.250612</td>
      <td>-2.248579</td>
      <td>-1.987230</td>
      <td>-2.250612</td>
      <td>-2.241886</td>
      <td>-2.247684</td>
      <td>-2.250612</td>
      <td>-1.955777</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.187300</td>
      <td>-2.250612</td>
      <td>-2.177097</td>
      <td>-2.091595</td>
      <td>-2.092251</td>
      <td>-2.250612</td>
      <td>-2.211228</td>
      <td>-2.144045</td>
      <td>-2.250612</td>
      <td>-1.811872</td>
      <td>-2.187300</td>
      <td>-2.250612</td>
      <td>-2.186420</td>
      <td>-2.067617</td>
      <td>-2.182350</td>
      <td>-2.250612</td>
      <td>-2.187300</td>
      <td>-2.245964</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.120867</td>
      <td>-2.250612</td>
      <td>-1.617403</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.241460</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.250206</td>
      <td>-2.250612</td>
      <td>-2.246227</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.247890</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-1.806090</td>
      <td>-2.185953</td>
      <td>-2.249146</td>
      <td>-2.250612</td>
      <td>-2.249708</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.144948</td>
      <td>-2.041330</td>
      <td>-2.250612</td>
      <td>-2.197706</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.227207</td>
      <td>-2.250612</td>
      <td>-2.034221</td>
      <td>-2.250612</td>
      <td>-2.009194</td>
      <td>-2.243216</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.241405</td>
      <td>-2.112990</td>
      <td>-2.250612</td>
      <td>-1.922432</td>
      <td>-2.249146</td>
      <td>-2.250597</td>
      <td>-2.250612</td>
      <td>-2.187300</td>
      <td>-2.187300</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
      <td>-2.250612</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">po</span> <span class="o">=</span> <span class="s1">&#39;is_booking&#39;</span> <span class="c1"># primary objective</span>
<span class="n">so</span> <span class="o">=</span> <span class="s1">&#39;is_package&#39;</span> <span class="c1"># sub-objective</span>

<span class="n">features</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">train_data</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">features</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s1">&#39;is_booking&#39;</span><span class="p">)</span>
<span class="n">outcome_flag</span> <span class="o">=</span> <span class="s1">&#39;is_booking&#39;</span>

<span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>156
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">features</span><span class="p">],</span>\
                                                    <span class="n">train_data</span><span class="p">[</span><span class="n">outcome_flag</span><span class="p">],</span>\
                                                    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>\
                                                    <span class="n">stratify</span><span class="o">=</span><span class="n">train_data</span><span class="p">[[</span><span class="s1">&#39;is_package&#39;</span><span class="p">,</span> <span class="s1">&#39;is_booking&#39;</span><span class="p">]],</span>\
                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">2021</span>
                                                   <span class="p">)</span>
<span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>((15000, 156), (3751, 156), (15000,), (3751,))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y_train</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0    13805
1     1195
Name: is_booking, dtype: int64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y_test</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0    3452
1     299
Name: is_booking, dtype: int64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y_train_</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">y_train_</span><span class="p">[</span><span class="s1">&#39;is_booking&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_train</span>
<span class="n">y_train_</span><span class="p">[</span><span class="s1">&#39;is_package&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="s1">&#39;is_package&#39;</span><span class="p">]</span>

<span class="n">y_test_</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">y_test_</span><span class="p">[</span><span class="s1">&#39;is_booking&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_test</span>
<span class="n">y_test_</span><span class="p">[</span><span class="s1">&#39;is_package&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="s1">&#39;is_package&#39;</span><span class="p">]</span>

<span class="n">y_train_</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_test_</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>((15000, 2), (3751, 2))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># X_train.drop(&#39;is_package&#39;, axis=1, inplace=True)</span>
<span class="c1"># X_test.drop(&#39;is_package&#39;, axis=1, inplace=True)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">evaluate_results</span><span class="p">(</span><span class="n">y_prob</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="n">avg_precision</span> <span class="o">=</span> <span class="n">average_precision_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_prob</span><span class="p">)</span>
    <span class="n">auc_roc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_prob</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">avg_precision</span><span class="p">,</span> <span class="n">auc_roc</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="unconstrained-gbt-on-primary-objective">
<h2>Unconstrained GBT on Primary Objective<a class="headerlink" href="#unconstrained-gbt-on-primary-objective" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">unconstrained_gbt</span> <span class="o">=</span> <span class="n">MooGBTClassifier</span><span class="p">(</span>
                        <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;deviance&#39;</span><span class="p">,</span>
                        <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                        <span class="n">random_state</span><span class="o">=</span><span class="mi">2021</span>
<span class="p">)</span>

<span class="n">unconstrained_gbt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MooGBTClassifier(ccp_alpha=0.0, constraints=[], criterion=&#39;friedman_mse&#39;,
                 init=None, learning_rate=0.1, loss=&#39;deviance&#39;, max_depth=3,
                 max_features=None, max_leaf_nodes=None,
                 min_impurity_decrease=0.0, min_impurity_split=None,
                 min_samples_leaf=1, min_samples_split=2,
                 min_weight_fraction_leaf=0.0, n_estimators=100,
                 n_iter_no_change=None, random_state=2021, subsample=1.0,
                 tol=0.0001, validation_fraction=0.1, verbose=0,
                 warm_start=False)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_binomial_deviance_cost</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">pred</span><span class="p">))</span>

<span class="n">pred_train</span> <span class="o">=</span> <span class="n">unconstrained_gbt</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_train</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">pred_test</span> <span class="o">=</span> <span class="n">unconstrained_gbt</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># get sub-objective costs</span>
<span class="n">so_train_cost</span> <span class="o">=</span> <span class="n">get_binomial_deviance_cost</span><span class="p">(</span><span class="n">pred_train</span><span class="p">,</span> <span class="n">y_train_</span><span class="p">[</span><span class="n">so</span><span class="p">])</span>
<span class="n">so_test_cost</span> <span class="o">=</span> <span class="n">get_binomial_deviance_cost</span><span class="p">(</span><span class="n">pred_test</span><span class="p">,</span> <span class="n">y_test_</span><span class="p">[</span><span class="n">so</span><span class="p">])</span>

<span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Sub-objective cost train - </span><span class="si">{</span><span class="n">so_train_cost</span><span class="si">}</span><span class="s2">,</span>
<span class="s2">Sub-objective cost test  - </span><span class="si">{</span><span class="n">so_test_cost</span><span class="si">}</span><span class="s2"></span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sub-objective cost train - 0.9292835375628071,
Sub-objective cost test  - 0.932535376136379
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Training Accuracies</span>
<span class="n">po_auprc</span><span class="p">,</span> <span class="n">po_auroc</span> <span class="o">=</span> <span class="n">evaluate_results</span><span class="p">(</span><span class="n">pred_train</span><span class="p">,</span> <span class="n">y_train_</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">so_auprc</span><span class="p">,</span> <span class="n">so_auroc</span> <span class="o">=</span> <span class="n">evaluate_results</span><span class="p">(</span><span class="n">pred_train</span><span class="p">,</span> <span class="n">y_train_</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot;Training Accuracies</span>
<span class="s2">Primary Objective AUROC - </span><span class="si">{</span><span class="n">po_auroc</span><span class="si">}</span><span class="s2"></span>
<span class="s2">Subobjective AUROC - </span><span class="si">{</span><span class="n">so_auroc</span><span class="si">}</span><span class="s2"></span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training Accuracies
Primary Objective AUROC - 0.8068458005179737
Subobjective AUROC - 0.29053245824551593
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Validation Accuracies</span>
<span class="n">po_auprc</span><span class="p">,</span> <span class="n">po_auroc</span> <span class="o">=</span> <span class="n">evaluate_results</span><span class="p">(</span><span class="n">pred_test</span><span class="p">,</span> <span class="n">y_test_</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">so_auprc</span><span class="p">,</span> <span class="n">so_auroc</span> <span class="o">=</span> <span class="n">evaluate_results</span><span class="p">(</span><span class="n">pred_test</span><span class="p">,</span> <span class="n">y_test_</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot;Validation Accuracies</span>
<span class="s2">Primary Objective AUROC - </span><span class="si">{</span><span class="n">po_auroc</span><span class="si">}</span><span class="s2"></span>
<span class="s2">Subobjective AUROC - </span><span class="si">{</span><span class="n">so_auroc</span><span class="si">}</span><span class="s2"></span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Validation Accuracies
Primary Objective AUROC - 0.7213786201203704
Subobjective AUROC - 0.291705532936315
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Primary and Sceondary Loss Values for Training</span>
<span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot;Training Losses - </span>
<span class="s2">Primary Loss - </span><span class="si">{</span><span class="n">get_binomial_deviance_cost</span><span class="p">(</span><span class="n">pred_train</span><span class="p">,</span> <span class="n">y_train_</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s2"></span>
<span class="s2">Secondary Loss - </span><span class="si">{</span><span class="n">get_binomial_deviance_cost</span><span class="p">(</span><span class="n">pred_train</span><span class="p">,</span> <span class="n">y_train_</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training Losses - 
Primary Loss - 0.22777149353278836
Secondary Loss - 0.9292835375628071
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Primary and Sceondary Loss Values for Validation</span>
<span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot;Validation Losses -</span>
<span class="s2">Primary Loss - </span><span class="si">{</span><span class="n">get_binomial_deviance_cost</span><span class="p">(</span><span class="n">pred_test</span><span class="p">,</span> <span class="n">y_test_</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s2"></span>
<span class="s2">Secondary Loss - </span><span class="si">{</span><span class="n">get_binomial_deviance_cost</span><span class="p">(</span><span class="n">pred_test</span><span class="p">,</span> <span class="n">y_test_</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Validation Losses -
Primary Loss - 0.25218489588118365
Secondary Loss - 0.932535376136379
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="constrained-gbt">
<h2>Constrained GBT<a class="headerlink" href="#constrained-gbt" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">b</span> <span class="o">=</span> <span class="mf">0.65</span> <span class="c1"># upper bound on cost</span>
<span class="n">mu</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">constrained_gbt</span> <span class="o">=</span> <span class="n">MooGBTClassifier</span><span class="p">(</span>
                    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;deviance&#39;</span><span class="p">,</span>
                    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                    <span class="n">constraints</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;mu&quot;</span><span class="p">:</span><span class="n">mu</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span><span class="n">b</span><span class="p">}],</span>
                    <span class="n">random_state</span><span class="o">=</span><span class="mi">2021</span>
<span class="p">)</span>

<span class="n">constrained_gbt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MooGBTClassifier(ccp_alpha=0.0, constraints=[{&#39;alpha&#39;: 0, &#39;b&#39;: 0.65, &#39;mu&#39;: 10}],
                 criterion=&#39;friedman_mse&#39;, init=None, learning_rate=0.1,
                 loss=&#39;deviance&#39;, max_depth=3, max_features=None,
                 max_leaf_nodes=None, min_impurity_decrease=0.0,
                 min_impurity_split=None, min_samples_leaf=1,
                 min_samples_split=2, min_weight_fraction_leaf=0.0,
                 n_estimators=100, n_iter_no_change=None, random_state=2021,
                 subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0,
                 warm_start=False)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">losses</span> <span class="o">=</span> <span class="n">constrained_gbt</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">get_losses</span><span class="p">()</span>
<span class="n">losses</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>n_estimators</th>
      <th>primary_objective</th>
      <th>sub_objective_1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0.273768</td>
      <td>0.698076</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>0.276905</td>
      <td>0.610448</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>0.273078</td>
      <td>0.627302</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>0.269804</td>
      <td>0.643549</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>0.266972</td>
      <td>0.659284</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">losses</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;n_estimators&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;primary_objective&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;primary objective&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">losses</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;n_estimators&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;sub_objective_1&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;subobjective&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;# estimators(trees)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Cost&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s2">&quot;upper right&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_57_0.png" src="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_57_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">losses</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;n_estimators&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;primary_objective&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;primary objective&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_58_0.png" src="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_58_0.png" />
</div>
</div>
</div>
<div class="section" id="changing-hyperparameters-b-and-mu">
<h2>Changing hyperparameters b and mu<a class="headerlink" href="#changing-hyperparameters-b-and-mu" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.65</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]</span>
<span class="n">mus</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># bs = [0.9]</span>
<span class="c1"># mus = [10000, 1000]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">gbt_results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">losses_master</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">bs</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">mu</span> <span class="ow">in</span> <span class="n">mus</span><span class="p">:</span>
        <span class="n">clf</span> <span class="o">=</span> <span class="n">MooGBTClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;deviance&quot;</span><span class="p">,</span> \
                                   <span class="n">constraints</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;mu&quot;</span><span class="p">:</span><span class="n">mu</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span><span class="n">b</span><span class="p">}])</span>
        
        <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train_</span><span class="p">)</span>
        
        <span class="n">losses</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">losses</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;primary&#39;</span><span class="p">,</span>\
                                          <span class="s1">&#39;subobjective_1&#39;</span><span class="p">])</span>\
                                <span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>\
                                <span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;index&#39;</span><span class="p">:</span> <span class="s1">&#39;n_estimators&#39;</span><span class="p">})</span>
        <span class="n">losses</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span>
        <span class="n">losses</span><span class="p">[</span><span class="s1">&#39;mu&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mu</span>
        <span class="n">losses</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">alphas</span><span class="p">)</span>
        
        <span class="n">losses_master</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">losses_master</span><span class="p">,</span> <span class="n">losses</span><span class="p">])</span>
        
        <span class="n">pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">pred_train</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_train</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="c1"># Validation Results</span>
        <span class="n">po_val_auprc</span><span class="p">,</span> <span class="n">po_val_auroc</span> <span class="o">=</span> <span class="n">evaluate_results</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span>\
                                                <span class="n">y_test_</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">so_val_auprc</span><span class="p">,</span> <span class="n">so_val_auroc</span> <span class="o">=</span> <span class="n">evaluate_results</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span>\
                                                <span class="n">y_test_</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
        
        <span class="c1"># Training Results</span>
        <span class="n">po_train_auprc</span><span class="p">,</span> <span class="n">po_train_auroc</span> <span class="o">=</span> <span class="n">evaluate_results</span><span class="p">(</span><span class="n">pred_train</span><span class="p">,</span>\
                                                    <span class="n">y_train_</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">so_train_auprc</span><span class="p">,</span> <span class="n">so_train_auroc</span> <span class="o">=</span> <span class="n">evaluate_results</span><span class="p">(</span><span class="n">pred_train</span><span class="p">,</span>\
                                                    <span class="n">y_train_</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
        
        
        <span class="c1"># Training Losses</span>
        <span class="n">po_loss_train</span> <span class="o">=</span> <span class="n">get_binomial_deviance_cost</span><span class="p">(</span><span class="n">pred_train</span><span class="p">,</span> <span class="n">y_train_</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">so_loss_train</span> <span class="o">=</span> <span class="n">get_binomial_deviance_cost</span><span class="p">(</span><span class="n">pred_train</span><span class="p">,</span> <span class="n">y_train_</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
        
        <span class="c1"># Testing Losses</span>
        <span class="n">po_loss_val</span> <span class="o">=</span> <span class="n">get_binomial_deviance_cost</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y_test_</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">so_loss_val</span> <span class="o">=</span> <span class="n">get_binomial_deviance_cost</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y_test_</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
        
        <span class="n">gbt_res</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">po_train_auroc</span><span class="p">,</span> <span class="n">so_train_auroc</span><span class="p">,</span> <span class="n">po_val_auroc</span><span class="p">,</span>\
                   <span class="n">so_val_auroc</span><span class="p">,</span> <span class="n">po_loss_train</span><span class="p">,</span> <span class="n">so_loss_train</span><span class="p">,</span>\
                   <span class="n">po_loss_val</span><span class="p">,</span> <span class="n">so_loss_val</span><span class="p">]</span>
        
        <span class="n">gbt_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gbt_res</span><span class="p">)</span>

<span class="n">gbt_results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">gbt_results</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;mu&#39;</span><span class="p">,</span> <span class="s1">&#39;po_train_auroc&#39;</span><span class="p">,</span>\
                                                    <span class="s1">&#39;so_train_auroc&#39;</span><span class="p">,</span> <span class="s1">&#39;po_val_auroc&#39;</span><span class="p">,</span>\
                                                    <span class="s1">&#39;so_val_auroc&#39;</span><span class="p">,</span> <span class="s1">&#39;po_loss_train&#39;</span><span class="p">,</span>\
                                                    <span class="s1">&#39;so_loss_train&#39;</span><span class="p">,</span> <span class="s1">&#39;po_loss_val&#39;</span><span class="p">,</span>\
                                                    <span class="s1">&#39;so_loss_val&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">gbt_results_df</span><span class="p">[</span><span class="s1">&#39;po_gain&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="n">gbt_results_df</span><span class="p">[</span><span class="s1">&#39;po_val_auroc&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">po_auroc</span><span class="p">)</span><span class="o">/</span><span class="n">po_auroc</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span>
<span class="n">gbt_results_df</span><span class="p">[</span><span class="s1">&#39;so_gain&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="n">gbt_results_df</span><span class="p">[</span><span class="s1">&#39;so_val_auroc&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">so_auroc</span><span class="p">)</span><span class="o">/</span><span class="n">so_auroc</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span>
<span class="n">gbt_results_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>b</th>
      <th>mu</th>
      <th>po_train_auroc</th>
      <th>so_train_auroc</th>
      <th>po_val_auroc</th>
      <th>so_val_auroc</th>
      <th>po_loss_train</th>
      <th>so_loss_train</th>
      <th>po_loss_val</th>
      <th>so_loss_val</th>
      <th>po_gain</th>
      <th>so_gain</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.90</td>
      <td>1000</td>
      <td>0.790331</td>
      <td>0.566643</td>
      <td>0.678841</td>
      <td>0.576453</td>
      <td>0.236490</td>
      <td>0.660478</td>
      <td>0.263191</td>
      <td>0.657078</td>
      <td>-5.896696</td>
      <td>97.614736</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.90</td>
      <td>1000</td>
      <td>0.790331</td>
      <td>0.566643</td>
      <td>0.678925</td>
      <td>0.576301</td>
      <td>0.236490</td>
      <td>0.660478</td>
      <td>0.263152</td>
      <td>0.657237</td>
      <td>-5.885011</td>
      <td>97.562525</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.90</td>
      <td>100</td>
      <td>0.729833</td>
      <td>0.758638</td>
      <td>0.613437</td>
      <td>0.768216</td>
      <td>0.252459</td>
      <td>0.560524</td>
      <td>0.281254</td>
      <td>0.554925</td>
      <td>-14.963210</td>
      <td>163.353380</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.90</td>
      <td>10</td>
      <td>0.813582</td>
      <td>0.308552</td>
      <td>0.715963</td>
      <td>0.310280</td>
      <td>0.226810</td>
      <td>0.899931</td>
      <td>0.254384</td>
      <td>0.900350</td>
      <td>-0.750769</td>
      <td>6.367562</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.90</td>
      <td>5</td>
      <td>0.812505</td>
      <td>0.303491</td>
      <td>0.714943</td>
      <td>0.303979</td>
      <td>0.226652</td>
      <td>0.904038</td>
      <td>0.254799</td>
      <td>0.905659</td>
      <td>-0.892059</td>
      <td>4.207544</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.80</td>
      <td>1000</td>
      <td>0.674920</td>
      <td>0.916271</td>
      <td>0.589144</td>
      <td>0.918022</td>
      <td>0.265386</td>
      <td>0.485378</td>
      <td>0.284254</td>
      <td>0.482359</td>
      <td>-18.330861</td>
      <td>214.708306</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.80</td>
      <td>1000</td>
      <td>0.674920</td>
      <td>0.916271</td>
      <td>0.589298</td>
      <td>0.918207</td>
      <td>0.265386</td>
      <td>0.485378</td>
      <td>0.284181</td>
      <td>0.482360</td>
      <td>-18.309506</td>
      <td>214.771714</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.80</td>
      <td>100</td>
      <td>0.804852</td>
      <td>0.405742</td>
      <td>0.708680</td>
      <td>0.409116</td>
      <td>0.233320</td>
      <td>0.763753</td>
      <td>0.257237</td>
      <td>0.761007</td>
      <td>-1.760279</td>
      <td>40.249515</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.80</td>
      <td>10</td>
      <td>0.806949</td>
      <td>0.392185</td>
      <td>0.711493</td>
      <td>0.391712</td>
      <td>0.231438</td>
      <td>0.810460</td>
      <td>0.255421</td>
      <td>0.810529</td>
      <td>-1.370389</td>
      <td>34.283360</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.80</td>
      <td>5</td>
      <td>0.811138</td>
      <td>0.396455</td>
      <td>0.708158</td>
      <td>0.397689</td>
      <td>0.229999</td>
      <td>0.810006</td>
      <td>0.256954</td>
      <td>0.808435</td>
      <td>-1.832737</td>
      <td>36.332317</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.70</td>
      <td>1000</td>
      <td>0.696530</td>
      <td>0.887927</td>
      <td>0.615722</td>
      <td>0.890290</td>
      <td>0.255204</td>
      <td>0.531049</td>
      <td>0.271720</td>
      <td>0.528734</td>
      <td>-14.646517</td>
      <td>205.201751</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.70</td>
      <td>1000</td>
      <td>0.696530</td>
      <td>0.887927</td>
      <td>0.615721</td>
      <td>0.890290</td>
      <td>0.255204</td>
      <td>0.531049</td>
      <td>0.271721</td>
      <td>0.528735</td>
      <td>-14.646652</td>
      <td>205.201751</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.70</td>
      <td>100</td>
      <td>0.792150</td>
      <td>0.470884</td>
      <td>0.701436</td>
      <td>0.474140</td>
      <td>0.238098</td>
      <td>0.699486</td>
      <td>0.258387</td>
      <td>0.697612</td>
      <td>-2.764483</td>
      <td>62.540541</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.70</td>
      <td>10</td>
      <td>0.798315</td>
      <td>0.486512</td>
      <td>0.697263</td>
      <td>0.491149</td>
      <td>0.236053</td>
      <td>0.700397</td>
      <td>0.259938</td>
      <td>0.697896</td>
      <td>-3.343006</td>
      <td>68.371419</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.70</td>
      <td>5</td>
      <td>0.797918</td>
      <td>0.495761</td>
      <td>0.698007</td>
      <td>0.498640</td>
      <td>0.235229</td>
      <td>0.701857</td>
      <td>0.259313</td>
      <td>0.700165</td>
      <td>-3.239792</td>
      <td>70.939615</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.65</td>
      <td>1000</td>
      <td>0.761901</td>
      <td>0.628761</td>
      <td>0.675195</td>
      <td>0.634537</td>
      <td>0.244864</td>
      <td>0.633833</td>
      <td>0.260656</td>
      <td>0.632571</td>
      <td>-6.402089</td>
      <td>117.526486</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.65</td>
      <td>1000</td>
      <td>0.761901</td>
      <td>0.628761</td>
      <td>0.675236</td>
      <td>0.634414</td>
      <td>0.244864</td>
      <td>0.633833</td>
      <td>0.260669</td>
      <td>0.632583</td>
      <td>-6.396448</td>
      <td>117.484431</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.65</td>
      <td>100</td>
      <td>0.764891</td>
      <td>0.650848</td>
      <td>0.672105</td>
      <td>0.655108</td>
      <td>0.243319</td>
      <td>0.626180</td>
      <td>0.262297</td>
      <td>0.624357</td>
      <td>-6.830524</td>
      <td>124.578640</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.65</td>
      <td>10</td>
      <td>0.776821</td>
      <td>0.616328</td>
      <td>0.679389</td>
      <td>0.623927</td>
      <td>0.239964</td>
      <td>0.642019</td>
      <td>0.262454</td>
      <td>0.639076</td>
      <td>-5.820746</td>
      <td>113.889348</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.65</td>
      <td>5</td>
      <td>0.789563</td>
      <td>0.564235</td>
      <td>0.689951</td>
      <td>0.572645</td>
      <td>0.237893</td>
      <td>0.662894</td>
      <td>0.261287</td>
      <td>0.659270</td>
      <td>-4.356611</td>
      <td>96.309350</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.60</td>
      <td>1000</td>
      <td>0.643140</td>
      <td>0.984868</td>
      <td>0.572364</td>
      <td>0.986370</td>
      <td>0.273478</td>
      <td>0.435269</td>
      <td>0.288653</td>
      <td>0.433648</td>
      <td>-20.656903</td>
      <td>238.138843</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.60</td>
      <td>1000</td>
      <td>0.643140</td>
      <td>0.984868</td>
      <td>0.572364</td>
      <td>0.986370</td>
      <td>0.273478</td>
      <td>0.435269</td>
      <td>0.288653</td>
      <td>0.433648</td>
      <td>-20.656903</td>
      <td>238.138843</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.60</td>
      <td>100</td>
      <td>0.706568</td>
      <td>0.868182</td>
      <td>0.624943</td>
      <td>0.868691</td>
      <td>0.252744</td>
      <td>0.542395</td>
      <td>0.269926</td>
      <td>0.541064</td>
      <td>-13.368262</td>
      <td>197.797276</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.60</td>
      <td>10</td>
      <td>0.750060</td>
      <td>0.752497</td>
      <td>0.651348</td>
      <td>0.759910</td>
      <td>0.244709</td>
      <td>0.592163</td>
      <td>0.266611</td>
      <td>0.589684</td>
      <td>-9.707824</td>
      <td>160.505775</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.60</td>
      <td>5</td>
      <td>0.754302</td>
      <td>0.713142</td>
      <td>0.658937</td>
      <td>0.719010</td>
      <td>0.244011</td>
      <td>0.607256</td>
      <td>0.265099</td>
      <td>0.604968</td>
      <td>-8.655874</td>
      <td>146.484795</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">gbt_results_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;gbt_results_df.csv&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">losses_master</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;losses_master.csv&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_losses</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">,</span> <span class="p">):</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="n">losses_master</span><span class="p">[(</span><span class="n">losses_master</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">b_</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">losses_master</span><span class="p">[</span><span class="s1">&#39;mu&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">mu_</span><span class="p">)]</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">losses</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;n_estimators&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;primary&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;primary objective&#39;</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">losses</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;n_estimators&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;subobjective_1&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;subobjective&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s2">&quot;upper left&quot;</span><span class="p">,</span> <span class="n">prop</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">})</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;mu = </span><span class="si">{</span><span class="n">mu_</span><span class="si">}</span><span class="s2">, b = </span><span class="si">{</span><span class="n">b_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;# estimators(trees)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Cost&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s2">&quot;upper right&quot;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">losses</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;n_estimators&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;alpha&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;mu = </span><span class="si">{</span><span class="n">mu_</span><span class="si">}</span><span class="s2">, b = </span><span class="si">{</span><span class="n">b_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;# estimators(trees)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Alpha&quot;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_primary_loss</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">):</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="n">losses_master</span><span class="p">[(</span><span class="n">losses_master</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">b_</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">losses_master</span><span class="p">[</span><span class="s1">&#39;mu&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">mu_</span><span class="p">)]</span>

    <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">losses</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;n_estimators&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;primary&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;primary objective&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s2">&quot;upper left&quot;</span><span class="p">,</span> <span class="n">prop</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">})</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Primary Objective Cost, mu = </span><span class="si">{</span><span class="n">mu_</span><span class="si">}</span><span class="s2">, b = </span><span class="si">{</span><span class="n">b_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;# estimators(trees)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Cost&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_sub_loss</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">):</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="n">losses_master</span><span class="p">[(</span><span class="n">losses_master</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">b_</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">losses_master</span><span class="p">[</span><span class="s1">&#39;mu&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">mu_</span><span class="p">)]</span>

    <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">losses</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;n_estimators&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;subobjective_1&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;subobjective&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s2">&quot;upper left&quot;</span><span class="p">,</span> <span class="n">prop</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">})</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Primary Objective Cost, mu = </span><span class="si">{</span><span class="n">mu_</span><span class="si">}</span><span class="s2">, b = </span><span class="si">{</span><span class="n">b_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;# estimators(trees)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Cost&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_alphas</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">):</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="n">losses_master</span><span class="p">[(</span><span class="n">losses_master</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">b_</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">losses_master</span><span class="p">[</span><span class="s1">&#39;mu&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">mu_</span><span class="p">)]</span>

    <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">losses</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;n_estimators&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;alpha&#39;</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">losses</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;n_estimators&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;subobjective&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">losses</span><span class="p">[</span><span class="s1">&#39;n_estimators&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;mu = </span><span class="si">{</span><span class="n">mu_</span><span class="si">}</span><span class="s2">, b = </span><span class="si">{</span><span class="n">b_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;# estimators(trees)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Alpha Value&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;axes.labelsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;axes.titlesize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;font.size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;20&quot;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;legend.fontsize&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">12</span>

<span class="c1"># params = {&#39;legend.fontsize&#39;: 20,</span>
<span class="c1">#           &#39;legend.handlelength&#39;: 2}</span>
<span class="c1"># plot.rcParams.update(params)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">b_</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">mu_</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">plot_losses</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">)</span>
<span class="n">plot_primary_loss</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">b_</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">mu_</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">plot_losses</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">)</span>
<span class="n">plot_primary_loss</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_71_0.png" src="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_71_0.png" />
<img alt="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_71_1.png" src="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_71_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">b_</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">mu_</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">plot_losses</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">)</span>
<span class="n">plot_primary_loss</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_72_0.png" src="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_72_0.png" />
<img alt="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_72_1.png" src="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_72_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">b_</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">mu_</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">plot_losses</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">)</span>
<span class="n">plot_primary_loss</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">b_</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">mu_</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">plot_losses</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">)</span>
<span class="n">plot_primary_loss</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_74_0.png" src="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_74_0.png" />
<img alt="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_74_1.png" src="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_74_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">b_</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">mu_</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">plot_losses</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">)</span>
<span class="n">plot_primary_loss</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_75_0.png" src="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_75_0.png" />
<img alt="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_75_1.png" src="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_75_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">b_</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">mu_</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">plot_losses</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">)</span>
<span class="n">plot_primary_loss</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_76_0.png" src="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_76_0.png" />
<img alt="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_76_1.png" src="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_76_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">b_</span> <span class="o">=</span> <span class="mf">0.65</span>
<span class="n">mu_</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">plot_losses</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">)</span>
<span class="n">plot_primary_loss</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_77_0.png" src="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_77_0.png" />
<img alt="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_77_1.png" src="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_77_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">b_</span> <span class="o">=</span> <span class="mf">0.65</span>
<span class="n">mu_</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">plot_losses</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">)</span>
<span class="n">plot_primary_loss</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_78_0.png" src="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_78_0.png" />
<img alt="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_78_1.png" src="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_78_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">b_</span> <span class="o">=</span> <span class="mf">0.6</span>
<span class="n">mu_</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">plot_losses</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">)</span>
<span class="n">plot_primary_loss</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">b_</span> <span class="o">=</span> <span class="mf">0.6</span>
<span class="n">mu_</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">plot_losses</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">)</span>
<span class="n">plot_primary_loss</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_80_0.png" src="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_80_0.png" />
<img alt="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_80_1.png" src="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_80_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">b_</span> <span class="o">=</span> <span class="mf">0.6</span>
<span class="n">mu_</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">plot_losses</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">)</span>
<span class="n">plot_primary_loss</span><span class="p">(</span><span class="n">b_</span><span class="p">,</span> <span class="n">mu_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_81_0.png" src="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_81_0.png" />
<img alt="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_81_1.png" src="_images/T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs_81_1.png" />
</div>
</div>
</div>
<div class="section" id="scalarization">
<h2>Scalarization<a class="headerlink" href="#scalarization" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Primary Objective Unconstrained GBT</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">po</span><span class="p">,</span> <span class="n">so</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&#39;is_booking&#39;, &#39;is_package&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">features_1</span> <span class="o">=</span> <span class="n">features</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;is_package&#39;</span><span class="p">]</span>
<span class="n">outcome_flag</span> <span class="o">=</span> <span class="n">po</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">features_1</span><span class="p">],</span>\
                                                    <span class="n">train_data</span><span class="p">[</span><span class="n">outcome_flag</span><span class="p">],</span>\
                                                    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>\
                                                    <span class="n">stratify</span><span class="o">=</span><span class="n">train_data</span><span class="p">[[</span><span class="s1">&#39;is_package&#39;</span><span class="p">,</span> <span class="s1">&#39;is_booking&#39;</span><span class="p">]],</span>\
                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">2021</span>
                                                   <span class="p">)</span>

<span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>((15000, 157), (3751, 157), (15000,), (3751,))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model_1</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span>
                    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;deviance&#39;</span><span class="p">,</span>
                    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                    <span class="n">random_state</span><span class="o">=</span><span class="mi">2021</span>
<span class="p">)</span>

<span class="n">model_1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GradientBoostingClassifier(ccp_alpha=0.0, criterion=&#39;friedman_mse&#39;, init=None,
                           learning_rate=0.1, loss=&#39;deviance&#39;, max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=100,
                           n_iter_no_change=None, presort=&#39;deprecated&#39;,
                           random_state=2021, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pred_train_po</span> <span class="o">=</span> <span class="n">model_1</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_train</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">pred_test_po</span> <span class="o">=</span> <span class="n">model_1</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>

<span class="n">po_train_auprc</span><span class="p">,</span> <span class="n">po_train_auroc</span> <span class="o">=</span> <span class="n">evaluate_results</span><span class="p">(</span><span class="n">pred_train_po</span><span class="p">,</span>\
                                                <span class="n">y_train</span><span class="p">)</span>

<span class="n">po_val_auprc</span><span class="p">,</span> <span class="n">po_val_auroc</span> <span class="o">=</span> <span class="n">evaluate_results</span><span class="p">(</span><span class="n">pred_test_po</span><span class="p">,</span>\
                                                <span class="n">y_test</span><span class="p">)</span>
<span class="n">po_train_auroc</span><span class="p">,</span> <span class="n">po_val_auroc</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.8068458005179737, 0.7212894856164038)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="primary-objective-unconstrained-gbt">
<h2>Primary Objective Unconstrained GBT<a class="headerlink" href="#primary-objective-unconstrained-gbt" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>156
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># features.remove(&#39;is_package&#39;)</span>
<span class="n">outcome_flag</span> <span class="o">=</span> <span class="n">so</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">features</span><span class="p">],</span>\
                                                    <span class="n">train_data</span><span class="p">[</span><span class="n">outcome_flag</span><span class="p">],</span>\
                                                    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>\
                                                    <span class="n">stratify</span><span class="o">=</span><span class="n">train_data</span><span class="p">[[</span><span class="s1">&#39;is_package&#39;</span><span class="p">,</span> <span class="s1">&#39;is_booking&#39;</span><span class="p">]],</span>\
                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">2021</span>
                                                   <span class="p">)</span>

<span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>((15000, 156), (3751, 156), (15000,), (3751,))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model_2</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span>
                    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;deviance&#39;</span><span class="p">,</span>
                    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                    <span class="n">random_state</span><span class="o">=</span><span class="mi">2021</span>
<span class="p">)</span>

<span class="n">model_2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GradientBoostingClassifier(ccp_alpha=0.0, criterion=&#39;friedman_mse&#39;, init=None,
                           learning_rate=0.1, loss=&#39;deviance&#39;, max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=100,
                           n_iter_no_change=None, presort=&#39;deprecated&#39;,
                           random_state=2021, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pred_train_so</span> <span class="o">=</span> <span class="n">model_2</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_train</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">pred_test_so</span> <span class="o">=</span> <span class="n">model_2</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>

<span class="n">so_train_auprc</span><span class="p">,</span> <span class="n">so_train_auroc</span> <span class="o">=</span> <span class="n">evaluate_results</span><span class="p">(</span><span class="n">pred_train_so</span><span class="p">,</span>\
                                                <span class="n">y_train</span><span class="p">)</span>

<span class="n">so_val_auprc</span><span class="p">,</span> <span class="n">so_val_auroc</span> <span class="o">=</span> <span class="n">evaluate_results</span><span class="p">(</span><span class="n">pred_test_so</span><span class="p">,</span>\
                                                <span class="n">y_test</span><span class="p">)</span>
<span class="n">so_train_auroc</span><span class="p">,</span> <span class="n">so_val_auroc</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.8430754782598948, 0.8163999832887701)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">features_</span> <span class="o">=</span> <span class="n">features</span> <span class="o">+</span> <span class="p">[</span><span class="n">po</span><span class="p">,</span> <span class="n">so</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">features_</span><span class="p">],</span>\
                                    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>\
                                    <span class="n">stratify</span><span class="o">=</span><span class="n">train_data</span><span class="p">[[</span><span class="s1">&#39;is_package&#39;</span><span class="p">,</span> <span class="s1">&#39;is_booking&#39;</span><span class="p">]],</span>\
                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">2021</span>
                                   <span class="p">)</span>
<span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>((15000, 157), (3751, 157))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>155
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># features.append(&#39;is_package&#39;)</span>
<span class="n">pred_train_po</span> <span class="o">=</span> <span class="n">model_1</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">features_1</span><span class="p">])[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">pred_test_po</span> <span class="o">=</span> <span class="n">model_1</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">features_1</span><span class="p">])[:,</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># features.remove(&#39;is_package&#39;)</span>
<span class="n">pred_train_so</span> <span class="o">=</span> <span class="n">model_2</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">features</span><span class="p">])[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">pred_test_so</span> <span class="o">=</span> <span class="n">model_2</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">features</span><span class="p">])[:,</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scalar_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

<span class="n">scalar_train</span><span class="p">[</span><span class="s1">&#39;po_pred&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pred_train_po</span>
<span class="n">scalar_train</span><span class="p">[</span><span class="s1">&#39;so_pred&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pred_train_so</span>

<span class="n">scalar_train</span><span class="p">[</span><span class="s1">&#39;po_true&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">po</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">scalar_train</span><span class="p">[</span><span class="s1">&#39;so_true&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">so</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

<span class="n">scalar_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

<span class="n">scalar_test</span><span class="p">[</span><span class="s1">&#39;po_pred&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pred_test_po</span>
<span class="n">scalar_test</span><span class="p">[</span><span class="s1">&#39;so_pred&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pred_test_so</span>

<span class="n">scalar_test</span><span class="p">[</span><span class="s1">&#39;po_true&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">po</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">scalar_test</span><span class="p">[</span><span class="s1">&#39;so_true&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">so</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

<span class="n">scalar_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">scalar_test</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>((15000, 4), (3751, 4))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">wt_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
    <span class="n">wt</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">wt_</span><span class="o">/</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">w1</span> <span class="o">=</span> <span class="n">wt</span>
    <span class="n">w2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">wt</span>
    
    <span class="n">scalar_train</span><span class="p">[</span><span class="s1">&#39;weighted_score&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">w1</span> <span class="o">*</span> <span class="n">scalar_train</span><span class="p">[</span><span class="s1">&#39;po_pred&#39;</span><span class="p">]</span> <span class="o">+</span> \
                                     <span class="n">w2</span> <span class="o">*</span> <span class="n">scalar_train</span><span class="p">[</span><span class="s1">&#39;so_pred&#39;</span><span class="p">]</span>

    <span class="n">scalar_test</span><span class="p">[</span><span class="s1">&#39;weighted_score&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">w1</span> <span class="o">*</span> <span class="n">scalar_test</span><span class="p">[</span><span class="s1">&#39;po_pred&#39;</span><span class="p">]</span> <span class="o">+</span> \
                                    <span class="n">w2</span> <span class="o">*</span> <span class="n">scalar_test</span><span class="p">[</span><span class="s1">&#39;so_pred&#39;</span><span class="p">]</span>
    
    <span class="n">_</span><span class="p">,</span> <span class="n">po_train_auroc</span> <span class="o">=</span> <span class="n">evaluate_results</span><span class="p">(</span><span class="n">scalar_train</span><span class="p">[</span><span class="s1">&#39;weighted_score&#39;</span><span class="p">]</span>\
                     <span class="p">,</span> <span class="n">scalar_train</span><span class="p">[</span><span class="s1">&#39;po_true&#39;</span><span class="p">])</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">so_train_auroc</span> <span class="o">=</span> <span class="n">evaluate_results</span><span class="p">(</span><span class="n">scalar_train</span><span class="p">[</span><span class="s1">&#39;weighted_score&#39;</span><span class="p">]</span>\
                     <span class="p">,</span> <span class="n">scalar_train</span><span class="p">[</span><span class="s1">&#39;so_true&#39;</span><span class="p">])</span>
    
    <span class="n">_</span><span class="p">,</span> <span class="n">po_test_auroc</span> <span class="o">=</span> <span class="n">evaluate_results</span><span class="p">(</span><span class="n">scalar_test</span><span class="p">[</span><span class="s1">&#39;weighted_score&#39;</span><span class="p">]</span>\
                     <span class="p">,</span> <span class="n">scalar_test</span><span class="p">[</span><span class="s1">&#39;po_true&#39;</span><span class="p">])</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">so_test_auroc</span> <span class="o">=</span> <span class="n">evaluate_results</span><span class="p">(</span><span class="n">scalar_test</span><span class="p">[</span><span class="s1">&#39;weighted_score&#39;</span><span class="p">]</span>\
                     <span class="p">,</span> <span class="n">scalar_test</span><span class="p">[</span><span class="s1">&#39;so_true&#39;</span><span class="p">])</span>
    
    <span class="n">l</span> <span class="o">=</span> <span class="p">[</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">po_train_auroc</span><span class="p">,</span> <span class="n">so_train_auroc</span><span class="p">,</span> <span class="n">po_test_auroc</span><span class="p">,</span> <span class="n">so_test_auroc</span><span class="p">]</span>
    
    <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
    
<span class="n">res_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;w1&#39;</span><span class="p">,</span> <span class="s1">&#39;w2&#39;</span><span class="p">,</span> <span class="s1">&#39;po_train_auroc&#39;</span><span class="p">,</span> \
                                    <span class="s1">&#39;so_train_auroc&#39;</span><span class="p">,</span> <span class="s1">&#39;po_test_auroc&#39;</span><span class="p">,</span>\
                                    <span class="s1">&#39;so_test_auroc&#39;</span><span class="p">])</span>
<span class="n">res_df</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(9, 6)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scalar_train</span><span class="p">[</span><span class="s1">&#39;po_true&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0    13805
1     1195
Name: po_true, dtype: int64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scalar_test</span><span class="p">[</span><span class="s1">&#39;po_true&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0    3452
1     299
Name: po_true, dtype: int64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scalar_train</span><span class="p">[</span><span class="s1">&#39;so_true&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0    11261
1     3739
Name: so_true, dtype: int64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scalar_test</span><span class="p">[</span><span class="s1">&#39;so_true&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0    2816
1     935
Name: so_true, dtype: int64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">res_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>w1</th>
      <th>w2</th>
      <th>po_train_auroc</th>
      <th>so_train_auroc</th>
      <th>po_test_auroc</th>
      <th>so_test_auroc</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.1</td>
      <td>0.9</td>
      <td>0.404459</td>
      <td>0.841821</td>
      <td>0.400868</td>
      <td>0.814337</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.2</td>
      <td>0.8</td>
      <td>0.434047</td>
      <td>0.839531</td>
      <td>0.413451</td>
      <td>0.811092</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.3</td>
      <td>0.7</td>
      <td>0.469698</td>
      <td>0.835152</td>
      <td>0.429599</td>
      <td>0.805548</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.4</td>
      <td>0.6</td>
      <td>0.511016</td>
      <td>0.827151</td>
      <td>0.451709</td>
      <td>0.795806</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.5</td>
      <td>0.5</td>
      <td>0.558968</td>
      <td>0.812353</td>
      <td>0.481267</td>
      <td>0.778483</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.6</td>
      <td>0.4</td>
      <td>0.613957</td>
      <td>0.784366</td>
      <td>0.521607</td>
      <td>0.748781</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.7</td>
      <td>0.3</td>
      <td>0.674730</td>
      <td>0.732273</td>
      <td>0.571582</td>
      <td>0.698271</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.8</td>
      <td>0.2</td>
      <td>0.739785</td>
      <td>0.634734</td>
      <td>0.631574</td>
      <td>0.609748</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.9</td>
      <td>0.1</td>
      <td>0.801156</td>
      <td>0.445425</td>
      <td>0.699006</td>
      <td>0.437452</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">po_auroc</span> <span class="o">=</span> <span class="mf">0.7213786201203704</span>
<span class="n">so_auroc</span> <span class="o">=</span> <span class="mf">0.291705532936315</span>

<span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;po_gain&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;po_test_auroc&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">po_auroc</span><span class="p">)</span><span class="o">/</span><span class="n">po_auroc</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span>
<span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;so_gain&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;so_test_auroc&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">so_auroc</span><span class="p">)</span><span class="o">/</span><span class="n">so_auroc</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">res_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>w1</th>
      <th>w2</th>
      <th>po_train_auroc</th>
      <th>so_train_auroc</th>
      <th>po_test_auroc</th>
      <th>so_test_auroc</th>
      <th>po_gain</th>
      <th>so_gain</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.1</td>
      <td>0.9</td>
      <td>0.404459</td>
      <td>0.841821</td>
      <td>0.400868</td>
      <td>0.814337</td>
      <td>-44.430305</td>
      <td>179.164220</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.2</td>
      <td>0.8</td>
      <td>0.434047</td>
      <td>0.839531</td>
      <td>0.413451</td>
      <td>0.811092</td>
      <td>-42.685941</td>
      <td>178.051661</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.3</td>
      <td>0.7</td>
      <td>0.469698</td>
      <td>0.835152</td>
      <td>0.429599</td>
      <td>0.805548</td>
      <td>-40.447467</td>
      <td>176.151131</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.4</td>
      <td>0.6</td>
      <td>0.511016</td>
      <td>0.827151</td>
      <td>0.451709</td>
      <td>0.795806</td>
      <td>-37.382474</td>
      <td>172.811500</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.5</td>
      <td>0.5</td>
      <td>0.558968</td>
      <td>0.812353</td>
      <td>0.481267</td>
      <td>0.778483</td>
      <td>-33.285073</td>
      <td>166.872947</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.6</td>
      <td>0.4</td>
      <td>0.613957</td>
      <td>0.784366</td>
      <td>0.521607</td>
      <td>0.748781</td>
      <td>-27.692982</td>
      <td>156.690654</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.7</td>
      <td>0.3</td>
      <td>0.674730</td>
      <td>0.732273</td>
      <td>0.571582</td>
      <td>0.698271</td>
      <td>-20.765355</td>
      <td>139.375352</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.8</td>
      <td>0.2</td>
      <td>0.739785</td>
      <td>0.634734</td>
      <td>0.631574</td>
      <td>0.609748</td>
      <td>-12.449006</td>
      <td>109.028460</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.9</td>
      <td>0.1</td>
      <td>0.801156</td>
      <td>0.445425</td>
      <td>0.699006</td>
      <td>0.437452</td>
      <td>-3.101322</td>
      <td>49.963609</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">res_df</span><span class="p">[[</span><span class="s1">&#39;w1&#39;</span><span class="p">,</span> <span class="s1">&#39;w2&#39;</span><span class="p">,</span> <span class="s1">&#39;po_test_auroc&#39;</span><span class="p">,</span> <span class="s1">&#39;po_gain&#39;</span><span class="p">,</span> <span class="s1">&#39;so_test_auroc&#39;</span><span class="p">,</span> <span class="s1">&#39;so_gain&#39;</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>w1</th>
      <th>w2</th>
      <th>po_test_auroc</th>
      <th>po_gain</th>
      <th>so_test_auroc</th>
      <th>so_gain</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.1</td>
      <td>0.9</td>
      <td>0.400868</td>
      <td>-44.430305</td>
      <td>0.814337</td>
      <td>179.164220</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.2</td>
      <td>0.8</td>
      <td>0.413451</td>
      <td>-42.685941</td>
      <td>0.811092</td>
      <td>178.051661</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.3</td>
      <td>0.7</td>
      <td>0.429599</td>
      <td>-40.447467</td>
      <td>0.805548</td>
      <td>176.151131</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.4</td>
      <td>0.6</td>
      <td>0.451709</td>
      <td>-37.382474</td>
      <td>0.795806</td>
      <td>172.811500</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.5</td>
      <td>0.5</td>
      <td>0.481267</td>
      <td>-33.285073</td>
      <td>0.778483</td>
      <td>166.872947</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.6</td>
      <td>0.4</td>
      <td>0.521607</td>
      <td>-27.692982</td>
      <td>0.748781</td>
      <td>156.690654</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.7</td>
      <td>0.3</td>
      <td>0.571582</td>
      <td>-20.765355</td>
      <td>0.698271</td>
      <td>139.375352</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.8</td>
      <td>0.2</td>
      <td>0.631574</td>
      <td>-12.449006</td>
      <td>0.609748</td>
      <td>109.028460</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.9</td>
      <td>0.1</td>
      <td>0.699006</td>
      <td>-3.101322</td>
      <td>0.437452</td>
      <td>49.963609</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
<div class="section" id="takeaways">
<h2>Takeaways<a class="headerlink" href="#takeaways" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p>Scalarization works surprisingly well and it should be your first choice as you head into MOO. It starts breaking down when you have more than two objectives or when the individual models start drifting in potentially opposing directions thereby invalidating the originally set λ value.</p></li>
<li><p>Moo-GBT-like methods appear to work better when the objectives are somewhat orthogonal in nature. For example, if your primary objective is improving clicks and the sub-objective is improving conversions and since all conversions require a click, the objectives are potentially highly correlated.</p></li>
</ol>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "sparsh-ai/multiobjective-optimizations",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="T948705_Sparsely_gated_Mixture_of_Experts.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Sparsely-gated Mixture-of-Experts</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="T145475_Multi_objective_Optimization_using_Pymoo_Library.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Multi-objective Optimization using Pymoo Library</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Sparsh A.<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>